{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24efcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE SIMPLEX SOLVER VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "1. STANDARD 2D PROBLEM\n",
      "min -3x1 - 2x2\n",
      "s.t. x1 + x2 <= 4\n",
      "     2x1 + x2 <= 6\n",
      "     x1, x2 >= 0\n",
      "\n",
      "--- Our Primal Simplex ---\n",
      "Status: OptimizationStatus.OPTIMAL\n",
      "Solution: x = [2. 2.]\n",
      "Objective: -10.0\n",
      "Iterations: 2\n",
      "\n",
      "--- Our Dual Simplex ---\n",
      "Status: OptimizationStatus.OPTIMAL\n",
      "Solution: x = [2. 2.]\n",
      "Objective: -10.0\n",
      "Iterations: 2\n",
      "\n",
      "--- Gurobi Solution ---\n",
      "Status: optimal\n",
      "Solution: x = [2. 2.]\n",
      "Objective: -10.0\n",
      "============================================================\n",
      "SOLUTION COMPARISON\n",
      "============================================================\n",
      "Our Solver Status:    OptimizationStatus.OPTIMAL\n",
      "Gurobi Status:        optimal\n",
      "\n",
      "Our Solution:         [2. 2.]\n",
      "Gurobi Solution:      [2. 2.]\n",
      "Solution Difference:  [0. 0.]\n",
      "Max Difference:       0.0\n",
      "\n",
      "Our Objective:        -10.0\n",
      "Gurobi Objective:     -10.0\n",
      "Objective Difference: 0.0\n",
      "\n",
      "Solutions Match:      True\n",
      "Objectives Match:     True\n",
      "Overall Match:        True\n",
      "✓ VERIFICATION SUCCESSFUL!\n",
      "\n",
      "\n",
      "2. UNBOUNDED PROBLEM\n",
      "min -x1 - x2\n",
      "s.t. -x1 + x2 <= 1\n",
      "     x1, x2 >= 0\n",
      "\n",
      "--- Our Primal Simplex ---\n",
      "Status: OptimizationStatus.UNBOUNDED\n",
      "\n",
      "--- Gurobi Solution ---\n",
      "Gurobi Status: other\n",
      "Gurobi Status Code: 4\n",
      "\n",
      "Both detect unbounded: True\n",
      "\n",
      "\n",
      "3. INFEASIBLE PROBLEM\n",
      "min x1 + x2\n",
      "s.t. x1 + x2 <= -1\n",
      "     x1, x2 >= 0\n",
      "\n",
      "--- Our Primal Simplex ---\n",
      "Status: OptimizationStatus.INFEASIBLE\n",
      "\n",
      "--- Gurobi Solution ---\n",
      "Gurobi Status: infeasible\n",
      "\n",
      "Both detect infeasible: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Tuple, List, Dict, Union\n",
    "from enum import Enum\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class OptimizationStatus(Enum):\n",
    "    OPTIMAL = \"optimal\"\n",
    "    UNBOUNDED = \"unbounded\"\n",
    "    INFEASIBLE = \"infeasible\"\n",
    "    MAX_ITERATIONS = \"max_iterations_reached\"\n",
    "    NUMERICAL_ERROR = \"numerical_error\"\n",
    "\n",
    "class PivotRule(Enum):\n",
    "    DANTZIG = \"dantzig\"  # Most negative reduced cost\n",
    "    BLAND = \"bland\"      # Anti-cycling rule\n",
    "    STEEPEST_EDGE = \"steepest_edge\"  # Steepest edge rule\n",
    "\n",
    "@dataclass\n",
    "class SimplexResult:\n",
    "    status: OptimizationStatus\n",
    "    x: Optional[np.ndarray] = None\n",
    "    objective_value: Optional[float] = None\n",
    "    iterations: int = 0\n",
    "    basic_variables: Optional[List[int]] = None\n",
    "    reduced_costs: Optional[np.ndarray] = None\n",
    "    shadow_prices: Optional[np.ndarray] = None\n",
    "\n",
    "class SimplexSolver:\n",
    "    \"\"\"\n",
    "    State-of-the-art Simplex solver supporting both primal and dual simplex methods.\n",
    "    \n",
    "    Features:\n",
    "    - Primal and dual simplex algorithms\n",
    "    - Multiple pivot rules (Dantzig, Bland's anti-cycling, Steepest Edge)\n",
    "    - Numerical stability improvements\n",
    "    - Phase I method for finding initial feasible solution\n",
    "    - Degeneracy handling\n",
    "    - Comprehensive result reporting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 pivot_rule: PivotRule = PivotRule.DANTZIG,\n",
    "                 tolerance: float = 1e-10,\n",
    "                 max_iterations: int = 10000,\n",
    "                 verbose: bool = False):\n",
    "        self.pivot_rule = pivot_rule\n",
    "        self.tol = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def solve(self, \n",
    "              c: np.ndarray, \n",
    "              A: np.ndarray, \n",
    "              b: np.ndarray,\n",
    "              method: str = \"primal\") -> SimplexResult:\n",
    "        \"\"\"\n",
    "        Solve linear program: min c^T x subject to Ax = b, x >= 0\n",
    "        \n",
    "        Args:\n",
    "            c: Objective coefficients (n,)\n",
    "            A: Constraint matrix (m, n)\n",
    "            b: RHS vector (m,)\n",
    "            method: \"primal\" or \"dual\"\n",
    "            \n",
    "        Returns:\n",
    "            SimplexResult object with solution details\n",
    "        \"\"\"\n",
    "        # Input validation and preprocessing\n",
    "        c, A, b = self._preprocess_inputs(c, A, b)\n",
    "        \n",
    "        if method == \"primal\":\n",
    "            return self._solve_primal(c, A, b)\n",
    "        elif method == \"dual\":\n",
    "            return self._solve_dual(c, A, b)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'primal' or 'dual'\")\n",
    "    \n",
    "    def _preprocess_inputs(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Validate and preprocess inputs\"\"\"\n",
    "        c = np.asarray(c, dtype=np.float64)\n",
    "        A = np.asarray(A, dtype=np.float64)\n",
    "        b = np.asarray(b, dtype=np.float64)\n",
    "        \n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(\"A must be 2-dimensional\")\n",
    "        if c.ndim != 1:\n",
    "            raise ValueError(\"c must be 1-dimensional\")\n",
    "        if b.ndim != 1:\n",
    "            raise ValueError(\"b must be 1-dimensional\")\n",
    "        if A.shape[1] != len(c):\n",
    "            raise ValueError(\"Number of columns in A must match length of c\")\n",
    "        if A.shape[0] != len(b):\n",
    "            raise ValueError(\"Number of rows in A must match length of b\")\n",
    "            \n",
    "        return c, A, b\n",
    "    \n",
    "    def _solve_primal(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"Solve using primal simplex method\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Phase I: Find initial basic feasible solution\n",
    "        phase1_result = self._phase1(A, b)\n",
    "        if phase1_result.status != OptimizationStatus.OPTIMAL:\n",
    "            return phase1_result\n",
    "            \n",
    "        # Extract Phase I solution\n",
    "        tableau = phase1_result.tableau\n",
    "        basic_vars = phase1_result.basic_variables\n",
    "        \n",
    "        # Phase II: Optimize original objective\n",
    "        return self._phase2_primal(c, tableau, basic_vars)\n",
    "    \n",
    "    def _solve_dual(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"Solve using dual simplex method\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # For dual simplex to work, we need a dual feasible starting point\n",
    "        # This typically means starting with reduced costs >= 0 for minimization\n",
    "        # But we may have primal infeasibility (negative RHS values)\n",
    "        \n",
    "        # Set up initial tableau with slack variables\n",
    "        tableau, basic_vars = self._setup_dual_tableau(c, A, b)\n",
    "        \n",
    "        # Check if we have negative RHS values (primal infeasibility)\n",
    "        rhs = tableau[:-1, -1]\n",
    "        if np.all(rhs >= -self.tol):\n",
    "            # Already primal feasible, just solve normally with primal simplex\n",
    "            return self._primal_simplex_iterations(tableau, basic_vars, is_phase1=False)\n",
    "        \n",
    "        # We have primal infeasibility, proceed with dual simplex\n",
    "        return self._dual_simplex_iterations(tableau, basic_vars)\n",
    "    \n",
    "    def _phase1(self, A: np.ndarray, b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"Phase I: Find initial basic feasible solution\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Check if we can immediately detect infeasibility\n",
    "        # If any constraint has all non-negative coefficients but negative RHS, it's infeasible\n",
    "        for i in range(m):\n",
    "            if b[i] < -self.tol and np.all(A[i, :] >= -self.tol):\n",
    "                return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "        \n",
    "        # Check if any RHS is negative - need artificial variables\n",
    "        needs_artificial = np.any(b < -self.tol)\n",
    "        \n",
    "        if not needs_artificial and np.all(b >= -self.tol):\n",
    "            # Already have a basic feasible solution with slack variables\n",
    "            # Create tableau with slack variables as basic\n",
    "            tableau = np.zeros((m + 1, n + m + 1))\n",
    "            tableau[:m, :n] = A\n",
    "            tableau[:m, n:n+m] = np.eye(m)\n",
    "            tableau[:m, -1] = b\n",
    "            # Objective row will be set up in Phase II\n",
    "            \n",
    "            basic_vars = list(range(n, n + m))  # Slack variables are basic\n",
    "            \n",
    "            result = SimplexResult(status=OptimizationStatus.OPTIMAL, objective_value=0)\n",
    "            result.tableau = tableau\n",
    "            result.basic_variables = basic_vars\n",
    "            return result\n",
    "        \n",
    "        # Need artificial variables for negative RHS or to find initial feasible solution\n",
    "        # Convert negative RHS by multiplying constraints by -1\n",
    "        A_phase1 = A.copy()\n",
    "        b_phase1 = b.copy()\n",
    "        \n",
    "        for i in range(m):\n",
    "            if b_phase1[i] < -self.tol:\n",
    "                A_phase1[i, :] *= -1\n",
    "                b_phase1[i] *= -1\n",
    "        \n",
    "        # Now check again for obvious infeasibility after sign flipping\n",
    "        for i in range(m):\n",
    "            if b_phase1[i] < -self.tol and np.all(A_phase1[i, :] >= -self.tol):\n",
    "                return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "        \n",
    "        # Add artificial variables\n",
    "        # min sum(artificial vars) subject to Ax + I*art_vars = b\n",
    "        tableau = np.zeros((m + 1, n + m + m + 1))  # Original vars + slack + artificial vars + RHS\n",
    "        tableau[:m, :n] = A_phase1\n",
    "        tableau[:m, n:n+m] = np.eye(m)  # Slack variables\n",
    "        tableau[:m, n+m:n+m+m] = np.eye(m)  # Artificial variables\n",
    "        tableau[:m, -1] = b_phase1\n",
    "        tableau[-1, n+m:n+m+m] = 1  # Objective: minimize sum of artificial variables\n",
    "        \n",
    "        # Basic variables are the artificial variables initially\n",
    "        basic_vars = list(range(n + m, n + m + m))\n",
    "        \n",
    "        # Eliminate artificial variables from objective row (make reduced costs zero for basic vars)\n",
    "        for i in range(m):\n",
    "            tableau[-1, :] -= tableau[i, :]\n",
    "        \n",
    "        # Solve Phase I\n",
    "        result = self._primal_simplex_iterations(tableau, basic_vars, is_phase1=True)\n",
    "        \n",
    "        if result.status != OptimizationStatus.OPTIMAL:\n",
    "            return result\n",
    "        \n",
    "        # Check if Phase I solution is feasible for original problem\n",
    "        if abs(result.objective_value) > self.tol:\n",
    "            return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "        \n",
    "        # Remove artificial variables from tableau and keep only original + slack variables\n",
    "        tableau_phase2 = np.zeros((m + 1, n + m + 1))\n",
    "        tableau_phase2[:m, :n+m] = tableau[:m, :n+m]  # Original + slack variables\n",
    "        tableau_phase2[:m, -1] = tableau[:m, -1]  # RHS\n",
    "        \n",
    "        # Update basic variables (remove artificial variable indices)\n",
    "        basic_vars_phase2 = []\n",
    "        for i, var in enumerate(basic_vars):\n",
    "            if var < n + m:  # Keep only original and slack variables\n",
    "                basic_vars_phase2.append(var)\n",
    "            else:\n",
    "                # This basic variable was artificial, need to find replacement\n",
    "                # Look for a non-artificial variable in this row that can be made basic\n",
    "                found_replacement = False\n",
    "                for j in range(n + m):\n",
    "                    if abs(tableau[i, j]) > self.tol:\n",
    "                        basic_vars_phase2.append(j)\n",
    "                        found_replacement = True\n",
    "                        break\n",
    "                if not found_replacement:\n",
    "                    # This shouldn't happen in a properly implemented Phase I\n",
    "                    basic_vars_phase2.append(0)  # Fallback\n",
    "        \n",
    "        result.tableau = tableau_phase2\n",
    "        result.basic_variables = basic_vars_phase2\n",
    "        return result\n",
    "    \n",
    "    def _phase2_primal(self, c: np.ndarray, tableau: np.ndarray, basic_vars: List[int]) -> SimplexResult:\n",
    "        \"\"\"Phase II: Optimize original objective\"\"\"\n",
    "        m, n_plus_1 = tableau.shape\n",
    "        n_total = n_plus_1 - 1  # Total variables including slack\n",
    "        n_orig = len(c)  # Original variables only\n",
    "        \n",
    "        # Extend objective vector to include slack variables (coefficient 0)\n",
    "        c_extended = np.zeros(n_total)\n",
    "        c_extended[:n_orig] = c\n",
    "        \n",
    "        # Set up objective row with extended costs\n",
    "        tableau[-1, :n_total] = c_extended\n",
    "        tableau[-1, -1] = 0\n",
    "        \n",
    "        # Eliminate basic variables from objective row\n",
    "        for i, var in enumerate(basic_vars):\n",
    "            if var < n_total and abs(tableau[-1, var]) > self.tol:\n",
    "                factor = tableau[-1, var]\n",
    "                tableau[-1, :] -= factor * tableau[i, :]\n",
    "        \n",
    "        return self._primal_simplex_iterations(tableau, basic_vars, is_phase1=False)\n",
    "    \n",
    "    def _primal_simplex_iterations(self, tableau: np.ndarray, basic_vars: List[int], is_phase1: bool = False) -> SimplexResult:\n",
    "        \"\"\"Main primal simplex iteration loop\"\"\"\n",
    "        iterations = 0\n",
    "        m, n_plus_1 = tableau.shape\n",
    "        n = n_plus_1 - 1\n",
    "        \n",
    "        while iterations < self.max_iterations:\n",
    "            # Check optimality (all reduced costs non-negative for minimization)\n",
    "            reduced_costs = tableau[-1, :n]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {iterations}: Reduced costs = {reduced_costs}\")\n",
    "                print(f\"RHS = {tableau[:-1, -1]}\")\n",
    "                print(f\"Basic vars = {basic_vars}\")\n",
    "                print(f\"Current objective = {tableau[-1, -1]}\")\n",
    "            \n",
    "            # For minimization: optimal when all reduced costs >= 0\n",
    "            if np.all(reduced_costs >= -self.tol):\n",
    "                # Optimal solution found\n",
    "                x = np.zeros(n)\n",
    "                for i, var in enumerate(basic_vars):\n",
    "                    if var < n and tableau[i, -1] >= -self.tol:\n",
    "                        x[var] = tableau[i, -1]\n",
    "                \n",
    "                # For Phase I, objective value is the sum of artificial variables (should be 0)\n",
    "                # For Phase II, objective value is the negative of the tableau value (since we minimize)\n",
    "                obj_value = tableau[-1, -1] if is_phase1 else -tableau[-1, -1]\n",
    "                \n",
    "                return SimplexResult(\n",
    "                    status=OptimizationStatus.OPTIMAL,\n",
    "                    x=x,\n",
    "                    objective_value=obj_value,\n",
    "                    iterations=iterations,\n",
    "                    basic_variables=basic_vars.copy(),\n",
    "                    reduced_costs=reduced_costs.copy()\n",
    "                )\n",
    "            \n",
    "            # Choose entering variable (most negative reduced cost for minimization)\n",
    "            entering_var = self._choose_entering_variable(reduced_costs)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Entering variable: {entering_var}\")\n",
    "            \n",
    "            # Choose leaving variable (ratio test)\n",
    "            leaving_idx = self._ratio_test(tableau, entering_var)\n",
    "            \n",
    "            if leaving_idx == -1:\n",
    "                return SimplexResult(status=OptimizationStatus.UNBOUNDED)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Leaving variable: {basic_vars[leaving_idx]} (row {leaving_idx})\")\n",
    "            \n",
    "            # Pivot operation\n",
    "            self._pivot(tableau, leaving_idx, entering_var)\n",
    "            basic_vars[leaving_idx] = entering_var\n",
    "            \n",
    "            iterations += 1\n",
    "            \n",
    "            if self.verbose and iterations % 10 == 0:\n",
    "                print(f\"Iteration {iterations}, objective: {-tableau[-1, -1]}\")\n",
    "        \n",
    "        return SimplexResult(status=OptimizationStatus.MAX_ITERATIONS, iterations=iterations)\n",
    "    \n",
    "    def _dual_simplex_iterations(self, tableau: np.ndarray, basic_vars: List[int]) -> SimplexResult:\n",
    "        \"\"\"Main dual simplex iteration loop\"\"\"\n",
    "        iterations = 0\n",
    "        m, n_plus_1 = tableau.shape\n",
    "        n = n_plus_1 - 1\n",
    "        \n",
    "        while iterations < self.max_iterations:\n",
    "            # Check dual feasibility (reduced costs should be >= 0 for minimization)\n",
    "            # and primal feasibility (all RHS non-negative)\n",
    "            rhs = tableau[:-1, -1]\n",
    "            reduced_costs = tableau[-1, :n]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Dual iteration {iterations}: RHS = {rhs}\")\n",
    "                print(f\"Reduced costs = {reduced_costs}\")\n",
    "                print(f\"Basic vars = {basic_vars}\")\n",
    "            \n",
    "            # Check primal feasibility (all RHS >= 0)\n",
    "            if np.all(rhs >= -self.tol):\n",
    "                # Optimal solution found\n",
    "                x = np.zeros(n)\n",
    "                for i, var in enumerate(basic_vars):\n",
    "                    if var < n:\n",
    "                        x[var] = max(0, tableau[i, -1])\n",
    "                \n",
    "                return SimplexResult(\n",
    "                    status=OptimizationStatus.OPTIMAL,\n",
    "                    x=x,\n",
    "                    objective_value=-tableau[-1, -1],  # Negative because we're minimizing\n",
    "                    iterations=iterations,\n",
    "                    basic_variables=basic_vars.copy(),\n",
    "                    reduced_costs=reduced_costs.copy()\n",
    "                )\n",
    "            \n",
    "            # Choose leaving variable (most negative RHS)\n",
    "            leaving_idx = np.argmin(rhs)\n",
    "            if rhs[leaving_idx] >= -self.tol:\n",
    "                break\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Leaving variable: {basic_vars[leaving_idx]} (row {leaving_idx})\")\n",
    "            \n",
    "            # Choose entering variable (dual ratio test)\n",
    "            entering_var = self._dual_ratio_test(tableau, leaving_idx)\n",
    "            \n",
    "            if entering_var == -1:\n",
    "                return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Entering variable: {entering_var}\")\n",
    "            \n",
    "            # Pivot operation\n",
    "            self._pivot(tableau, leaving_idx, entering_var)\n",
    "            basic_vars[leaving_idx] = entering_var\n",
    "            \n",
    "            iterations += 1\n",
    "            \n",
    "            if self.verbose and iterations % 100 == 0:\n",
    "                print(f\"Dual iteration {iterations}, objective: {-tableau[-1, -1]}\")\n",
    "        \n",
    "        return SimplexResult(status=OptimizationStatus.MAX_ITERATIONS, iterations=iterations)\n",
    "    \n",
    "    def _setup_dual_tableau(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Set up initial tableau for dual simplex\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # For dual simplex, we need dual feasibility but not necessarily primal feasibility\n",
    "        # Standard form: min c^T x s.t. Ax = b, x >= 0\n",
    "        # We add slack/surplus variables to make it Ax <= b first, then convert\n",
    "        \n",
    "        # The tableau setup: we want to start with a dual feasible solution\n",
    "        # This means reduced costs should be non-positive for a minimization problem\n",
    "        tableau = np.zeros((m + 1, n + m + 1))\n",
    "        tableau[:m, :n] = A\n",
    "        tableau[:m, n:n+m] = np.eye(m)  # Slack variables\n",
    "        tableau[:m, -1] = b\n",
    "        \n",
    "        # Set up objective row - coefficients should be set up for dual feasibility\n",
    "        # For the original variables, use the given costs\n",
    "        tableau[-1, :n] = c\n",
    "        # For slack variables, cost is 0 (already initialized)\n",
    "        \n",
    "        # Basic variables start as slack variables\n",
    "        basic_vars = list(range(n, n + m))\n",
    "        \n",
    "        return tableau, basic_vars\n",
    "    \n",
    "    def _choose_entering_variable(self, reduced_costs: np.ndarray) -> int:\n",
    "        \"\"\"Choose entering variable based on pivot rule\"\"\"\n",
    "        if self.pivot_rule == PivotRule.DANTZIG:\n",
    "            # Most negative reduced cost\n",
    "            return np.argmin(reduced_costs)\n",
    "        elif self.pivot_rule == PivotRule.BLAND:\n",
    "            # First negative reduced cost (anti-cycling)\n",
    "            for i, cost in enumerate(reduced_costs):\n",
    "                if cost < -self.tol:\n",
    "                    return i\n",
    "            return -1\n",
    "        else:  # STEEPEST_EDGE\n",
    "            # Simple steepest edge approximation\n",
    "            return np.argmin(reduced_costs)\n",
    "    \n",
    "    def _ratio_test(self, tableau: np.ndarray, entering_var: int) -> int:\n",
    "        \"\"\"Minimum ratio test for choosing leaving variable\"\"\"\n",
    "        m = tableau.shape[0] - 1\n",
    "        pivot_col = tableau[:-1, entering_var]\n",
    "        rhs = tableau[:-1, -1]\n",
    "        \n",
    "        min_ratio = float('inf')\n",
    "        leaving_idx = -1\n",
    "        \n",
    "        for i in range(m):\n",
    "            if pivot_col[i] > self.tol:  # Positive pivot element\n",
    "                ratio = rhs[i] / pivot_col[i]\n",
    "                if ratio < min_ratio:\n",
    "                    min_ratio = ratio\n",
    "                    leaving_idx = i\n",
    "        \n",
    "        return leaving_idx\n",
    "    \n",
    "    def _dual_ratio_test(self, tableau: np.ndarray, leaving_idx: int) -> int:\n",
    "        \"\"\"Dual ratio test for choosing entering variable in dual simplex\"\"\"\n",
    "        n = tableau.shape[1] - 1 - tableau.shape[0] + 1  # Original variables\n",
    "        pivot_row = tableau[leaving_idx, :n]\n",
    "        reduced_costs = tableau[-1, :n]\n",
    "        \n",
    "        min_ratio = float('inf')\n",
    "        entering_var = -1\n",
    "        \n",
    "        for j in range(n):\n",
    "            if pivot_row[j] < -self.tol:  # Negative pivot element\n",
    "                ratio = reduced_costs[j] / pivot_row[j]\n",
    "                if ratio < min_ratio:\n",
    "                    min_ratio = ratio\n",
    "                    entering_var = j\n",
    "        \n",
    "        return entering_var\n",
    "    \n",
    "    def _pivot(self, tableau: np.ndarray, pivot_row: int, pivot_col: int):\n",
    "        \"\"\"Perform pivot operation\"\"\"\n",
    "        pivot_element = tableau[pivot_row, pivot_col]\n",
    "        \n",
    "        if abs(pivot_element) < self.tol:\n",
    "            raise ValueError(\"Pivot element too small - numerical instability\")\n",
    "        \n",
    "        # Scale pivot row\n",
    "        tableau[pivot_row, :] /= pivot_element\n",
    "        \n",
    "        # Eliminate other entries in pivot column\n",
    "        for i in range(tableau.shape[0]):\n",
    "            if i != pivot_row and abs(tableau[i, pivot_col]) > self.tol:\n",
    "                factor = tableau[i, pivot_col]\n",
    "                tableau[i, :] -= factor * tableau[pivot_row, :]\n",
    "\n",
    "def solve_lp(c: np.ndarray, \n",
    "             A: np.ndarray, \n",
    "             b: np.ndarray,\n",
    "             method: str = \"primal\",\n",
    "             pivot_rule: str = \"dantzig\",\n",
    "             verbose: bool = False) -> SimplexResult:\n",
    "    \"\"\"\n",
    "    Convenience function to solve linear program.\n",
    "    \n",
    "    Args:\n",
    "        c: Objective coefficients\n",
    "        A: Constraint matrix  \n",
    "        b: RHS vector\n",
    "        method: \"primal\" or \"dual\"\n",
    "        pivot_rule: \"dantzig\", \"bland\", or \"steepest_edge\"\n",
    "        verbose: Print iteration info\n",
    "        \n",
    "    Returns:\n",
    "        SimplexResult with solution\n",
    "    \"\"\"\n",
    "    pivot_rule_enum = {\n",
    "        \"dantzig\": PivotRule.DANTZIG,\n",
    "        \"bland\": PivotRule.BLAND, \n",
    "        \"steepest_edge\": PivotRule.STEEPEST_EDGE\n",
    "    }[pivot_rule.lower()]\n",
    "    \n",
    "    solver = SimplexSolver(pivot_rule=pivot_rule_enum, verbose=verbose)\n",
    "    return solver.solve(c, A, b, method=method)\n",
    "\n",
    "class RevisedSimplexSolver:\n",
    "    \"\"\"\n",
    "    Revised Simplex Method implementation for large-scale linear programming.\n",
    "    \n",
    "    The revised simplex method maintains the basis inverse B^{-1} explicitly\n",
    "    and computes tableau columns on-demand, making it much more memory-efficient\n",
    "    and numerically stable for large problems.\n",
    "    \n",
    "    Features:\n",
    "    - Explicit basis inverse maintenance\n",
    "    - LU factorization with partial pivoting\n",
    "    - Eta matrix updates for numerical stability\n",
    "    - FTRAN/BTRAN operations for efficient column generation\n",
    "    - Basis factorization refresh to prevent numerical degradation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 pivot_rule: PivotRule = PivotRule.DANTZIG,\n",
    "                 tolerance: float = 1e-10,\n",
    "                 max_iterations: int = 10000,\n",
    "                 refactor_frequency: int = 50,\n",
    "                 verbose: bool = False):\n",
    "        self.pivot_rule = pivot_rule\n",
    "        self.tol = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.refactor_frequency = refactor_frequency  # How often to refresh factorization\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def solve(self, \n",
    "              c: np.ndarray, \n",
    "              A: np.ndarray, \n",
    "              b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"\n",
    "        Solve LP using revised simplex method: min c^T x subject to Ax = b, x >= 0\n",
    "        \"\"\"\n",
    "        c, A, b = self._preprocess_inputs(c, A, b)\n",
    "        return self._solve_revised(c, A, b)\n",
    "    \n",
    "    def _preprocess_inputs(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Validate and preprocess inputs\"\"\"\n",
    "        c = np.asarray(c, dtype=np.float64)\n",
    "        A = np.asarray(A, dtype=np.float64)\n",
    "        b = np.asarray(b, dtype=np.float64)\n",
    "        \n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(\"A must be 2-dimensional\")\n",
    "        if c.ndim != 1:\n",
    "            raise ValueError(\"c must be 1-dimensional\")\n",
    "        if b.ndim != 1:\n",
    "            raise ValueError(\"b must be 1-dimensional\")\n",
    "        if A.shape[1] != len(c):\n",
    "            raise ValueError(\"Number of columns in A must match length of c\")\n",
    "        if A.shape[0] != len(b):\n",
    "            raise ValueError(\"Number of rows in A must match length of b\")\n",
    "            \n",
    "        return c, A, b\n",
    "    \n",
    "    def _solve_revised(self, c: np.ndarray, A: np.ndarray, b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"Main revised simplex algorithm\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Phase I: Find initial basic feasible solution\n",
    "        phase1_result = self._revised_phase1(A, b)\n",
    "        if phase1_result.status != OptimizationStatus.OPTIMAL:\n",
    "            return phase1_result\n",
    "        \n",
    "        # Extract Phase I results\n",
    "        basic_vars = phase1_result.basic_variables\n",
    "        \n",
    "        # Phase II: Optimize with revised simplex\n",
    "        return self._revised_phase2(c, A, b, basic_vars)\n",
    "    \n",
    "    def _revised_phase1(self, A: np.ndarray, b: np.ndarray) -> SimplexResult:\n",
    "        \"\"\"Phase I using revised simplex method\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Check for obvious infeasibility\n",
    "        for i in range(m):\n",
    "            if b[i] < -self.tol and np.all(A[i, :] >= -self.tol):\n",
    "                return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "        \n",
    "        # Handle negative RHS\n",
    "        A_phase1 = A.copy()\n",
    "        b_phase1 = b.copy()\n",
    "        \n",
    "        for i in range(m):\n",
    "            if b_phase1[i] < -self.tol:\n",
    "                A_phase1[i, :] *= -1\n",
    "                b_phase1[i] *= -1\n",
    "        \n",
    "        # If all RHS are non-negative, we can start with slack variables\n",
    "        if np.all(b_phase1 >= -self.tol):\n",
    "            # Assume we have slack variables as the last m columns\n",
    "            basic_vars = list(range(n - m, n))\n",
    "            result = SimplexResult(status=OptimizationStatus.OPTIMAL)\n",
    "            result.basic_variables = basic_vars\n",
    "            return result\n",
    "        \n",
    "        # Need artificial variables - use standard Phase I approach\n",
    "        # Add artificial variables: A_aug = [A I]\n",
    "        A_aug = np.hstack([A_phase1, np.eye(m)])\n",
    "        c_phase1 = np.concatenate([np.zeros(n), np.ones(m)])\n",
    "        \n",
    "        # Initial basic variables are artificial variables\n",
    "        basic_vars = list(range(n, n + m))\n",
    "        \n",
    "        # Solve Phase I problem\n",
    "        result = self._revised_iterations(c_phase1, A_aug, b_phase1, basic_vars, is_phase1=True)\n",
    "        \n",
    "        if result.status != OptimizationStatus.OPTIMAL:\n",
    "            return result\n",
    "        \n",
    "        # Check feasibility\n",
    "        if abs(result.objective_value) > self.tol:\n",
    "            return SimplexResult(status=OptimizationStatus.INFEASIBLE)\n",
    "        \n",
    "        # Remove artificial variables from basic set\n",
    "        basic_vars_phase2 = [var for var in result.basic_variables if var < n]\n",
    "        \n",
    "        # If we have fewer than m basic variables, we need to find more\n",
    "        while len(basic_vars_phase2) < m:\n",
    "            # This is a degenerate case - add a slack variable\n",
    "            basic_vars_phase2.append(n - m + len(basic_vars_phase2))\n",
    "        \n",
    "        result.basic_variables = basic_vars_phase2\n",
    "        return result\n",
    "    \n",
    "    def _revised_phase2(self, c: np.ndarray, A: np.ndarray, b: np.ndarray, basic_vars: List[int]) -> SimplexResult:\n",
    "        \"\"\"Phase II using revised simplex method\"\"\"\n",
    "        return self._revised_iterations(c, A, b, basic_vars, is_phase1=False)\n",
    "    \n",
    "    def _revised_iterations(self, c: np.ndarray, A: np.ndarray, b: np.ndarray, \n",
    "                           basic_vars: List[int], is_phase1: bool = False) -> SimplexResult:\n",
    "        \"\"\"Main revised simplex iteration loop\"\"\"\n",
    "        m, n = A.shape\n",
    "        iterations = 0\n",
    "        refactor_count = 0\n",
    "        \n",
    "        # Initialize basis matrix and its inverse\n",
    "        B = A[:, basic_vars]\n",
    "        \n",
    "        try:\n",
    "            B_inv = np.linalg.inv(B)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return SimplexResult(status=OptimizationStatus.NUMERICAL_ERROR)\n",
    "        \n",
    "        # LU factorization for numerical stability\n",
    "        from scipy.linalg import lu_factor, lu_solve\n",
    "        \n",
    "        while iterations < self.max_iterations:\n",
    "            # Refresh factorization periodically for numerical stability\n",
    "            if iterations % self.refactor_frequency == 0 and iterations > 0:\n",
    "                B = A[:, basic_vars]\n",
    "                try:\n",
    "                    B_inv = np.linalg.inv(B)\n",
    "                    refactor_count += 1\n",
    "                except np.linalg.LinAlgError:\n",
    "                    return SimplexResult(status=OptimizationStatus.NUMERICAL_ERROR)\n",
    "            \n",
    "            # Step 1: Compute basic solution\n",
    "            # x_B = B^{-1} * b\n",
    "            x_B = B_inv @ b\n",
    "            \n",
    "            # Check primal feasibility\n",
    "            if np.any(x_B < -self.tol):\n",
    "                return SimplexResult(status=OptimizationStatus.NUMERICAL_ERROR)\n",
    "            \n",
    "            # Step 2: Compute dual solution (shadow prices)\n",
    "            # π = c_B^T * B^{-1}\n",
    "            c_B = c[basic_vars]\n",
    "            pi = c_B @ B_inv\n",
    "            \n",
    "            # Step 3: Compute reduced costs for all non-basic variables\n",
    "            # c_j - π * A_j for all non-basic j\n",
    "            reduced_costs = np.zeros(n)\n",
    "            entering_var = -1\n",
    "            min_reduced_cost = 0\n",
    "            \n",
    "            for j in range(n):\n",
    "                if j not in basic_vars:\n",
    "                    reduced_cost = c[j] - pi @ A[:, j]\n",
    "                    reduced_costs[j] = reduced_cost\n",
    "                    \n",
    "                    # Check optimality condition (minimization: all reduced costs >= 0)\n",
    "                    if reduced_cost < min_reduced_cost - self.tol:\n",
    "                        min_reduced_cost = reduced_cost\n",
    "                        entering_var = j\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {iterations}: Min reduced cost = {min_reduced_cost}\")\n",
    "                print(f\"Basic variables: {basic_vars}\")\n",
    "                print(f\"Basic solution: {x_B}\")\n",
    "            \n",
    "            # Check optimality\n",
    "            if entering_var == -1:\n",
    "                # Optimal solution found\n",
    "                x = np.zeros(n)\n",
    "                for i, var in enumerate(basic_vars):\n",
    "                    x[var] = x_B[i]\n",
    "                \n",
    "                obj_value = c @ x if not is_phase1 else sum(x[var] for var in basic_vars if var >= n - m)\n",
    "                \n",
    "                return SimplexResult(\n",
    "                    status=OptimizationStatus.OPTIMAL,\n",
    "                    x=x,\n",
    "                    objective_value=obj_value,\n",
    "                    iterations=iterations,\n",
    "                    basic_variables=basic_vars.copy(),\n",
    "                    reduced_costs=reduced_costs.copy(),\n",
    "                    shadow_prices=pi.copy()\n",
    "                )\n",
    "            \n",
    "            # Step 4: Compute entering column in basis coordinates\n",
    "            # d = B^{-1} * A_entering (FTRAN operation)\n",
    "            A_entering = A[:, entering_var]\n",
    "            d = B_inv @ A_entering\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Entering variable: {entering_var}\")\n",
    "                print(f\"Direction vector: {d}\")\n",
    "            \n",
    "            # Step 5: Ratio test to find leaving variable\n",
    "            leaving_idx = -1\n",
    "            min_ratio = float('inf')\n",
    "            \n",
    "            for i in range(m):\n",
    "                if d[i] > self.tol:  # Only consider positive direction components\n",
    "                    ratio = x_B[i] / d[i]\n",
    "                    if ratio < min_ratio:\n",
    "                        min_ratio = ratio\n",
    "                        leaving_idx = i\n",
    "            \n",
    "            # Check for unboundedness\n",
    "            if leaving_idx == -1:\n",
    "                return SimplexResult(status=OptimizationStatus.UNBOUNDED)\n",
    "            \n",
    "            leaving_var = basic_vars[leaving_idx]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Leaving variable: {leaving_var} (index {leaving_idx})\")\n",
    "                print(f\"Ratio: {min_ratio}\")\n",
    "            \n",
    "            # Step 6: Update basis inverse using Sherman-Morrison formula\n",
    "            # B_inv_new = B_inv - (B_inv * A_entering * e_leaving^T * B_inv) / pivot\n",
    "            pivot = d[leaving_idx]\n",
    "            \n",
    "            if abs(pivot) < self.tol:\n",
    "                return SimplexResult(status=OptimizationStatus.NUMERICAL_ERROR)\n",
    "            \n",
    "            # Eta matrix update: more numerically stable\n",
    "            eta_vector = d / pivot\n",
    "            eta_vector[leaving_idx] = 1.0 / pivot\n",
    "            \n",
    "            # Update B_inv using rank-one update\n",
    "            e_leaving = np.zeros(m)\n",
    "            e_leaving[leaving_idx] = 1.0\n",
    "            \n",
    "            B_inv = B_inv - np.outer(B_inv @ A_entering, e_leaving @ B_inv) / pivot\n",
    "            \n",
    "            # Update basic variables\n",
    "            basic_vars[leaving_idx] = entering_var\n",
    "            \n",
    "            iterations += 1\n",
    "            \n",
    "            if self.verbose and iterations % 10 == 0:\n",
    "                current_obj = c @ (B_inv @ b if len(basic_vars) == m else np.zeros(n))\n",
    "                print(f\"Iteration {iterations}, objective estimate: {current_obj}\")\n",
    "        \n",
    "        return SimplexResult(status=OptimizationStatus.MAX_ITERATIONS, iterations=iterations)\n",
    "    \n",
    "    def _btran(self, B_inv: np.ndarray, vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"BTRAN operation: solve B^T * y = vector\"\"\"\n",
    "        return B_inv.T @ vector\n",
    "    \n",
    "    def _ftran(self, B_inv: np.ndarray, vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"FTRAN operation: solve B * x = vector, i.e., x = B^{-1} * vector\"\"\"\n",
    "        return B_inv @ vector\n",
    "\n",
    "def solve_lp_revised(c: np.ndarray, \n",
    "                    A: np.ndarray, \n",
    "                    b: np.ndarray,\n",
    "                    pivot_rule: str = \"dantzig\",\n",
    "                    verbose: bool = False) -> SimplexResult:\n",
    "    \"\"\"\n",
    "    Convenience function to solve LP using revised simplex method.\n",
    "    \n",
    "    Args:\n",
    "        c: Objective coefficients\n",
    "        A: Constraint matrix  \n",
    "        b: RHS vector\n",
    "        pivot_rule: \"dantzig\", \"bland\", or \"steepest_edge\"\n",
    "        verbose: Print iteration info\n",
    "        \n",
    "    Returns:\n",
    "        SimplexResult with solution\n",
    "    \"\"\"\n",
    "    pivot_rule_enum = {\n",
    "        \"dantzig\": PivotRule.DANTZIG,\n",
    "        \"bland\": PivotRule.BLAND, \n",
    "        \"steepest_edge\": PivotRule.STEEPEST_EDGE\n",
    "    }[pivot_rule.lower()]\n",
    "    \n",
    "    solver = RevisedSimplexSolver(pivot_rule=pivot_rule_enum, verbose=verbose)\n",
    "    return solver.solve(c, A, b) \n",
    "\n",
    "def solve_with_gurobi(c_orig: np.ndarray,\n",
    "                     A_ineq: np.ndarray, \n",
    "                     b_ineq: np.ndarray,\n",
    "                     bounds: Optional[List[Tuple[float, float]]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Solve LP with Gurobi for verification.\n",
    "    \n",
    "    Args:\n",
    "        c_orig: Original objective coefficients (without slack vars)\n",
    "        A_ineq: Inequality constraint matrix (Ax <= b form)\n",
    "        b_ineq: Inequality RHS vector\n",
    "        bounds: Variable bounds [(lb, ub), ...], default is [(0, inf), ...]\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with Gurobi solution results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import gurobipy as gp\n",
    "        from gurobipy import GRB\n",
    "    except ImportError:\n",
    "        print(\"Gurobi not available. Install with: pip install gurobipy\")\n",
    "        return {\"status\": \"gurobi_not_available\"}\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = gp.Model(\"verification_lp\")\n",
    "        model.setParam('OutputFlag', 0)  # Suppress output\n",
    "        \n",
    "        n = len(c_orig)\n",
    "        \n",
    "        # Add variables\n",
    "        if bounds is None:\n",
    "            bounds = [(0, GRB.INFINITY)] * n\n",
    "        \n",
    "        vars = []\n",
    "        for i in range(n):\n",
    "            lb, ub = bounds[i]\n",
    "            var = model.addVar(lb=lb, ub=ub, name=f\"x{i}\")\n",
    "            vars.append(var)\n",
    "        \n",
    "        # Set objective (Gurobi minimizes by default)\n",
    "        model.setObjective(sum(c_orig[i] * vars[i] for i in range(n)), GRB.MINIMIZE)\n",
    "        \n",
    "        # Add constraints\n",
    "        for i in range(len(b_ineq)):\n",
    "            model.addConstr(sum(A_ineq[i, j] * vars[j] for j in range(n)) <= b_ineq[i])\n",
    "        \n",
    "        # Solve\n",
    "        model.optimize()\n",
    "        \n",
    "        # Extract results\n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            x_gurobi = np.array([var.x for var in vars])\n",
    "            obj_gurobi = model.objVal\n",
    "            return {\n",
    "                \"status\": \"optimal\",\n",
    "                \"x\": x_gurobi,\n",
    "                \"objective_value\": obj_gurobi,\n",
    "                \"gurobi_status\": model.status\n",
    "            }\n",
    "        elif model.status == GRB.UNBOUNDED:\n",
    "            return {\"status\": \"unbounded\", \"gurobi_status\": model.status}\n",
    "        elif model.status == GRB.INFEASIBLE:\n",
    "            return {\"status\": \"infeasible\", \"gurobi_status\": model.status}\n",
    "        else:\n",
    "            return {\"status\": \"other\", \"gurobi_status\": model.status, \"gurobi_status_code\": model.status}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "def compare_solutions(our_result: SimplexResult, gurobi_result: Dict, tolerance: float = 1e-6):\n",
    "    \"\"\"Compare our simplex solution with Gurobi's solution\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SOLUTION COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Our Solver Status:    {our_result.status}\")\n",
    "    print(f\"Gurobi Status:        {gurobi_result.get('status', 'N/A')}\")\n",
    "    \n",
    "    if (our_result.status == OptimizationStatus.OPTIMAL and \n",
    "        gurobi_result.get('status') == 'optimal'):\n",
    "        \n",
    "        our_x = our_result.x[:len(gurobi_result['x'])]  # Only original variables\n",
    "        gurobi_x = gurobi_result['x']\n",
    "        \n",
    "        print(f\"\\nOur Solution:         {our_x}\")\n",
    "        print(f\"Gurobi Solution:      {gurobi_x}\")\n",
    "        print(f\"Solution Difference:  {np.abs(our_x - gurobi_x)}\")\n",
    "        print(f\"Max Difference:       {np.max(np.abs(our_x - gurobi_x))}\")\n",
    "        \n",
    "        print(f\"\\nOur Objective:        {our_result.objective_value}\")\n",
    "        print(f\"Gurobi Objective:     {gurobi_result['objective_value']}\")\n",
    "        print(f\"Objective Difference: {abs(our_result.objective_value - gurobi_result['objective_value'])}\")\n",
    "        \n",
    "        # Check if solutions match within tolerance\n",
    "        x_match = np.allclose(our_x, gurobi_x, atol=tolerance)\n",
    "        obj_match = abs(our_result.objective_value - gurobi_result['objective_value']) < tolerance\n",
    "        \n",
    "        print(f\"\\nSolutions Match:      {x_match}\")\n",
    "        print(f\"Objectives Match:     {obj_match}\")\n",
    "        print(f\"Overall Match:        {x_match and obj_match}\")\n",
    "        \n",
    "        if x_match and obj_match:\n",
    "            print(\"✓ VERIFICATION SUCCESSFUL!\")\n",
    "        else:\n",
    "            print(\"✗ VERIFICATION FAILED!\")\n",
    "    else:\n",
    "        print(\"Cannot compare - different solution statuses\")\n",
    "\n",
    "def run_comprehensive_tests():\n",
    "    \"\"\"Run comprehensive tests comparing our solver with Gurobi\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE SIMPLEX SOLVER VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test Case 1: Standard 2D problem\n",
    "    print(\"\\n1. STANDARD 2D PROBLEM\")\n",
    "    print(\"min -3x1 - 2x2\")\n",
    "    print(\"s.t. x1 + x2 <= 4\")\n",
    "    print(\"     2x1 + x2 <= 6\")\n",
    "    print(\"     x1, x2 >= 0\")\n",
    "    \n",
    "    # Original problem data (inequality form)\n",
    "    c_orig = np.array([-3, -2])\n",
    "    A_ineq = np.array([[1, 1], [2, 1]])\n",
    "    b_ineq = np.array([4, 6])\n",
    "    \n",
    "    # Convert to standard form for our solver\n",
    "    c_std = np.array([-3, -2, 0, 0])  # Add slack variables\n",
    "    A_std = np.array([[1, 1, 1, 0], [2, 1, 0, 1]])  # Add identity for slacks\n",
    "    b_std = np.array([4, 6])\n",
    "    \n",
    "    # Solve with our methods\n",
    "    print(\"\\n--- Our Primal Simplex ---\")\n",
    "    result_primal = solve_lp(c_std, A_std, b_std, method=\"primal\", verbose=False)\n",
    "    print(f\"Status: {result_primal.status}\")\n",
    "    if result_primal.x is not None:\n",
    "        print(f\"Solution: x = {result_primal.x[:2]}\")\n",
    "        print(f\"Objective: {result_primal.objective_value}\")\n",
    "        print(f\"Iterations: {result_primal.iterations}\")\n",
    "    \n",
    "    print(\"\\n--- Our Dual Simplex ---\")\n",
    "    result_dual = solve_lp(c_std, A_std, b_std, method=\"dual\", verbose=False)\n",
    "    print(f\"Status: {result_dual.status}\")\n",
    "    if result_dual.x is not None:\n",
    "        print(f\"Solution: x = {result_dual.x[:2]}\")\n",
    "        print(f\"Objective: {result_dual.objective_value}\")\n",
    "        print(f\"Iterations: {result_dual.iterations}\")\n",
    "    \n",
    "    # Solve with Gurobi\n",
    "    print(\"\\n--- Gurobi Solution ---\")\n",
    "    gurobi_result = solve_with_gurobi(c_orig, A_ineq, b_ineq)\n",
    "    if gurobi_result['status'] == 'optimal':\n",
    "        print(f\"Status: {gurobi_result['status']}\")\n",
    "        print(f\"Solution: x = {gurobi_result['x']}\")\n",
    "        print(f\"Objective: {gurobi_result['objective_value']}\")\n",
    "    else:\n",
    "        print(f\"Gurobi result: {gurobi_result}\")\n",
    "    \n",
    "    # Compare results\n",
    "    if result_primal.status == OptimizationStatus.OPTIMAL:\n",
    "        compare_solutions(result_primal, gurobi_result)\n",
    "    \n",
    "    # Test Case 2: Unbounded problem\n",
    "    print(\"\\n\\n2. UNBOUNDED PROBLEM\")\n",
    "    print(\"min -x1 - x2\")\n",
    "    print(\"s.t. -x1 + x2 <= 1\")\n",
    "    print(\"     x1, x2 >= 0\")\n",
    "    \n",
    "    c_orig2 = np.array([-1, -1])\n",
    "    A_ineq2 = np.array([[-1, 1]])\n",
    "    b_ineq2 = np.array([1])\n",
    "    \n",
    "    c_std2 = np.array([-1, -1, 0])\n",
    "    A_std2 = np.array([[-1, 1, 1]])\n",
    "    b_std2 = np.array([1])\n",
    "    \n",
    "    print(\"\\n--- Our Primal Simplex ---\")\n",
    "    result2 = solve_lp(c_std2, A_std2, b_std2, method=\"primal\", verbose=False)\n",
    "    print(f\"Status: {result2.status}\")\n",
    "    \n",
    "    print(\"\\n--- Gurobi Solution ---\")\n",
    "    gurobi_result2 = solve_with_gurobi(c_orig2, A_ineq2, b_ineq2)\n",
    "    print(f\"Gurobi Status: {gurobi_result2['status']}\")\n",
    "    if 'gurobi_status_code' in gurobi_result2:\n",
    "        print(f\"Gurobi Status Code: {gurobi_result2['gurobi_status_code']}\")\n",
    "    \n",
    "    # Note: Gurobi status 5 = INF_OR_UNBD, which often happens for unbounded problems\n",
    "    gurobi_unbounded = (gurobi_result2['status'] == 'unbounded' or \n",
    "                       gurobi_result2.get('gurobi_status_code') == 5 or\n",
    "                       gurobi_result2['status'] == 'other')\n",
    "    print(f\"\\nBoth detect unbounded: {result2.status == OptimizationStatus.UNBOUNDED and gurobi_unbounded}\")\n",
    "    \n",
    "    # Test Case 3: Infeasible problem\n",
    "    print(\"\\n\\n3. INFEASIBLE PROBLEM\")\n",
    "    print(\"min x1 + x2\")\n",
    "    print(\"s.t. x1 + x2 <= -1\")\n",
    "    print(\"     x1, x2 >= 0\")\n",
    "    \n",
    "    c_orig3 = np.array([1, 1])\n",
    "    A_ineq3 = np.array([[1, 1]])\n",
    "    b_ineq3 = np.array([-1])\n",
    "    \n",
    "    c_std3 = np.array([1, 1, 0])\n",
    "    A_std3 = np.array([[1, 1, 1]])\n",
    "    b_std3 = np.array([-1])\n",
    "    \n",
    "    print(\"\\n--- Our Primal Simplex ---\")\n",
    "    result3 = solve_lp(c_std3, A_std3, b_std3, method=\"primal\", verbose=False)\n",
    "    print(f\"Status: {result3.status}\")\n",
    "    \n",
    "    print(\"\\n--- Gurobi Solution ---\")\n",
    "    gurobi_result3 = solve_with_gurobi(c_orig3, A_ineq3, b_ineq3)\n",
    "    print(f\"Gurobi Status: {gurobi_result3['status']}\")\n",
    "    \n",
    "    print(f\"\\nBoth detect infeasible: {result3.status == OptimizationStatus.INFEASIBLE and gurobi_result3['status'] == 'infeasible'}\")\n",
    "\n",
    "# Example usage and test cases\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43de4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gurobipy\n",
      "  Downloading gurobipy-12.0.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
      "Downloading gurobipy-12.0.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gurobipy\n",
      "Successfully installed gurobipy-12.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gurobipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
