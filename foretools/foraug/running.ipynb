{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673d38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMO: Augmentation Transformations\n",
      "======================================================================\n",
      "\n",
      "Input shape: torch.Size([1, 100, 1])\n",
      "Intensity: 0.30\n",
      "\n",
      "  Raw          -> mean abs difference: 0.0000\n",
      "  Jittering    -> mean abs difference: 0.2437\n",
      "  Scaling      -> mean abs difference: 0.0296\n",
      "  Resample     -> mean abs difference: 0.0029\n",
      "  TimeWarp     -> mean abs difference: 0.0202\n",
      "  FreqWarp     -> mean abs difference: 0.1150\n",
      "  MagWarp      -> mean abs difference: 0.0299\n",
      "  TimeMask     -> mean abs difference: 0.0398\n",
      "  Drift        -> mean abs difference: 0.0298\n",
      "\n",
      "======================================================================\n",
      "DEMO: Feature Extraction\n",
      "======================================================================\n",
      "\n",
      "  Sine -> feature shape: torch.Size([1, 24])\n",
      "    Mean: 0.0000, Std: 0.7071, Skew: -0.0000, Kurt: 1.4925\n",
      "    AC1: 1.0030, AC2: 1.0021, SpectralEnt: 0.0008\n",
      "\n",
      "  Noisy Sine -> feature shape: torch.Size([1, 24])\n",
      "    Mean: 0.0796, Std: 0.8527, Skew: 0.1119, Kurt: 2.2489\n",
      "    AC1: 0.7469, AC2: 0.7255, SpectralEnt: 0.3983\n",
      "\n",
      "  Trend + Sine -> feature shape: torch.Size([1, 24])\n",
      "    Mean: 0.0628, Std: 0.6938, Skew: -0.0000, Kurt: 1.5117\n",
      "    AC1: 1.0029, AC2: 1.0018, SpectralEnt: 0.0238\n",
      "======================================================================\n",
      "EXPERIMENT 1: Time Series Classification\n",
      "======================================================================\n",
      "Device: cuda\n",
      "\n",
      "--- Baseline: NoAug ---\n",
      "  NoAug Validation Accuracy: 1.0000\n",
      "\n",
      "--- AutoDA-Timeseries ---\n",
      "Precomputing time series features...\n",
      "  Cached features for 13 batches.\n",
      "Epoch 1/30 | Train Loss: -0.5602 | Val Loss: 1.3265 | Val Acc: 0.2500 | Temps: ['1.000', '1.002', '1.000']\n",
      "Epoch 10/30 | Train Loss: -25.4883 | Val Loss: 0.7396 | Val Acc: 0.7500 | Temps: ['1.004', '1.011', '1.018']\n",
      "Epoch 20/30 | Train Loss: -29.9954 | Val Loss: 0.4619 | Val Acc: 1.0000 | Temps: ['1.004', '1.010', '1.077']\n",
      "Epoch 30/30 | Train Loss: -34.2531 | Val Loss: 0.2582 | Val Acc: 1.0000 | Temps: ['1.004', '1.010', '1.148']\n",
      "\n",
      "  AutoDA-Timeseries Final Validation Accuracy: 1.0000\n",
      "  NoAug Validation Accuracy:                   1.0000\n",
      "  Improvement: +0.00%\n",
      "\n",
      "  Final Augmentation Policy:\n",
      "    layer_0 (temp=1.004):\n",
      "      TimeWarp: 1.000\n",
      "      TimeMask: 0.000\n",
      "      MagWarp: 0.000\n",
      "    layer_1 (temp=1.010):\n",
      "      Jittering: 0.999\n",
      "      FreqWarp: 0.001\n",
      "      Resample: 0.000\n",
      "    layer_2 (temp=1.148):\n",
      "      TimeWarp: 0.255\n",
      "      MagWarp: 0.253\n",
      "      Scaling: 0.252\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: Time Series Forecasting\n",
      "======================================================================\n",
      "\n",
      "--- Baseline: NoAug ---\n",
      "  NoAug Validation MSE: 0.0147\n",
      "\n",
      "--- AutoDA-Timeseries ---\n",
      "Precomputing time series features...\n",
      "  Cached features for 13 batches.\n",
      "Epoch 1/30 | Train Loss: -0.7200 | Val Loss: 0.6745 | Temps: ['1.004', '1.001', '0.998']\n",
      "Epoch 10/30 | Train Loss: -25.9795 | Val Loss: 0.0147 | Temps: ['1.006', '1.011', '1.004']\n",
      "Epoch 20/30 | Train Loss: -30.4361 | Val Loss: 0.0147 | Temps: ['1.006', '1.011', '1.004']\n",
      "Epoch 30/30 | Train Loss: -34.6043 | Val Loss: 0.0146 | Temps: ['1.006', '1.011', '1.005']\n",
      "\n",
      "  AutoDA-Timeseries Final Validation MSE: 0.0146\n",
      "  NoAug Validation MSE:                   0.0147\n",
      "  Improvement: +0.59%\n",
      "\n",
      "======================================================================\n",
      "All experiments completed.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example: AutoDA-Timeseries on synthetic time series tasks.\n",
    "\n",
    "Demonstrates:\n",
    "  1. Classification on synthetic waveform data\n",
    "  2. Forecasting on synthetic autoregressive data\n",
    "  3. Visualization of augmentation policies\n",
    "\n",
    "Run: python example.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/claude\")\n",
    "\n",
    "from foretools.foraug import (\n",
    "    AutoDATimeseries,\n",
    "    AutoDATrainer,\n",
    "    extract_features,\n",
    "    TRANSFORM_NAMES,\n",
    ")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1. Synthetic Data Generation\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def make_classification_data(\n",
    "    n_samples: int = 500,\n",
    "    seq_len: int = 64,\n",
    "    n_channels: int = 3,\n",
    "    n_classes: int = 4,\n",
    "    noise: float = 0.3,\n",
    "):\n",
    "    \"\"\"Generate synthetic waveform classification data.\n",
    "\n",
    "    Each class corresponds to a different frequency/amplitude pattern.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(n_samples):\n",
    "        cls = i % n_classes\n",
    "        t = np.linspace(0, 2 * np.pi, seq_len)\n",
    "        channels = []\n",
    "        for c in range(n_channels):\n",
    "            freq = (cls + 1) * (c + 1) * 0.5\n",
    "            amp = 1.0 + 0.5 * cls\n",
    "            signal = amp * np.sin(freq * t + c * np.pi / 4)\n",
    "            signal += noise * np.random.randn(seq_len)\n",
    "            channels.append(signal)\n",
    "        X.append(np.stack(channels, axis=-1))\n",
    "        Y.append(cls)\n",
    "\n",
    "    X = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def make_forecasting_data(\n",
    "    n_samples: int = 500,\n",
    "    input_len: int = 96,\n",
    "    pred_len: int = 24,\n",
    "    n_channels: int = 1,\n",
    "):\n",
    "    \"\"\"Generate synthetic autoregressive forecasting data.\"\"\"\n",
    "    total_len = input_len + pred_len\n",
    "    X_input, Y_target = [], []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # AR(2) process + trend + seasonality\n",
    "        series = np.zeros((total_len, n_channels))\n",
    "        for c in range(n_channels):\n",
    "            s = np.zeros(total_len)\n",
    "            s[0] = np.random.randn()\n",
    "            s[1] = np.random.randn()\n",
    "            for t in range(2, total_len):\n",
    "                s[t] = 0.6 * s[t-1] - 0.3 * s[t-2] + 0.1 * np.random.randn()\n",
    "            # Add seasonality\n",
    "            t_axis = np.arange(total_len)\n",
    "            s += 0.5 * np.sin(2 * np.pi * t_axis / 12)\n",
    "            s += 0.01 * t_axis  # trend\n",
    "            series[:, c] = s\n",
    "\n",
    "        X_input.append(series[:input_len])\n",
    "        Y_target.append(series[input_len:])\n",
    "\n",
    "    X = torch.tensor(np.array(X_input), dtype=torch.float32)\n",
    "    Y = torch.tensor(np.array(Y_target), dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 2. Simple Downstream Models\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SimpleTCN(nn.Module):\n",
    "    \"\"\"Minimal TCN-like classifier for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, seq_len: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, C) -> (B, C, L)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"Minimal RNN forecaster for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, pred_len: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_channels, hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim, pred_len * input_channels)\n",
    "        self.pred_len = pred_len\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, C)\n",
    "        _, h = self.rnn(x)\n",
    "        h = h[-1]  # last layer hidden state\n",
    "        out = self.fc(h)  # (B, pred_len * C)\n",
    "        return out.view(-1, self.pred_len, self.input_channels)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3. Run Experiments\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def run_classification_experiment():\n",
    "    \"\"\"Classification experiment with AutoDA-Timeseries.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXPERIMENT 1: Time Series Classification\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Generate data\n",
    "    n_classes = 4\n",
    "    X_train, Y_train = make_classification_data(400, seq_len=64, n_channels=3, n_classes=n_classes)\n",
    "    X_val, Y_val = make_classification_data(100, seq_len=64, n_channels=3, n_classes=n_classes)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=32)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # --- Baseline: NoAug ---\n",
    "    print(\"\\n--- Baseline: NoAug ---\")\n",
    "    model_noaug = SimpleTCN(input_channels=3, seq_len=64, n_classes=n_classes)\n",
    "    opt = torch.optim.Adam(model_noaug.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_noaug.to(device)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model_noaug.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss_fn(model_noaug(xb), yb).backward()\n",
    "            opt.step()\n",
    "\n",
    "    model_noaug.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model_noaug(xb).argmax(dim=-1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    noaug_acc = correct / total\n",
    "    print(f\"  NoAug Validation Accuracy: {noaug_acc:.4f}\")\n",
    "\n",
    "    # --- AutoDA-Timeseries ---\n",
    "    print(\"\\n--- AutoDA-Timeseries ---\")\n",
    "    autoda = AutoDATimeseries(\n",
    "        num_layers=3,\n",
    "        hidden_dim=64,\n",
    "        init_temperature=1.0,\n",
    "        raw_bias=0.1,\n",
    "    )\n",
    "    downstream = SimpleTCN(input_channels=3, seq_len=64, n_classes=n_classes)\n",
    "\n",
    "    trainer = AutoDATrainer(\n",
    "        autoda=autoda,\n",
    "        downstream_model=downstream,\n",
    "        task=\"classification\",\n",
    "        lr=1e-3,\n",
    "        aug_lr=5e-4,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        train_loader, val_loader,\n",
    "        epochs=30, log_interval=10,\n",
    "    )\n",
    "\n",
    "    final_acc = history.get(\"val_accuracy\", [0])[-1]\n",
    "    print(f\"\\n  AutoDA-Timeseries Final Validation Accuracy: {final_acc:.4f}\")\n",
    "    print(f\"  NoAug Validation Accuracy:                   {noaug_acc:.4f}\")\n",
    "    print(f\"  Improvement: {(final_acc - noaug_acc) * 100:+.2f}%\")\n",
    "\n",
    "    # Print final augmentation policy\n",
    "    print(\"\\n  Final Augmentation Policy:\")\n",
    "    policy = autoda.get_policy_summary(\n",
    "        *trainer._get_sample_policy(train_loader)\n",
    "    )\n",
    "    for layer_name, info in policy.items():\n",
    "        print(f\"    {layer_name} (temp={info['temperature']:.3f}):\")\n",
    "        probs = info[\"avg_probabilities\"]\n",
    "        top3 = sorted(probs.items(), key=lambda x: -x[1])[:3]\n",
    "        for name, p in top3:\n",
    "            print(f\"      {name}: {p:.3f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_forecasting_experiment():\n",
    "    \"\"\"Forecasting experiment with AutoDA-Timeseries.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXPERIMENT 2: Time Series Forecasting\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    input_len, pred_len = 96, 24\n",
    "    X_train, Y_train = make_forecasting_data(400, input_len, pred_len, n_channels=1)\n",
    "    X_val, Y_val = make_forecasting_data(100, input_len, pred_len, n_channels=1)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=32)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Baseline: NoAug ---\n",
    "    print(\"\\n--- Baseline: NoAug ---\")\n",
    "    model_noaug = SimpleRNN(input_channels=1, pred_len=pred_len)\n",
    "    opt = torch.optim.Adam(model_noaug.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model_noaug.to(device)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model_noaug.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss_fn(model_noaug(xb), yb).backward()\n",
    "            opt.step()\n",
    "\n",
    "    model_noaug.eval()\n",
    "    val_mse = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_mse += loss_fn(model_noaug(xb), yb).item()\n",
    "            count += 1\n",
    "    noaug_mse = val_mse / count\n",
    "    print(f\"  NoAug Validation MSE: {noaug_mse:.4f}\")\n",
    "\n",
    "    # --- AutoDA-Timeseries ---\n",
    "    print(\"\\n--- AutoDA-Timeseries ---\")\n",
    "    autoda = AutoDATimeseries(\n",
    "        num_layers=3,\n",
    "        hidden_dim=64,\n",
    "        init_temperature=1.0,\n",
    "        raw_bias=0.1,\n",
    "    )\n",
    "    downstream = SimpleRNN(input_channels=1, pred_len=pred_len)\n",
    "\n",
    "    trainer = AutoDATrainer(\n",
    "        autoda=autoda,\n",
    "        downstream_model=downstream,\n",
    "        task=\"forecasting\",\n",
    "        lr=1e-3,\n",
    "        aug_lr=5e-4,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        train_loader, val_loader,\n",
    "        epochs=30, log_interval=10,\n",
    "    )\n",
    "\n",
    "    final_mse = history[\"val_loss\"][-1]\n",
    "    print(f\"\\n  AutoDA-Timeseries Final Validation MSE: {final_mse:.4f}\")\n",
    "    print(f\"  NoAug Validation MSE:                   {noaug_mse:.4f}\")\n",
    "    improvement = (noaug_mse - final_mse) / noaug_mse * 100\n",
    "    print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def demonstrate_augmentations():\n",
    "    \"\"\"Demonstrate individual augmentation transformations.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DEMO: Augmentation Transformations\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    from foretools.foraug.transformations import TRANSFORMATIONS, TRANSFORM_NAMES\n",
    "\n",
    "    # Create a simple sine wave\n",
    "    t = torch.linspace(0, 4 * np.pi, 100)\n",
    "    x = torch.sin(t).unsqueeze(0).unsqueeze(-1)  # (1, 100, 1)\n",
    "    intensity = torch.tensor([0.3])\n",
    "\n",
    "    print(f\"\\nInput shape: {x.shape}\")\n",
    "    print(f\"Intensity: {intensity.item():.2f}\\n\")\n",
    "\n",
    "    for name, transform_fn in zip(TRANSFORM_NAMES, TRANSFORMATIONS):\n",
    "        x_aug = transform_fn(x.clone(), intensity)\n",
    "        diff = (x_aug - x).abs().mean().item()\n",
    "        print(f\"  {name:12s} -> mean abs difference: {diff:.4f}\")\n",
    "\n",
    "\n",
    "def demonstrate_features():\n",
    "    \"\"\"Demonstrate feature extraction.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DEMO: Feature Extraction\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create different types of time series\n",
    "    t = torch.linspace(0, 4 * np.pi, 200)\n",
    "    sine = torch.sin(t).unsqueeze(0).unsqueeze(-1)         # (1, 200, 1)\n",
    "    noisy = (torch.sin(t) + 0.5 * torch.randn(200)).unsqueeze(0).unsqueeze(-1)\n",
    "    trend = (0.01 * t + torch.sin(t)).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    for name, x in [(\"Sine\", sine), (\"Noisy Sine\", noisy), (\"Trend + Sine\", trend)]:\n",
    "        features = extract_features(x)\n",
    "        print(f\"\\n  {name} -> feature shape: {features.shape}\")\n",
    "        print(f\"    Mean: {features[0, 0]:.4f}, Std: {features[0, 1]:.4f}, \"\n",
    "              f\"Skew: {features[0, 2]:.4f}, Kurt: {features[0, 3]:.4f}\")\n",
    "        print(f\"    AC1: {features[0, 8]:.4f}, AC2: {features[0, 9]:.4f}, \"\n",
    "              f\"SpectralEnt: {features[0, 17]:.4f}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Main\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    demonstrate_augmentations()\n",
    "    demonstrate_features()\n",
    "    run_classification_experiment()\n",
    "    run_forecasting_experiment()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"All experiments completed.\")\n",
    "    print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foreblocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
