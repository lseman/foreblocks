{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a778a89d",
   "metadata": {},
   "source": [
    "# TreeSHAP Algorithm: A Complete Pedagogical Guide\n",
    "\n",
    "## 🎯 What is SHAP and Why Do We Need It?\n",
    "\n",
    "Imagine you're a doctor using an AI model to diagnose patients. The model says \"80% chance of disease X\" for a patient. As a doctor, you'd naturally ask: **\"Why? Which symptoms led to this conclusion?\"**\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) answers this question by telling us **how much each feature contributed** to the final prediction.\n",
    "\n",
    "### The Fundamental SHAP Equation\n",
    "\n",
    "For any prediction, SHAP guarantees:\n",
    "\n",
    "$$\\text{Prediction} = \\text{Expected Value} + \\sum_{i=1}^{n} \\phi_i$$\n",
    "\n",
    "Where:\n",
    "- $\\phi_i$ = SHAP value for feature $i$ (its contribution)\n",
    "- Expected Value = average prediction across all data\n",
    "- $n$ = number of features\n",
    "\n",
    "**Example**: If a house price model predicts $400K:\n",
    "- Expected price (baseline): $300K\n",
    "- Square footage contribution: +$80K  \n",
    "- Location contribution: +$30K\n",
    "- Age contribution: -$10K\n",
    "- **Total**: $300K + $80K + $30K - $10K = $400K ✓\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 The Core Intuition: Shapley Values from Game Theory\n",
    "\n",
    "SHAP values come from **cooperative game theory**. Think of features as \"players\" in a team trying to achieve a prediction.\n",
    "\n",
    "### The Coalition Game\n",
    "\n",
    "Imagine features as teammates:\n",
    "1. **Solo performance**: How good is each feature alone?\n",
    "2. **Team performance**: How good are they together?\n",
    "3. **Fair credit**: How do we fairly distribute the team's success?\n",
    "\n",
    "**Key insight**: A feature's contribution = its **marginal impact** when added to different coalitions of other features.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For feature $i$, its Shapley value is:\n",
    "\n",
    "$$\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \\cup \\{i\\}) - f(S)]$$\n",
    "\n",
    "Where:\n",
    "- $S$ = coalition of features (subset not including $i$)\n",
    "- $N$ = all features\n",
    "- $f(S)$ = model's expected output using only features in $S$\n",
    "- $\\frac{|S|!(|N|-|S|-1)!}{|N|!}$ = combinatorial weight (how often this coalition occurs)\n",
    "\n",
    "**Translation**: Average the marginal contribution of feature $i$ across all possible coalitions, weighted by how likely each coalition is.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 Why TreeSHAP is Special\n",
    "\n",
    "Computing exact Shapley values normally requires $2^n$ evaluations (exponential!). For 20 features, that's over 1 million calculations per prediction.\n",
    "\n",
    "**TreeSHAP breakthrough**: Exploit tree structure to compute exact Shapley values in **polynomial time**.\n",
    "\n",
    "### Key Insight: Trees Have Structure\n",
    "\n",
    "In a tree model:\n",
    "1. **Path uniqueness**: Each prediction follows exactly one path\n",
    "2. **Feature splits**: Each internal node tests one feature\n",
    "3. **Conditional expectations**: We can compute $f(S)$ efficiently using tree structure\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 TreeSHAP Algorithm: Step by Step\n",
    "\n",
    "### Step 1: Understanding Tree Predictions\n",
    "\n",
    "For any tree, a prediction is determined by:\n",
    "1. **Starting at root**: Begin with all data\n",
    "2. **Following splits**: At each node, go left or right based on feature values\n",
    "3. **Reaching leaf**: Final prediction is the leaf value\n",
    "\n",
    "```\n",
    "Example Tree:\n",
    "     [Root: Age < 30?]\n",
    "    /                \\\n",
    "[Income < 50K?]    [Leaf: +0.8]\n",
    "  /          \\\n",
    "[Leaf: -0.2] [Leaf: +0.3]\n",
    "```\n",
    "\n",
    "### Step 2: Feature Presence vs Absence\n",
    "\n",
    "TreeSHAP compares two scenarios for each feature:\n",
    "- **Present**: Feature has its actual value (sample follows its natural path)\n",
    "- **Absent**: Feature is unknown (sample follows training data distribution)\n",
    "\n",
    "### Step 3: Path Probability Tracking\n",
    "\n",
    "For each node in the tree, we track:\n",
    "- $p_0$ = probability of reaching this node when feature is **absent**\n",
    "- $p_1$ = probability of reaching this node when feature is **present**\n",
    "\n",
    "**Key equations**:\n",
    "- When feature is absent: $p_0 = \\text{fraction of training data that went this way}$\n",
    "- When feature is present: $p_1 = 1$ (if sample goes this way) or $0$ (if not)\n",
    "\n",
    "### Step 4: The Recursive Magic\n",
    "\n",
    "```python\n",
    "def tree_shap_recursive(node, p_zero, p_one, parent_feature):\n",
    "    if node.is_leaf:\n",
    "        # Contribution = difference in probabilities × leaf value\n",
    "        contribution = (p_one - p_zero) × node.leaf_value\n",
    "        shap_values[parent_feature] += contribution\n",
    "        return\n",
    "    \n",
    "    # Get training data split probabilities\n",
    "    p_left = node.left_samples / node.total_samples\n",
    "    p_right = node.right_samples / node.total_samples\n",
    "    \n",
    "    feature = node.split_feature\n",
    "    \n",
    "    if sample_goes_left(feature):\n",
    "        # Sample goes left, so when feature is present: p_one = 1 for left, 0 for right\n",
    "        recurse(left_child, p_zero × p_left, p_one, feature)\n",
    "        recurse(right_child, p_zero × p_right, 0, feature)\n",
    "    else:\n",
    "        # Sample goes right\n",
    "        recurse(left_child, p_zero × p_left, 0, feature)\n",
    "        recurse(right_child, p_zero × p_right, p_one, feature)\n",
    "```\n",
    "\n",
    "### Step 5: Combining Across Trees\n",
    "\n",
    "For ensemble models (like gradient boosting):\n",
    "\n",
    "$$\\phi_i^{\\text{total}} = \\sum_{t=1}^{T} \\eta \\cdot \\phi_i^{(t)}$$\n",
    "\n",
    "Where:\n",
    "- $T$ = number of trees\n",
    "- $\\eta$ = learning rate\n",
    "- $\\phi_i^{(t)}$ = SHAP value for feature $i$ from tree $t$\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Concrete Example: How It Works\n",
    "\n",
    "Let's trace through a simple example.\n",
    "\n",
    "### The Data\n",
    "```\n",
    "Training data:\n",
    "Age | Income | Bought\n",
    "25  | 30K    | No\n",
    "35  | 60K    | Yes  \n",
    "45  | 40K    | No\n",
    "```\n",
    "\n",
    "### The Tree\n",
    "```\n",
    "     [Age < 30?]\n",
    "    /           \\\n",
    " [No: -0.5]   [Income < 50K?]\n",
    "               /            \\\n",
    "            [No: -0.2]   [Yes: +0.8]\n",
    "```\n",
    "\n",
    "### New Sample to Explain\n",
    "```\n",
    "Age: 35, Income: 70K\n",
    "Prediction: +0.8\n",
    "```\n",
    "\n",
    "### TreeSHAP Calculation\n",
    "\n",
    "**Step 1**: Sample path = Right → Right (Age ≥ 30, Income ≥ 50K)\n",
    "\n",
    "**Step 2**: Calculate contributions\n",
    "\n",
    "*For Age feature:*\n",
    "- At root: 1/3 of training data goes left (Age < 30), 2/3 goes right\n",
    "- When Age is absent: $p_0 = 1.0$ (start), then splits to 1/3 left, 2/3 right\n",
    "- When Age is present: Sample goes right, so $p_1 = 1.0$ for right subtree\n",
    "\n",
    "*For Income feature:*\n",
    "- At Income node: 1/2 of remaining data goes left, 1/2 goes right  \n",
    "- When Income is absent: Follow training distribution\n",
    "- When Income is present: Sample goes right to +0.8 leaf\n",
    "\n",
    "**Step 3**: Final SHAP values\n",
    "- Age contribution: Difference between \"with Age\" vs \"without Age\" expected outcomes\n",
    "- Income contribution: Similar calculation for Income feature\n",
    "- Must sum to: Prediction - Expected = 0.8 - (expected baseline)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 Visual Intuition\n",
    "\n",
    "Think of TreeSHAP as asking: **\"What if this feature didn't exist?\"**\n",
    "\n",
    "```\n",
    "Without Feature A:     With Feature A:\n",
    "     [?]         →         [A < 5?]\n",
    "   /     \\               /         \\\n",
    "[Mixed]  [Mixed]    [Clear]    [Clear]\n",
    "```\n",
    "\n",
    "The **difference** in expected outcomes gives us Feature A's contribution.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Counterfactual reasoning**: \"What would happen if...\"\n",
    "2. **Fair attribution**: Each feature gets credit for its unique contribution\n",
    "3. **Mathematical guarantee**: Always adds up to the actual prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Advanced Concepts\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "When a feature value is missing:\n",
    "```python\n",
    "if feature_is_missing:\n",
    "    # Follow the tree's default direction for missing values\n",
    "    p_one = p_zero  # No contribution from this feature\n",
    "```\n",
    "\n",
    "### Feature Interactions\n",
    "\n",
    "TreeSHAP naturally captures feature interactions:\n",
    "- If features A and B work together, their combined effect shows up in their individual SHAP values\n",
    "- Non-linear relationships are automatically handled by tree structure\n",
    "\n",
    "### Ensemble Models\n",
    "\n",
    "For gradient boosting with multiple trees:\n",
    "1. Compute SHAP for each tree independently  \n",
    "2. Scale by learning rate\n",
    "3. Sum across all trees\n",
    "4. Result: Total feature contribution across entire ensemble\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Implementation Details\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- **Naive Shapley**: $O(2^n \\cdot T \\cdot \\text{tree_depth})$ \n",
    "- **TreeSHAP**: $O(n \\cdot T \\cdot \\text{tree_depth})$\n",
    "\n",
    "For 10 features and 100 trees:\n",
    "- Naive: ~102,400 operations\n",
    "- TreeSHAP: ~1,000 operations (100× faster!)\n",
    "\n",
    "### Memory Requirements\n",
    "\n",
    "TreeSHAP needs to store:\n",
    "- Path probabilities: $O(\\text{tree_depth})$\n",
    "- Feature contributions: $O(n)$\n",
    "- Total: $O(n + \\text{tree_depth})$ per prediction\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "Key considerations:\n",
    "1. **Probability tracking**: Use log-space for very deep trees\n",
    "2. **Additivity enforcement**: Explicitly correct small numerical errors\n",
    "3. **Missing value handling**: Ensure consistent behavior\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Validation and Debugging\n",
    "\n",
    "### The Additivity Test\n",
    "\n",
    "Every SHAP implementation must pass:\n",
    "```python\n",
    "def test_additivity(model, X):\n",
    "    shap_values = model.shap_values(X)\n",
    "    predictions = model.predict(X)\n",
    "    expected = model.expected_value\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        reconstructed = expected + sum(shap_values[i])\n",
    "        actual = predictions[i]\n",
    "        error = abs(reconstructed - actual)\n",
    "        assert error < 1e-10, f\"Additivity violated: {error}\"\n",
    "```\n",
    "\n",
    "### Common Issues and Fixes\n",
    "\n",
    "1. **Large additivity errors**: Usually indicates wrong probability calculations\n",
    "2. **Inconsistent explanations**: Check feature masking and tree traversal\n",
    "3. **Performance issues**: Verify tree depth and ensemble size\n",
    "\n",
    "### Debugging Tips\n",
    "\n",
    "```python\n",
    "# Check individual tree contributions\n",
    "for tree_idx, (tree, mask) in enumerate(model.trees):\n",
    "    tree_shap = compute_tree_shap(tree, mask, x)\n",
    "    tree_pred = tree.predict(x[mask])\n",
    "    tree_expected = tree.expected_value\n",
    "    print(f\"Tree {tree_idx}: sum(SHAP)={sum(tree_shap):.6f}, \"\n",
    "          f\"pred-expected={tree_pred - tree_expected:.6f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Key Takeaways\n",
    "\n",
    "### Theoretical Foundations\n",
    "1. **Game theory**: SHAP values are the unique fair attribution method\n",
    "2. **Efficiency**: TreeSHAP exploits tree structure for polynomial-time computation\n",
    "3. **Additivity**: Mathematical guarantee that explanations sum to predictions\n",
    "\n",
    "### Practical Benefits\n",
    "1. **Local explanations**: Understand individual predictions\n",
    "2. **Global insights**: Aggregate SHAP values for feature importance\n",
    "3. **Model debugging**: Identify problematic features or interactions\n",
    "\n",
    "### When to Use TreeSHAP\n",
    "- ✅ Tree-based models (Random Forest, XGBoost, LightGBM, etc.)\n",
    "- ✅ Need exact explanations (not approximations)\n",
    "- ✅ Want guaranteed additivity\n",
    "- ✅ Have reasonable number of features (< 1000)\n",
    "\n",
    "### Limitations\n",
    "- ❌ Only works for tree-based models\n",
    "- ❌ Assumes features are independent (for baseline calculation)\n",
    "- ❌ Can be slow for very large ensembles\n",
    "- ❌ Requires understanding of tree structure\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Further Reading\n",
    "\n",
    "### Essential Papers\n",
    "1. **Original SHAP**: Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\"\n",
    "2. **TreeSHAP**: Lundberg et al. (2020) - \"From local explanations to global understanding with explainable AI for trees\"\n",
    "3. **Shapley Values**: Shapley (1953) - \"A value for n-person games\"\n",
    "\n",
    "### Practical Resources\n",
    "- **SHAP Python Library**: `pip install shap`\n",
    "- **XGBoost Integration**: Built-in `.get_booster().predict(contrib=True)`\n",
    "- **Interpretability Guidelines**: Model Cards, Fairness considerations\n",
    "\n",
    "### Advanced Topics\n",
    "- **SHAP for Deep Learning**: DeepLIFT, GradientSHAP\n",
    "- **Causal SHAP**: Handling confounded features\n",
    "- **Distributional SHAP**: Explanations for probability distributions\n",
    "\n",
    "---\n",
    "\n",
    "*TreeSHAP represents a beautiful intersection of game theory, computer science, and machine learning - turning the complex problem of model interpretation into an elegant, efficient algorithm that provides mathematically principled explanations for tree-based models.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
