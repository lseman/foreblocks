{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db3ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Comparing Tree Growth Strategies\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Level-wise Tree Growth:\n",
      "üöÄ Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.597733, Val: 0.669090, Time: 0.03s\n",
      "[  20] Train: 0.140449, Val: 0.176388, Time: 0.06s\n",
      "[  30] Train: 0.046897, Val: 0.072945, Time: 0.09s\n",
      "[  40] Train: 0.089216, Val: 0.118033, Time: 0.12s\n",
      "[  50] Train: 0.025446, Val: 0.042422, Time: 0.14s\n",
      "‚úÖ Training completed in 0.14s, 50 trees\n",
      "   Time: 0.14s\n",
      "   Test MSE: 0.042422\n",
      "   Trees: 50\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "# ================ USAGE EXAMPLE ================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 10\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(\"üî• Comparing Tree Growth Strategies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Level-wise (original)\n",
    "    print(\"\\n1Ô∏è‚É£ Level-wise Tree Growth:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        tree_learner=\"leaf\",  # Original approach\n",
    "        tree_method=\"hist\",\n",
    "        verbose=True,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # # compare with scikit learn's GradientBoostingRegressor\n",
    "    # from sklearn.ensemble import GradientBoostingRegressor\n",
    "    # model_sklearn = GradientBoostingRegressor(\n",
    "    #     n_estimators=50,\n",
    "    #     learning_rate=0.1,\n",
    "    #     max_depth=6,\n",
    "    #     verbose=1,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "    # model_sklearn.fit(X_train, y_train)\n",
    "    # sklearn_time = time.time() - start_time\n",
    "    # sklearn_pred = model_sklearn.predict(X_test)\n",
    "    # sklearn_mse = np.mean((y_test - sklearn_pred) ** 2)\n",
    "    # print(f\"   Scikit-learn Time: {sklearn_time:.2f}s\")\n",
    "    # print(f\"   Scikit-learn Test MSE: {sklearn_mse:.6f}\")\n",
    "    # print(f\"   Scikit-learn Trees: {len(model_sklearn.estimators_)}\")\n",
    "    \n",
    "    model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "    level_time = time.time() - start_time\n",
    "    level_pred = model_level.predict(X_test)\n",
    "    level_mse = np.mean((y_test - level_pred) ** 2)\n",
    "    \n",
    "    print(f\"   Time: {level_time:.2f}s\")\n",
    "    print(f\"   Test MSE: {level_mse:.6f}\")\n",
    "    print(f\"   Trees: {len(model_level.trees)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceecab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.598155, Val: 0.681858, Time: 0.15s\n",
      "[  20] Train: 0.124275, Val: 0.166428, Time: 0.42s\n",
      "[  30] Train: 0.045702, Val: 0.075435, Time: 0.59s\n",
      "[  40] Train: 0.028798, Val: 0.051505, Time: 0.88s\n",
      "[  50] Train: 0.026389, Val: 0.047181, Time: 1.33s\n",
      "‚úÖ Training completed in 1.33s, 50 trees\n",
      "[TreeSHAP] Computing SHAP for 10 samples, 10 features, 50 trees\n",
      "  Sample 0: Adjusting SHAP sum from 0.34707874 to 0.36686922\n",
      "  Sample 1: Adjusting SHAP sum from 1.36649310 to 1.38628358\n",
      "  Sample 2: Adjusting SHAP sum from 1.58066085 to 1.60045133\n",
      "  Sample 3: Adjusting SHAP sum from 0.29288147 to 0.31267195\n",
      "  Sample 4: Adjusting SHAP sum from -0.17464071 to -0.15485023\n",
      "  Sample 5: Adjusting SHAP sum from 3.11251401 to 3.13230449\n",
      "  Sample 6: Adjusting SHAP sum from -0.92486115 to -0.90507067\n",
      "  Sample 7: Adjusting SHAP sum from -1.90024282 to -1.88045234\n",
      "  Sample 8: Adjusting SHAP sum from -2.40424028 to -2.38444980\n",
      "  Sample 9: Adjusting SHAP sum from -1.93526956 to -1.91547909\n",
      "[TreeSHAP] Final additivity: max_err=8.882e-16, mean_err=2.137e-16\n",
      "Prediction: 0.320533 = E[f(X)] -0.046336 + sum(phi) 0.366869\n",
      "Additivity error: 0.00000000\n",
      "Top Feature Contributions:\n",
      "------------------------------------------------------------\n",
      " 1. feature_1         :  +0.364169 ‚Üë (x=1.0127)\n",
      " 2. feature_2         :  +0.043014 ‚Üë (x=-0.198187)\n",
      " 3. feature_0         :  -0.038846 ‚Üì (x=-0.471858)\n",
      " 4. feature_3         :  -0.005858 ‚Üì (x=0.0905693)\n",
      " 5. feature_7         :  +0.001573 ‚Üë (x=1.04059)\n",
      " 6. feature_5         :  +0.001499 ‚Üë (x=-0.0589634)\n",
      " 7. feature_6         :  +0.000760 ‚Üë (x=-1.81785)\n",
      " 8. feature_9         :  +0.000749 ‚Üë (x=-1.83112)\n",
      " 9. feature_8         :  -0.000391 ‚Üì (x=1.25493)\n",
      "10. feature_4         :  +0.000200 ‚Üë (x=0.717391)\n"
     ]
    }
   ],
   "source": [
    "from shap import *\n",
    "\n",
    "BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "model_level = BoostRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    tree_learner=\"leaf\",  # Original approach\n",
    "    tree_method=\"hist\",\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "level_time = time.time() - start_time\n",
    "# Set background for proper expected value\n",
    "model_level.set_shap_background(X_train[:100])  # Use sample of training data\n",
    "\n",
    "# Compute SHAP values (should have much lower additivity errors)\n",
    "shap_values = model_level.shap_values(X_test[:10], debug=True)\n",
    "\n",
    "# Validate the fix\n",
    "# Explain individual predictions\n",
    "explanation = model_level.explain_prediction(X_test[0])\n",
    "# Get feature importance\n",
    "#importance = model_level.shap_feature_importance(X_test[:100])\n",
    "#print(\"Feature Importance:\", importance)\n",
    "\n",
    "# Analyze model behavior\n",
    "#shap_values = model_level.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SHAP to your model\n",
    "from your_corrected_shap import add_shap_to_boostregressor\n",
    "\n",
    "add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "# Train model\n",
    "model = BoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91107196e-01,  4.20794623e-04, -8.77049296e-03, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12, -2.25141219e-10],\n",
       "       [ 5.39337510e-01,  4.20436384e-04,  3.54941428e-02, ...,\n",
       "         6.65237358e-27,  0.00000000e+00, -5.44576677e-25],\n",
       "       [ 1.04287985e-02,  2.56604225e-01,  7.41113806e-02, ...,\n",
       "        -4.26782522e-10,  0.00000000e+00,  2.08622529e-08],\n",
       "       ...,\n",
       "       [ 6.47171653e-03,  4.69619953e-02,  1.41931347e-03, ...,\n",
       "        -4.91279500e-08,  5.19424793e-12,  2.06371118e-08],\n",
       "       [ 2.42425815e-01, -1.79334008e-02, -1.95592688e-02, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12,  2.08622529e-08],\n",
       "       [-1.37218747e-01, -2.01557929e-01, -2.90648754e-01, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12, -1.52744722e-08]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
