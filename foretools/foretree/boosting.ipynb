{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Comparing Tree Growth Strategies\n",
      "==================================================\n",
      "Training leaf-wise trees\n",
      "  Loss: mse, Base: -0.0095\n",
      "[  10] Train: 0.590026, Val: 0.663195, Time: 0.11s\n",
      "[  20] Train: 0.135681, Val: 0.176386, Time: 0.22s\n",
      "[  30] Train: 0.052351, Val: 0.082435, Time: 0.32s\n",
      "[  40] Train: 0.033992, Val: 0.054255, Time: 0.38s\n",
      "[  50] Train: 0.027840, Val: 0.047505, Time: 0.48s\n",
      "Training completed in 0.48s\n",
      "   Time: 0.49s\n",
      "   Test MSE: 0.047505\n",
      "   Trees: 50\n",
      "   Feature Importances: [3.52513675e-01 3.15337841e-01 3.28674262e-01 4.20885014e-04\n",
      " 2.95784968e-04 4.58280092e-04 6.03707692e-04 6.48259849e-04\n",
      " 4.22619366e-04 6.24684671e-04]\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.6504            0.68s\n",
      "         2           2.2393            0.67s\n",
      "         3           1.8968            0.64s\n",
      "         4           1.6071            0.62s\n",
      "         5           1.3665            0.61s\n",
      "         6           1.1653            0.60s\n",
      "         7           0.9935            0.59s\n",
      "         8           0.8490            0.57s\n",
      "         9           0.7280            0.56s\n",
      "        10           0.6254            0.54s\n",
      "        20           0.1493            0.41s\n",
      "        30           0.0467            0.27s\n",
      "        40           0.0225            0.14s\n",
      "        50           0.0163            0.00s\n",
      "   Scikit-learn Time: 1.17s\n",
      "   Scikit-learn Test MSE: 0.042175\n",
      "   Scikit-learn Trees: 50\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "# ================ USAGE EXAMPLE ================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 10\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(\"ðŸ”¥ Comparing Tree Growth Strategies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Level-wise (original)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        adaptive_lr=False,\n",
    "        # lr_schedule=\"cosine_restart\",  # Original approach\n",
    "        max_depth=6,\n",
    "        tree_learner=\"leaf\",  # Original approach\n",
    "        tree_method=\"binned\",\n",
    "        binned_mode=\"hist\",\n",
    "        verbose=True,\n",
    "        batch_size=1,\n",
    "        use_gpu=False,\n",
    "        use_goss=True,\n",
    "        use_neural=False,  # Original approach\n",
    "        enable_interactions=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "    model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "    level_time = time.time() - start_time\n",
    "    level_pred = model_level.predict(X_test)\n",
    "    level_mse = np.mean((y_test - level_pred) ** 2)\n",
    "    print(f\"   Time: {level_time:.2f}s\")\n",
    "    print(f\"   Test MSE: {level_mse:.6f}\")\n",
    "    print(f\"   Trees: {len(model_level.trees)}\")\n",
    "    \n",
    "    # print feature importances\n",
    "    importances = model_level.feature_importances()\n",
    "    print(\"   Feature Importances:\", importances)\n",
    "    # # plot feature importances like shap\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.bar(range(n_features), importances)\n",
    "    # plt.xlabel(\"Feature\")\n",
    "    # plt.ylabel(\"Importance\")\n",
    "    # plt.title(\"Feature Importances\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # # # # compare with scikit learn's GradientBoostingRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    model_sklearn = GradientBoostingRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        verbose=1,\n",
    "    )\n",
    "    model_sklearn.fit(X_train, y_train)\n",
    "    sklearn_time = time.time() - start_time\n",
    "    sklearn_pred = model_sklearn.predict(X_test)\n",
    "    sklearn_mse = np.mean((y_test - sklearn_pred) ** 2)\n",
    "    print(f\"   Scikit-learn Time: {sklearn_time:.2f}s\")\n",
    "    print(f\"   Scikit-learn Test MSE: {sklearn_mse:.6f}\")\n",
    "    print(f\"   Scikit-learn Trees: {len(model_sklearn.estimators_)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e21da697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Comparing Regressors\n",
      "============================================================\n",
      "Training leaf-wise trees\n",
      "  Loss: mse, Base: -0.0180\n",
      "[  10] Train: 0.676082, Time: 0.18s\n",
      "[  20] Train: 0.172662, Time: 0.32s\n",
      "[  30] Train: 0.066244, Time: 0.47s\n",
      "[  40] Train: 0.038745, Time: 0.58s\n",
      "[  50] Train: 0.029299, Time: 0.67s\n",
      "[  60] Train: 0.027196, Time: 0.74s\n",
      "[  70] Train: 0.025173, Time: 0.85s\n",
      "[  80] Train: 0.026411, Time: 0.98s\n",
      "[  90] Train: 0.024041, Time: 1.16s\n",
      "[ 100] Train: 0.019182, Time: 1.33s\n",
      "[ 110] Train: 0.018191, Time: 1.53s\n",
      "[ 120] Train: 0.017076, Time: 1.69s\n",
      "[ 130] Train: 0.016655, Time: 1.82s\n",
      "[ 140] Train: 0.036784, Time: 2.09s\n",
      "[ 150] Train: 0.037602, Time: 2.36s\n",
      "[ 160] Train: 0.013887, Time: 2.58s\n",
      "[ 170] Train: 0.018052, Time: 2.81s\n",
      "[ 180] Train: 0.020789, Time: 3.16s\n",
      "[ 190] Train: 0.039319, Time: 3.52s\n",
      "[ 200] Train: 0.020572, Time: 3.73s\n",
      "Training completed in 3.73s\n",
      "\n",
      "Model          |   Time (s) |        MSE |        MAE |        RÂ² | Trees\n",
      "----------------------------------------------------------------------------\n",
      "BoostRegressor |      3.733 |   0.035547 |   0.136979 |   0.9887 | 200\n",
      "XGBoost        |      0.182 |   0.038710 |   0.142558 |   0.9877 | 200\n",
      "sklearn-GBR    |      1.544 |   0.040944 |   0.144670 |   0.9869 | 50\n",
      "RandomForest   |      0.256 |   0.159407 |   0.279178 |   0.9491 | 200\n",
      "\n",
      "Top-5 feature importances (if available):\n",
      " - BoostRegressor: f2=0.335, f0=0.327, f1=0.320, f12=0.001, f16=0.001\n",
      " - XGBoost: f2=0.370, f1=0.331, f0=0.285, f19=0.001, f6=0.001\n",
      " - sklearn-GBR: f2=0.343, f0=0.330, f1=0.326, f17=0.000, f4=0.000\n",
      " - RandomForest: f2=0.335, f0=0.335, f1=0.329, f12=0.000, f15=0.000\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Optional dependencies\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    _HAS_XGB = False\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"R2\":  r2_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark(name, model, X_train, y_train, X_test, y_test, get_importance=None):\n",
    "    \"\"\"Fit, time, predict, score, and (optionally) collect feature importance.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time.perf_counter() - t0\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    m = metrics(y_test, y_pred)\n",
    "\n",
    "    importance = None\n",
    "    if get_importance is not None:\n",
    "        try:\n",
    "            importance = get_importance(model)\n",
    "        except Exception:\n",
    "            importance = None\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"time_s\": fit_time,\n",
    "        **m,\n",
    "        \"n_trees\": getattr(model, \"n_estimators\", getattr(model, \"n_estimators_\", None)),\n",
    "        \"feature_importance\": importance,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ----------------- data -----------------\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 20\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "    # Split\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    print(\"ðŸ”¥ Comparing Regressors\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1) Your BoostRegressor (as before)\n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.09,\n",
    "        adaptive_lr=False,\n",
    "        max_depth=6,\n",
    "        tree_learner=\"leaf\",\n",
    "        tree_method=\"binned\",\n",
    "        binned_mode=\"hist\",\n",
    "        verbose=True,\n",
    "        batch_size=1,\n",
    "        use_gpu=False,\n",
    "        use_goss=True,\n",
    "        use_neural=False,\n",
    "        enable_interactions=False,\n",
    "    )\n",
    "    res_boost = benchmark(\n",
    "        \"BoostRegressor\",\n",
    "        model_level,\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        get_importance=lambda m: getattr(m, \"feature_importances\", lambda: None)(),\n",
    "    )\n",
    "    results.append(res_boost)\n",
    "\n",
    "    # 2) Scikit-learn GradientBoostingRegressor (kept for reference)\n",
    "    model_sklearn_gbr = GradientBoostingRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=4,\n",
    "        verbose=0,\n",
    "    )\n",
    "    res_gbr = benchmark(\n",
    "        \"sklearn-GBR\",\n",
    "        model_sklearn_gbr,\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        get_importance=lambda m: getattr(m, \"feature_importances_\", None),\n",
    "    )\n",
    "    results.append(res_gbr)\n",
    "\n",
    "    # 3) Random Forest\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200,          # RF benefits from more trees; adjust if you want strict parity\n",
    "        max_depth=6,\n",
    "        n_jobs=-1,\n",
    "        random_state=4,\n",
    "        verbose=0,\n",
    "    )\n",
    "    res_rf = benchmark(\n",
    "        \"RandomForest\",\n",
    "        rf,\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        get_importance=lambda m: getattr(m, \"feature_importances_\", None),\n",
    "    )\n",
    "    results.append(res_rf)\n",
    "\n",
    "    # 4) XGBoost (if available)\n",
    "    if _HAS_XGB:\n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=200,      # modest; XGB often likes more, but this keeps it snappy\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=1.0,\n",
    "            colsample_bytree=1.0,\n",
    "            reg_lambda=1.0,\n",
    "            tree_method=\"hist\",    # fast, robust default\n",
    "            random_state=4,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        res_xgb = benchmark(\n",
    "            \"XGBoost\",\n",
    "            xgb,\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            get_importance=lambda m: getattr(m, \"feature_importances_\", None),\n",
    "        )\n",
    "        results.append(res_xgb)\n",
    "    else:\n",
    "        print(\"âš ï¸  xgboost not installed; skipping XGBoost comparison.\")\n",
    "\n",
    "    # ----------------- pretty print -----------------\n",
    "    # Sort by MSE ascending\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"MSE\"])\n",
    "    w = max(len(r[\"name\"]) for r in results_sorted)\n",
    "    print(f\"\\n{'Model'.ljust(w)} |   Time (s) |        MSE |        MAE |        RÂ² | Trees\")\n",
    "    print(\"-\" * (w + 62))\n",
    "    for r in results_sorted:\n",
    "        trees = r[\"n_trees\"] if r[\"n_trees\"] is not None else \"-\"\n",
    "        print(\n",
    "            f\"{r['name'].ljust(w)} | {r['time_s']:10.3f} | {r['MSE']:10.6f} | \"\n",
    "            f\"{r['MAE']:10.6f} | {r['R2']:8.4f} | {trees}\"\n",
    "        )\n",
    "\n",
    "    # ----------------- feature importances (quick glance) -----------------\n",
    "    # Show top-5 importances for each model that provides them\n",
    "    print(\"\\nTop-5 feature importances (if available):\")\n",
    "    for r in results_sorted:\n",
    "        imp = r[\"feature_importance\"]\n",
    "        if imp is None:\n",
    "            print(f\" - {r['name']}: (no importances)\")\n",
    "            continue\n",
    "        # Ensure numpy array\n",
    "        imp = np.asarray(imp)\n",
    "        top_idx = np.argsort(imp)[::-1][:5]\n",
    "        pairs = \", \".join([f\"f{j}={imp[j]:.3f}\" for j in top_idx])\n",
    "        print(f\" - {r['name']}: {pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa06419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539a0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceecab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training with Leaf-wise trees (batch_size=1\n",
      "   Objective: reg:squarederror, Loss: mse, Base score: -0.0180\n",
      "   NODE layers disabled.\n",
      "   GOSS: top_rate=0.2, other_rate=0.1\n",
      "   DART: rate_drop=0.1, skip_drop=0.5, normalize_type=tree, one_drop=no\n",
      "   Binning: hist (hist), 256 bins\n",
      "   Training on 4000 samples with 20 features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tree_method must be 'binned' or 'exact'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      3\u001b[39m BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n\u001b[32m      5\u001b[39m model_level = BoostRegressor(\n\u001b[32m      6\u001b[39m     n_estimators=\u001b[32m50\u001b[39m,\n\u001b[32m      7\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     batch_size=\u001b[32m1\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmodel_level\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m level_time = time.time() - start_time\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Set background for proper expected value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foretree/boosting.py:843\u001b[39m, in \u001b[36mBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, eval_set, verbose)\u001b[39m\n\u001b[32m    840\u001b[39m grad, hess = \u001b[38;5;28mself\u001b[39m.loss_fn.grad_hess(y, y_pred_iter)\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# 3) Build a batch of trees from (X, grad, hess)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m trees = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_tree_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;66;03m# 4) LR for this round; store per-tree LRs\u001b[39;00m\n\u001b[32m    846\u001b[39m current_lr = \u001b[38;5;28mself\u001b[39m._get_learning_rate(iteration)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foretree/boosting.py:712\u001b[39m, in \u001b[36mBoostRegressor._build_tree_batch\u001b[39m\u001b[34m(self, X, y, grad, hess)\u001b[39m\n\u001b[32m    710\u001b[39m n_features_tree = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.sampling_config.colsample_bytree * n_features))\n\u001b[32m    711\u001b[39m feature_mask = \u001b[38;5;28mself\u001b[39m._rng.choice(n_features, size=n_features_tree, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m tree = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[38;5;66;03m# ---------------- Build histograms (FAST) or fallback (DENSE) ----------------\u001b[39;00m\n\u001b[32m    715\u001b[39m tree._histogram_system = \u001b[38;5;28mself\u001b[39m._histogram_system.clone(feature_idx=feature_mask, row_idx=selected_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foretree/boosting.py:550\u001b[39m, in \u001b[36mBoostRegressor._create_tree\u001b[39m\u001b[34m(self, feature_mask)\u001b[39m\n\u001b[32m    529\u001b[39m common_args = {\n\u001b[32m    530\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.tree_config.max_depth,\n\u001b[32m    531\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmin_samples_split\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.tree_config.min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33menable_interactions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.enable_interactions,\n\u001b[32m    548\u001b[39m }\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tree_learner == \u001b[33m\"\u001b[39m\u001b[33mleaf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnifiedTree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_leaves\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_leaves\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcommon_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrowth_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleaf_wise\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UnifiedTree(\n\u001b[32m    557\u001b[39m         max_leaves=\u001b[38;5;28mself\u001b[39m.tree_config.max_leaves,\n\u001b[32m    558\u001b[39m         **common_args,\n\u001b[32m    559\u001b[39m         growth_policy=\u001b[33m\"\u001b[39m\u001b[33mlevel_wise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    560\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foretree/boosting_tree.py:65\u001b[39m, in \u001b[36mUnifiedTree.__init__\u001b[39m\u001b[34m(self, growth_policy, max_depth, max_leaves, min_samples_split, min_samples_leaf, min_child_weight, lambda_, gamma, alpha, max_delta_step, tree_method, binned_mode, n_bins, feature_indices, bin_edges, monotone_constraints, interaction_constraints, gpu_accelerator, n_jobs, adaptive_hist, use_gpu, feature_importance_, enable_interactions)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgrowth_policy must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mleaf_wise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel_wise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tree_method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinned\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexact\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtree_method must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbinned\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexact\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tree_method == \u001b[33m\"\u001b[39m\u001b[33mbinned\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m binned_mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapprox\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33madaptive\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     70\u001b[39m ):\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbinned_mode must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mapprox\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33madaptive\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: tree_method must be 'binned' or 'exact'"
     ]
    }
   ],
   "source": [
    "from shap import *\n",
    "\n",
    "BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "model_level = BoostRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    tree_learner=\"leaf\",  # Original approach\n",
    "    tree_method=\"hist\",\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "level_time = time.time() - start_time\n",
    "# Set background for proper expected value\n",
    "model_level.set_shap_background(X_train[:100])  # Use sample of training data\n",
    "\n",
    "# Compute SHAP values (should have much lower additivity errors)\n",
    "shap_values = model_level.shap_values(X_test[:10], debug=True)\n",
    "\n",
    "# Validate the fix\n",
    "# Explain individual predictions\n",
    "explanation = model_level.explain_prediction(X_test[0])\n",
    "# Get feature importance\n",
    "#importance = model_level.shap_feature_importance(X_test[:100])\n",
    "#print(\"Feature Importance:\", importance)\n",
    "\n",
    "# Analyze model behavior\n",
    "#shap_values = model_level.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SHAP to your model\n",
    "from your_corrected_shap import add_shap_to_boostregressor\n",
    "\n",
    "add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "# Train model\n",
    "model = BoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
