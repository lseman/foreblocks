{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Comparing Tree Growth Strategies\n",
      "==================================================\n",
      "ðŸš€ Training with Level-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.615273, Val: 0.688379, Time: 0.38s\n",
      "[  20] Train: 0.116879, Val: 0.155410, Time: 0.76s\n",
      "[  30] Train: 0.044363, Val: 0.071079, Time: 1.08s\n",
      "[  40] Train: 0.027127, Val: 0.048741, Time: 1.39s\n",
      "[  50] Train: 0.034554, Val: 0.049874, Time: 1.70s\n",
      "âœ… Training completed in 1.70s, 50 trees\n",
      "   Time: 1.70s\n",
      "   Test MSE: 0.045364\n",
      "   Trees: 50\n",
      "   Feature Importances: [3.46952902e-01 3.38097412e-01 3.12128474e-01 4.71444334e-04\n",
      " 3.84886108e-04 3.56998600e-04 4.50378622e-04 5.66395126e-04\n",
      " 1.44903493e-04 4.46205560e-04]\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "# ================ USAGE EXAMPLE ================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 10\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(\"ðŸ”¥ Comparing Tree Growth Strategies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Level-wise (original)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        tree_learner=\"level\",  # Original approach\n",
    "        tree_method=\"binned\",\n",
    "        binned_mode=\"hist\",\n",
    "        verbose=True,\n",
    "        batch_size=1,\n",
    "        use_gpu=False,\n",
    "        adaptive_hist=False,\n",
    "        use_goss=True,\n",
    "        use_neural=False,  # Original approach\n",
    "    )\n",
    "\n",
    "    \n",
    "    model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "    level_time = time.time() - start_time\n",
    "    level_pred = model_level.predict(X_test)\n",
    "    level_mse = np.mean((y_test - level_pred) ** 2)\n",
    "    print(f\"   Time: {level_time:.2f}s\")\n",
    "    print(f\"   Test MSE: {level_mse:.6f}\")\n",
    "    print(f\"   Trees: {len(model_level.trees)}\")\n",
    "    \n",
    "    # print feature importances\n",
    "    importances = model_level.feature_importances()\n",
    "    print(\"   Feature Importances:\", importances)\n",
    "    \n",
    "    # # # # compare with scikit learn's GradientBoostingRegressor\n",
    "    # from sklearn.ensemble import GradientBoostingRegressor\n",
    "    # model_sklearn = GradientBoostingRegressor(\n",
    "    #     n_estimators=50,\n",
    "    #     learning_rate=0.1,\n",
    "    #     max_depth=6,\n",
    "    #     verbose=1,\n",
    "    #     random_state=4#\n",
    "    # )\n",
    "    # model_sklearn.fit(X_train, y_train)\n",
    "    # sklearn_time = time.time() - start_time\n",
    "    # sklearn_pred = model_sklearn.predict(X_test)\n",
    "    # sklearn_mse = np.mean((y_test - sklearn_pred) ** 2)\n",
    "    # print(f\"   Scikit-learn Time: {sklearn_time:.2f}s\")\n",
    "    # print(f\"   Scikit-learn Test MSE: {sklearn_mse:.6f}\")\n",
    "    # print(f\"   Scikit-learn Trees: {len(model_sklearn.estimators_)}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceecab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.560007, Val: 0.635823, Time: 0.12s\n",
      "[  20] Train: 0.131810, Val: 0.169781, Time: 0.25s\n",
      "[  30] Train: 0.048517, Val: 0.074908, Time: 0.34s\n",
      "[  40] Train: 0.031832, Val: 0.053086, Time: 0.42s\n",
      "[  50] Train: 0.046573, Val: 0.068918, Time: 0.51s\n",
      "âœ… Training completed in 0.51s, 50 trees\n",
      "[TreeSHAP] Computing SHAP for 10 samples, 10 features, 50 trees\n",
      "  Sample 0: Adjusting SHAP sum from 0.31848894 to 0.43675774\n",
      "  Sample 1: Adjusting SHAP sum from 1.29052109 to 1.40878989\n",
      "  Sample 2: Adjusting SHAP sum from 1.59079794 to 1.70906674\n",
      "  Sample 3: Adjusting SHAP sum from 0.28618006 to 0.40444887\n",
      "  Sample 4: Adjusting SHAP sum from -0.07780282 to -0.09129796\n",
      "  Sample 5: Adjusting SHAP sum from 2.88544908 to 3.00371789\n",
      "  Sample 6: Adjusting SHAP sum from -1.20012455 to -1.14608999\n",
      "  Sample 7: Adjusting SHAP sum from -1.71232373 to -1.61104609\n",
      "  Sample 8: Adjusting SHAP sum from -2.47165796 to -2.42312511\n",
      "  Sample 9: Adjusting SHAP sum from -2.09803122 to -1.97976242\n",
      "[TreeSHAP] Final additivity: max_err=4.441e-16, mean_err=9.437e-17\n",
      "Prediction: 0.369691 = E[f(X)] -0.067066 + sum(phi) 0.436758\n",
      "Additivity error: 0.00000000\n",
      "Top Feature Contributions:\n",
      "------------------------------------------------------------\n",
      " 1. feature_1         :  +0.494270 â†‘ (x=1.0127)\n",
      " 2. feature_2         :  -0.063998 â†“ (x=-0.198187)\n",
      " 3. feature_3         :  +0.022370 â†‘ (x=0.0905693)\n",
      " 4. feature_0         :  -0.018751 â†“ (x=-0.471858)\n",
      " 5. feature_7         :  +0.014847 â†‘ (x=1.04059)\n",
      " 6. feature_5         :  -0.013096 â†“ (x=-0.0589634)\n",
      " 7. feature_6         :  +0.004192 â†‘ (x=-1.81785)\n",
      " 8. feature_9         :  -0.003888 â†“ (x=-1.83112)\n",
      " 9. feature_4         :  +0.002343 â†‘ (x=0.717391)\n",
      "10. feature_8         :  -0.001530 â†“ (x=1.25493)\n"
     ]
    }
   ],
   "source": [
    "from shap import *\n",
    "\n",
    "BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "model_level = BoostRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    tree_learner=\"leaf\",  # Original approach\n",
    "    tree_method=\"hist\",\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "level_time = time.time() - start_time\n",
    "# Set background for proper expected value\n",
    "model_level.set_shap_background(X_train[:100])  # Use sample of training data\n",
    "\n",
    "# Compute SHAP values (should have much lower additivity errors)\n",
    "shap_values = model_level.shap_values(X_test[:10], debug=True)\n",
    "\n",
    "# Validate the fix\n",
    "# Explain individual predictions\n",
    "explanation = model_level.explain_prediction(X_test[0])\n",
    "# Get feature importance\n",
    "#importance = model_level.shap_feature_importance(X_test[:100])\n",
    "#print(\"Feature Importance:\", importance)\n",
    "\n",
    "# Analyze model behavior\n",
    "#shap_values = model_level.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cc7cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_corrected_shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add SHAP to your model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myour_corrected_shap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_shap_to_boostregressor\n\u001b[32m      4\u001b[39m add_shap_to_boostregressor(BoostRegressor)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'your_corrected_shap'"
     ]
    }
   ],
   "source": [
    "# Add SHAP to your model\n",
    "from your_corrected_shap import add_shap_to_boostregressor\n",
    "\n",
    "add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "# Train model\n",
    "model = BoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
