{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db3ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Comparing Tree Growth Strategies\n",
      "==================================================\n",
      "\n",
      "1️⃣ Level-wise Tree Growth:\n",
      "🚀 Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.596217, Val: 0.680102, Time: 0.04s\n",
      "[  20] Train: 0.130052, Val: 0.176376, Time: 0.08s\n",
      "[  30] Train: 0.041564, Val: 0.070371, Time: 0.12s\n",
      "[  40] Train: 0.085109, Val: 0.117890, Time: 0.15s\n",
      "[  50] Train: 0.022351, Val: 0.041749, Time: 0.18s\n",
      "✅ Training completed in 0.18s, 50 trees\n",
      "   Time: 0.18s\n",
      "   Test MSE: 0.041749\n",
      "   Trees: 50\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "# ================ USAGE EXAMPLE ================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 10\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(\"🔥 Comparing Tree Growth Strategies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Level-wise (original)\n",
    "    print(\"\\n1️⃣ Level-wise Tree Growth:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        tree_learner=\"leaf\",  # Original approach\n",
    "        tree_method=\"exact\",\n",
    "        verbose=True,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # compare with scikit learn's GradientBoostingRegressor\n",
    "    # from sklearn.ensemble import GradientBoostingRegressor\n",
    "    # model_sklearn = GradientBoostingRegressor(\n",
    "    #     n_estimators=50,\n",
    "    #     learning_rate=0.1,\n",
    "    #     max_depth=6,\n",
    "    #     verbose=1,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "    # model_sklearn.fit(X_train, y_train)\n",
    "    # sklearn_time = time.time() - start_time\n",
    "    # sklearn_pred = model_sklearn.predict(X_test)\n",
    "    # sklearn_mse = np.mean((y_test - sklearn_pred) ** 2)\n",
    "    # print(f\"   Scikit-learn Time: {sklearn_time:.2f}s\")\n",
    "    # print(f\"   Scikit-learn Test MSE: {sklearn_mse:.6f}\")\n",
    "    # print(f\"   Scikit-learn Trees: {len(model_sklearn.estimators_)}\")\n",
    "    \n",
    "    model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "    level_time = time.time() - start_time\n",
    "    level_pred = model_level.predict(X_test)\n",
    "    level_mse = np.mean((y_test - level_pred) ** 2)\n",
    "    \n",
    "    print(f\"   Time: {level_time:.2f}s\")\n",
    "    print(f\"   Test MSE: {level_mse:.6f}\")\n",
    "    print(f\"   Trees: {len(model_level.trees)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Refined Multi-Level Binning Results ===\n",
      "\n",
      "=== Exponential (skewed) Feature ===\n",
      "Unique values: 10000\n",
      "Uniform bins (target 128): 129\n",
      "Adaptive bins: 129\n",
      "Refinement regions: 1\n",
      "Promising regions:\n",
      "  Bins 16-17: gain = 17.8097\n",
      "Efficiency ratio: 1.00 (>1 means fewer bins)\n",
      "\n",
      "=== Uniform Feature ===\n",
      "Unique values: 10000\n",
      "Uniform bins (target 128): 129\n",
      "Adaptive bins: 97\n",
      "Refinement regions: 1\n",
      "Promising regions:\n",
      "  Bins 5-17: gain = 39108.9701\n",
      "Efficiency ratio: 1.33 (>1 means fewer bins)\n",
      "\n",
      "=== Categorical Feature ===\n",
      "Unique values: 5\n",
      "Uniform bins (target 128): 5\n",
      "Adaptive bins: 5\n",
      "Refinement regions: 0\n",
      "Efficiency ratio: 1.00 (>1 means fewer bins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "from typing import Tuple, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@njit\n",
    "def _build_histogram_numba(values: np.ndarray, \n",
    "                          gradients: np.ndarray, \n",
    "                          hessians: np.ndarray,\n",
    "                          bin_edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Standalone numba function for building histograms.\"\"\"\n",
    "    n_bins = len(bin_edges) - 1\n",
    "    hist_g = np.zeros(n_bins, dtype=np.float64)\n",
    "    hist_h = np.zeros(n_bins, dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        val = values[i]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "            \n",
    "        # Find bin using binary search\n",
    "        bin_idx = np.searchsorted(bin_edges[1:], val)\n",
    "        if bin_idx < n_bins:\n",
    "            hist_g[bin_idx] += gradients[i]\n",
    "            hist_h[bin_idx] += hessians[i]\n",
    "    \n",
    "    return hist_g, hist_h\n",
    "\n",
    "\n",
    "@njit\n",
    "def _find_split_gains_numba(hist_g: np.ndarray, hist_h: np.ndarray, \n",
    "                           lambda_reg: float, gamma: float) -> np.ndarray:\n",
    "    \"\"\"Numba-optimized gain calculation for all split points.\"\"\"\n",
    "    n_bins = len(hist_g)\n",
    "    gains = np.full(n_bins - 1, -np.inf, dtype=np.float64)\n",
    "    \n",
    "    # Calculate cumulative sums\n",
    "    cum_g = np.cumsum(hist_g)\n",
    "    cum_h = np.cumsum(hist_h)\n",
    "    total_g = cum_g[-1]\n",
    "    total_h = cum_h[-1]\n",
    "    \n",
    "    if total_h <= 0:\n",
    "        return gains\n",
    "    \n",
    "    # Parent score\n",
    "    parent_score = total_g * total_g / (total_h + lambda_reg)\n",
    "    \n",
    "    # Calculate gains for all split points\n",
    "    for i in range(n_bins - 1):\n",
    "        g_left = cum_g[i]\n",
    "        h_left = cum_h[i]\n",
    "        g_right = total_g - g_left\n",
    "        h_right = total_h - h_left\n",
    "        \n",
    "        if h_left > 0 and h_right > 0:\n",
    "            left_score = g_left * g_left / (h_left + lambda_reg)\n",
    "            right_score = g_right * g_right / (h_right + lambda_reg)\n",
    "            gain = 0.5 * (left_score + right_score - parent_score) - gamma\n",
    "            gains[i] = gain\n",
    "    \n",
    "    return gains\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BinningConfig:\n",
    "    \"\"\"Configuration for multi-level binning strategy.\"\"\"\n",
    "    coarse_bins: int = 32          # Reduced from 64 for better efficiency\n",
    "    max_total_bins: int = 128      # Hard cap on total bins\n",
    "    top_regions: int = 3           # Maximum regions to refine\n",
    "    min_region_samples: int = 50   # Minimum samples to justify refinement\n",
    "    min_gain_threshold: float = 0.1  # Skip refinement below this gain\n",
    "    overlap_merge_threshold: int = 2  # Merge regions if they overlap by this many bins\n",
    "\n",
    "\n",
    "class MultiLevelBinner:\n",
    "    \"\"\"\n",
    "    Refined multi-level binning that prevents bin explosion and overlapping regions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BinningConfig = None):\n",
    "        self.config = config or BinningConfig()\n",
    "        \n",
    "    def create_adaptive_bins(self, \n",
    "                           feature_values: np.ndarray, \n",
    "                           gradients: np.ndarray,\n",
    "                           hessians: np.ndarray,\n",
    "                           lambda_reg: float = 1.0,\n",
    "                           gamma: float = 0.0) -> Tuple[np.ndarray, List[Tuple[int, int, float]]]:\n",
    "        \"\"\"\n",
    "        Create multi-level bins with proper constraints to prevent bin explosion.\n",
    "        \"\"\"\n",
    "        n_samples = len(feature_values)\n",
    "        finite_values = feature_values[np.isfinite(feature_values)]\n",
    "        n_unique = len(np.unique(finite_values))\n",
    "        \n",
    "        # Early exit conditions\n",
    "        if (n_samples < self.config.min_region_samples * 2 or \n",
    "            n_unique <= self.config.coarse_bins):\n",
    "            # Use uniform binning for simple cases\n",
    "            uniform_bins = self._create_uniform_bins(finite_values, min(n_unique, self.config.max_total_bins))\n",
    "            return uniform_bins, []\n",
    "        \n",
    "        # Step 1: Create coarse bins\n",
    "        coarse_edges = self._create_uniform_bins(finite_values, self.config.coarse_bins)\n",
    "        \n",
    "        # Step 2: Build coarse histogram\n",
    "        coarse_hist_g, coarse_hist_h = self._build_histogram(\n",
    "            feature_values, gradients, hessians, coarse_edges\n",
    "        )\n",
    "        \n",
    "        # Step 3: Find and merge promising regions\n",
    "        regions = self._find_promising_regions_fixed(\n",
    "            coarse_hist_g, coarse_hist_h, coarse_edges, lambda_reg, gamma\n",
    "        )\n",
    "        \n",
    "        if not regions:\n",
    "            return coarse_edges, []\n",
    "        \n",
    "        # Step 4: Create refined bins with budget constraints\n",
    "        refined_edges = self._create_refined_bins_with_budget(\n",
    "            finite_values, coarse_edges, regions\n",
    "        )\n",
    "        \n",
    "        return refined_edges, regions\n",
    "    \n",
    "    def _create_uniform_bins(self, values: np.ndarray, n_bins: int) -> np.ndarray:\n",
    "        \"\"\"Create uniform quantile-based bins.\"\"\"\n",
    "        if len(values) == 0:\n",
    "            return np.array([0.0, 1.0])\n",
    "        \n",
    "        unique_vals = np.unique(values)\n",
    "        if len(unique_vals) <= n_bins:\n",
    "            return unique_vals\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_edges = np.quantile(unique_vals, quantiles)\n",
    "        return np.unique(bin_edges)\n",
    "    \n",
    "    def _build_histogram(self, values: np.ndarray, \n",
    "                        gradients: np.ndarray, \n",
    "                        hessians: np.ndarray,\n",
    "                        bin_edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Build gradient/hessian histograms.\"\"\"\n",
    "        return _build_histogram_numba(values, gradients, hessians, bin_edges)\n",
    "    \n",
    "    def _find_promising_regions_fixed(self, \n",
    "                                    hist_g: np.ndarray,\n",
    "                                    hist_h: np.ndarray, \n",
    "                                    bin_edges: np.ndarray,\n",
    "                                    lambda_reg: float,\n",
    "                                    gamma: float) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Find promising regions with proper overlap handling.\"\"\"\n",
    "        \n",
    "        # Get gains for all split points\n",
    "        gains = _find_split_gains_numba(hist_g, hist_h, lambda_reg, gamma)\n",
    "        \n",
    "        # Find peaks above threshold\n",
    "        valid_gains = gains[gains > self.config.min_gain_threshold]\n",
    "        if len(valid_gains) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Get top split points\n",
    "        top_indices = np.argsort(gains)[-self.config.top_regions * 3:]  # Get more candidates\n",
    "        top_indices = top_indices[gains[top_indices] > self.config.min_gain_threshold]\n",
    "        \n",
    "        if len(top_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Sort by index to process left to right\n",
    "        top_indices = np.sort(top_indices)\n",
    "        \n",
    "        # Merge overlapping regions\n",
    "        merged_regions = []\n",
    "        current_start = top_indices[0]\n",
    "        current_end = top_indices[0] + 1\n",
    "        current_max_gain = gains[top_indices[0]]\n",
    "        \n",
    "        for i in range(1, len(top_indices)):\n",
    "            idx = top_indices[i]\n",
    "            \n",
    "            # Check if this region overlaps with current\n",
    "            if idx <= current_end + self.config.overlap_merge_threshold:\n",
    "                # Extend current region\n",
    "                current_end = max(current_end, idx + 1)\n",
    "                current_max_gain = max(current_max_gain, gains[idx])\n",
    "            else:\n",
    "                # Save current region and start new one\n",
    "                if current_max_gain > self.config.min_gain_threshold:\n",
    "                    merged_regions.append((current_start, current_end, current_max_gain))\n",
    "                current_start = idx\n",
    "                current_end = idx + 1\n",
    "                current_max_gain = gains[idx]\n",
    "        \n",
    "        # Don't forget the last region\n",
    "        if current_max_gain > self.config.min_gain_threshold:\n",
    "            merged_regions.append((current_start, current_end, current_max_gain))\n",
    "        \n",
    "        # Sort by gain and take top regions\n",
    "        merged_regions.sort(key=lambda x: x[2], reverse=True)\n",
    "        return merged_regions[:self.config.top_regions]\n",
    "    \n",
    "    def _create_refined_bins_with_budget(self, \n",
    "                                       values: np.ndarray,\n",
    "                                       coarse_edges: np.ndarray, \n",
    "                                       regions: List[Tuple[int, int, float]]) -> np.ndarray:\n",
    "        \"\"\"Create refined bins while respecting the total bin budget.\"\"\"\n",
    "        \n",
    "        # Start with coarse edges\n",
    "        all_edges = set(coarse_edges)\n",
    "        coarse_bins_used = len(coarse_edges)\n",
    "        \n",
    "        # Calculate remaining budget\n",
    "        remaining_budget = self.config.max_total_bins - coarse_bins_used\n",
    "        if remaining_budget <= 0:\n",
    "            return coarse_edges\n",
    "        \n",
    "        # Distribute budget among regions based on their gain\n",
    "        total_gain = sum(gain for _, _, gain in regions)\n",
    "        if total_gain <= 0:\n",
    "            return coarse_edges\n",
    "        \n",
    "        for start_idx, end_idx, gain in regions:\n",
    "            if remaining_budget <= 0:\n",
    "                break\n",
    "                \n",
    "            # Allocate budget proportional to gain\n",
    "            region_budget = max(4, int(remaining_budget * gain / total_gain))\n",
    "            region_budget = min(region_budget, remaining_budget)\n",
    "            \n",
    "            # Get region boundaries\n",
    "            region_start = coarse_edges[start_idx]\n",
    "            region_end = coarse_edges[min(end_idx + 1, len(coarse_edges) - 1)]\n",
    "            \n",
    "            # Find values in this region\n",
    "            in_region = (values >= region_start) & (values <= region_end)\n",
    "            region_values = values[in_region]\n",
    "            \n",
    "            if len(region_values) >= self.config.min_region_samples:\n",
    "                # Create fine bins in this region\n",
    "                region_edges = self._create_uniform_bins(region_values, region_budget)\n",
    "                \n",
    "                # Only add edges that are truly new\n",
    "                new_edges = set(region_edges) - all_edges\n",
    "                if len(new_edges) > 0:\n",
    "                    all_edges.update(new_edges)\n",
    "                    remaining_budget -= len(new_edges)\n",
    "        \n",
    "        # Return sorted unique edges\n",
    "        return np.array(sorted(all_edges))\n",
    "\n",
    "\n",
    "class AdaptiveMultiLevelBinner(MultiLevelBinner):\n",
    "    \"\"\"\n",
    "    Version that adapts binning parameters based on feature characteristics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_adaptive_bins(self, \n",
    "                           feature_values: np.ndarray,\n",
    "                           gradients: np.ndarray,\n",
    "                           hessians: np.ndarray,\n",
    "                           feature_idx: int = None,\n",
    "                           lambda_reg: float = 1.0,\n",
    "                           gamma: float = 0.0) -> Tuple[np.ndarray, List[Tuple[int, int, float]]]:\n",
    "        \"\"\"\n",
    "        Adaptive binning that adjusts strategy based on feature characteristics.\n",
    "        \"\"\"\n",
    "        # Analyze feature\n",
    "        stats = self._analyze_feature(feature_values, gradients)\n",
    "        \n",
    "        # Adapt configuration\n",
    "        adapted_config = self._adapt_config(stats)\n",
    "        \n",
    "        # Temporarily use adapted config\n",
    "        original_config = self.config\n",
    "        self.config = adapted_config\n",
    "        \n",
    "        try:\n",
    "            result = super().create_adaptive_bins(\n",
    "                feature_values, gradients, hessians, lambda_reg, gamma\n",
    "            )\n",
    "        finally:\n",
    "            self.config = original_config\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _analyze_feature(self, values: np.ndarray, gradients: np.ndarray) -> dict:\n",
    "        \"\"\"Analyze feature characteristics.\"\"\"\n",
    "        finite_vals = values[np.isfinite(values)]\n",
    "        finite_grads = gradients[np.isfinite(values)]\n",
    "        \n",
    "        if len(finite_vals) < 10:\n",
    "            return {'n_unique': len(finite_vals), 'correlation': 0.0, 'skewness': 0.0}\n",
    "        \n",
    "        stats = {\n",
    "            'n_unique': len(np.unique(finite_vals)),\n",
    "            'n_samples': len(finite_vals),\n",
    "            'missing_rate': 1.0 - len(finite_vals) / len(values),\n",
    "        }\n",
    "        \n",
    "        # Calculate correlation safely\n",
    "        if np.std(finite_vals) > 1e-8 and np.std(finite_grads) > 1e-8:\n",
    "            stats['correlation'] = np.corrcoef(finite_vals, finite_grads)[0, 1]\n",
    "        else:\n",
    "            stats['correlation'] = 0.0\n",
    "        \n",
    "        # Calculate skewness\n",
    "        if len(finite_vals) > 2:\n",
    "            mean_val = np.mean(finite_vals)\n",
    "            std_val = np.std(finite_vals)\n",
    "            if std_val > 1e-8:\n",
    "                stats['skewness'] = np.mean(((finite_vals - mean_val) / std_val) ** 3)\n",
    "            else:\n",
    "                stats['skewness'] = 0.0\n",
    "        else:\n",
    "            stats['skewness'] = 0.0\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _adapt_config(self, stats: dict) -> BinningConfig:\n",
    "        \"\"\"Adapt binning configuration based on feature statistics.\"\"\"\n",
    "        config = BinningConfig()\n",
    "        \n",
    "        # For categorical-like features (few unique values)\n",
    "        if stats['n_unique'] < 20:\n",
    "            config.coarse_bins = min(16, stats['n_unique'])\n",
    "            config.max_total_bins = min(32, stats['n_unique'] * 2)\n",
    "            config.top_regions = 1\n",
    "            config.min_gain_threshold = 0.01\n",
    "        \n",
    "        # For features with strong gradient correlation\n",
    "        elif abs(stats.get('correlation', 0)) > 0.3:\n",
    "            config.coarse_bins = 24\n",
    "            config.max_total_bins = 96\n",
    "            config.top_regions = 4\n",
    "            config.min_gain_threshold = 0.05\n",
    "        \n",
    "        # For highly skewed features\n",
    "        elif abs(stats.get('skewness', 0)) > 2:\n",
    "            config.coarse_bins = 20\n",
    "            config.max_total_bins = 80\n",
    "            config.top_regions = 2  # Focus on fewer regions\n",
    "            config.min_gain_threshold = 0.1\n",
    "        \n",
    "        # Default case - balanced approach\n",
    "        else:\n",
    "            config.coarse_bins = 32\n",
    "            config.max_total_bins = 128\n",
    "            config.top_regions = 3\n",
    "            config.min_gain_threshold = 0.1\n",
    "        \n",
    "        return config\n",
    "\n",
    "\n",
    "def demonstrate_multilevel_binning():\n",
    "    \"\"\"Demonstrate the refined multi-level binning approach.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create more realistic test data\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # Exponential feature (skewed)\n",
    "    feature1 = np.random.exponential(2, n_samples)\n",
    "    \n",
    "    # Uniform feature\n",
    "    feature2 = np.random.uniform(0, 100, n_samples)\n",
    "    \n",
    "    # Categorical-like feature\n",
    "    feature3 = np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.5, 0.2, 0.15, 0.1, 0.05])\n",
    "    \n",
    "    # Create more realistic gradients (smaller values, less extreme correlation)\n",
    "    gradients = (0.1 * np.log(feature1 + 1) + \n",
    "                 0.05 * feature2 + \n",
    "                 0.2 * feature3 + \n",
    "                 np.random.normal(0, 0.5, n_samples))\n",
    "    \n",
    "    hessians = np.random.uniform(0.1, 0.3, n_samples)  # Smaller hessian values\n",
    "    \n",
    "    # Test refined binning strategies\n",
    "    binner = AdaptiveMultiLevelBinner()\n",
    "    \n",
    "    features = [feature1, feature2, feature3]\n",
    "    feature_names = ['Exponential (skewed)', 'Uniform', 'Categorical']\n",
    "    \n",
    "    print(\"=== Refined Multi-Level Binning Results ===\\n\")\n",
    "    \n",
    "    for i, (feature, name) in enumerate(zip(features, feature_names)):\n",
    "        print(f\"=== {name} Feature ===\")\n",
    "        print(f\"Unique values: {len(np.unique(feature[np.isfinite(feature)]))}\")\n",
    "        \n",
    "        # Standard uniform binning\n",
    "        uniform_bins = binner._create_uniform_bins(feature[np.isfinite(feature)], 128)\n",
    "        print(f\"Uniform bins (target 128): {len(uniform_bins)}\")\n",
    "        \n",
    "        # Multi-level adaptive binning\n",
    "        adaptive_bins, regions = binner.create_adaptive_bins(\n",
    "            feature, gradients, hessians, feature_idx=i\n",
    "        )\n",
    "        print(f\"Adaptive bins: {len(adaptive_bins)}\")\n",
    "        print(f\"Refinement regions: {len(regions)}\")\n",
    "        \n",
    "        if regions:\n",
    "            print(\"Promising regions:\")\n",
    "            for start, end, gain in regions:\n",
    "                print(f\"  Bins {start}-{end}: gain = {gain:.4f}\")\n",
    "        \n",
    "        # Show efficiency gain\n",
    "        efficiency = len(uniform_bins) / len(adaptive_bins) if len(adaptive_bins) > 0 else 0\n",
    "        print(f\"Efficiency ratio: {efficiency:.2f} (>1 means fewer bins)\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_multilevel_binning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670f63d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare_exact_methods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboosting_tree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcompare_exact_methods\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'compare_exact_methods' is not defined"
     ]
    }
   ],
   "source": [
    "from boosting_tree import *\n",
    "compare_exact_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.624518, Val: 0.696365, Time: 0.04s\n",
      "[  20] Train: 0.178994, Val: 0.224378, Time: 0.08s\n",
      "[  30] Train: 0.052540, Val: 0.085379, Time: 0.11s\n",
      "[  40] Train: 0.031857, Val: 0.056084, Time: 0.13s\n",
      "[  50] Train: 0.026562, Val: 0.050178, Time: 0.15s\n",
      "✅ Training completed in 0.15s, 50 trees\n",
      "[TreeSHAP] Computing SHAP for 10 samples, 10 features, 50 trees\n",
      "  Sample 0: Adjusting SHAP sum from 0.30939024 to 0.33801284\n",
      "  Sample 1: Adjusting SHAP sum from 1.23546252 to 1.26408512\n",
      "  Sample 2: Adjusting SHAP sum from 1.61465677 to 1.64327937\n",
      "  Sample 3: Adjusting SHAP sum from 0.49134412 to 0.51996672\n",
      "  Sample 4: Adjusting SHAP sum from -0.03185380 to -0.00323120\n",
      "  Sample 5: Adjusting SHAP sum from 3.24775689 to 3.27637949\n",
      "  Sample 6: Adjusting SHAP sum from -1.02974121 to -1.00111861\n",
      "  Sample 7: Adjusting SHAP sum from -1.83947136 to -1.81084876\n",
      "  Sample 8: Adjusting SHAP sum from -2.52004928 to -2.49142668\n",
      "  Sample 9: Adjusting SHAP sum from -2.00285394 to -1.97423134\n",
      "[TreeSHAP] Final additivity: max_err=4.441e-16, mean_err=1.443e-16\n",
      "Prediction: 0.309342 = E[f(X)] -0.028670 + sum(phi) 0.338013\n",
      "Additivity error: 0.00000000\n",
      "Top Feature Contributions:\n",
      "------------------------------------------------------------\n",
      " 1. feature_2         :  +0.153448 ↑ (x=-0.198187)\n",
      " 2. feature_1         :  +0.135565 ↑ (x=1.0127)\n",
      " 3. feature_0         :  +0.031411 ↑ (x=-0.471858)\n",
      " 4. feature_4         :  +0.012296 ↑ (x=0.717391)\n",
      " 5. feature_6         :  +0.006370 ↑ (x=-1.81785)\n",
      " 6. feature_7         :  -0.001871 ↓ (x=1.04059)\n",
      " 7. feature_9         :  +0.001656 ↑ (x=-1.83112)\n",
      " 8. feature_8         :  -0.000513 ↓ (x=1.25493)\n",
      " 9. feature_3         :  -0.000297 ↓ (x=0.0905693)\n",
      "10. feature_5         :  -0.000053 ↓ (x=-0.0589634)\n"
     ]
    }
   ],
   "source": [
    "from shap import *\n",
    "\n",
    "BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "model_level = BoostRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    tree_learner=\"leaf\",  # Original approach\n",
    "    tree_method=\"hist\",\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "level_time = time.time() - start_time\n",
    "# Set background for proper expected value\n",
    "model_level.set_shap_background(X_train[:100])  # Use sample of training data\n",
    "\n",
    "# Compute SHAP values (should have much lower additivity errors)\n",
    "shap_values = model_level.shap_values(X_test[:10], debug=True)\n",
    "\n",
    "# Validate the fix\n",
    "# Explain individual predictions\n",
    "explanation = model_level.explain_prediction(X_test[0])\n",
    "# Get feature importance\n",
    "#importance = model_level.shap_feature_importance(X_test[:100])\n",
    "#print(\"Feature Importance:\", importance)\n",
    "\n",
    "# Analyze model behavior\n",
    "#shap_values = model_level.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc7cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_corrected_shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add SHAP to your model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myour_corrected_shap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_shap_to_boostregressor\n\u001b[32m      4\u001b[39m add_shap_to_boostregressor(BoostRegressor)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'your_corrected_shap'"
     ]
    }
   ],
   "source": [
    "# Add SHAP to your model\n",
    "from your_corrected_shap import add_shap_to_boostregressor\n",
    "\n",
    "add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "# Train model\n",
    "model = BoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
