{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db3ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Comparing Tree Growth Strategies\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Level-wise Tree Growth:\n",
      "üöÄ Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.644340, Val: 0.721932, Time: 1.83s\n",
      "[  20] Train: 0.141160, Val: 0.183251, Time: 1.86s\n",
      "[  30] Train: 0.047511, Val: 0.077846, Time: 1.89s\n",
      "[  40] Train: 0.088175, Val: 0.115822, Time: 1.93s\n",
      "[  50] Train: 0.026139, Val: 0.047356, Time: 1.95s\n",
      "‚úÖ Training completed in 1.95s, 50 trees\n",
      "   Time: 2.03s\n",
      "   Test MSE: 0.047356\n",
      "   Trees: 50\n"
     ]
    }
   ],
   "source": [
    "from boosting import BoostRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "# ================ USAGE EXAMPLE ================\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 5000, 10\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.sum(X[:, :3], axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(\"üî• Comparing Tree Growth Strategies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Level-wise (original)\n",
    "    print(\"\\n1Ô∏è‚É£ Level-wise Tree Growth:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_level = BoostRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        tree_learner=\"leaf\",  # Original approach\n",
    "        tree_method=\"hist\",\n",
    "        verbose=True,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # # compare with scikit learn's GradientBoostingRegressor\n",
    "    # from sklearn.ensemble import GradientBoostingRegressor\n",
    "    # model_sklearn = GradientBoostingRegressor(\n",
    "    #     n_estimators=50,\n",
    "    #     learning_rate=0.1,\n",
    "    #     max_depth=6,\n",
    "    #     verbose=1,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "    # model_sklearn.fit(X_train, y_train)\n",
    "    # sklearn_time = time.time() - start_time\n",
    "    # sklearn_pred = model_sklearn.predict(X_test)\n",
    "    # sklearn_mse = np.mean((y_test - sklearn_pred) ** 2)\n",
    "    # print(f\"   Scikit-learn Time: {sklearn_time:.2f}s\")\n",
    "    # print(f\"   Scikit-learn Test MSE: {sklearn_mse:.6f}\")\n",
    "    # print(f\"   Scikit-learn Trees: {len(model_sklearn.estimators_)}\")\n",
    "    \n",
    "    model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "    level_time = time.time() - start_time\n",
    "    level_pred = model_level.predict(X_test)\n",
    "    level_mse = np.mean((y_test - level_pred) ** 2)\n",
    "    \n",
    "    print(f\"   Time: {level_time:.2f}s\")\n",
    "    print(f\"   Test MSE: {level_mse:.6f}\")\n",
    "    print(f\"   Trees: {len(model_level.trees)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceecab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training with Leaf-wise trees (DART=on, GOSS=on), batch_size=1\n",
      "[  10] Train: 0.587451, Val: 0.662892, Time: 0.03s\n",
      "[  20] Train: 0.125221, Val: 0.170599, Time: 0.07s\n",
      "[  30] Train: 0.046902, Val: 0.075848, Time: 0.09s\n",
      "[  40] Train: 0.029554, Val: 0.052284, Time: 0.11s\n",
      "[  50] Train: 0.026333, Val: 0.045630, Time: 0.14s\n",
      "‚úÖ Training completed in 0.14s, 50 trees\n",
      "[TreeSHAP] Computing SHAP for 10 samples, 10 features, 50 trees\n",
      "  Sample 0: Adjusting SHAP sum from 0.47145656 to 0.49913301\n",
      "  Sample 1: Adjusting SHAP sum from 1.37536788 to 1.40304432\n",
      "  Sample 2: Adjusting SHAP sum from 1.54892413 to 1.57660058\n",
      "  Sample 3: Adjusting SHAP sum from 0.16006203 to 0.18773847\n",
      "  Sample 4: Adjusting SHAP sum from -0.12725600 to -0.09957956\n",
      "  Sample 5: Adjusting SHAP sum from 2.99801349 to 3.02568993\n",
      "  Sample 6: Adjusting SHAP sum from -0.89579542 to -0.86811897\n",
      "  Sample 7: Adjusting SHAP sum from -1.93003967 to -1.90236323\n",
      "  Sample 8: Adjusting SHAP sum from -2.29444136 to -2.26676491\n",
      "  Sample 9: Adjusting SHAP sum from -2.27891724 to -2.25124080\n",
      "[TreeSHAP] Final additivity: max_err=4.441e-16, mean_err=1.416e-16\n",
      "Prediction: 0.435779 = E[f(X)] -0.063354 + sum(phi) 0.499133\n",
      "Additivity error: 0.00000000\n",
      "Top Feature Contributions:\n",
      "------------------------------------------------------------\n",
      " 1. feature_2         :  +0.214303 ‚Üë (x=-0.198187)\n",
      " 2. feature_0         :  +0.151638 ‚Üë (x=-0.471858)\n",
      " 3. feature_1         :  +0.142431 ‚Üë (x=1.0127)\n",
      " 4. feature_5         :  +0.015421 ‚Üë (x=-0.0589634)\n",
      " 5. feature_9         :  -0.013759 ‚Üì (x=-1.83112)\n",
      " 6. feature_6         :  -0.007007 ‚Üì (x=-1.81785)\n",
      " 7. feature_7         :  -0.004169 ‚Üì (x=1.04059)\n",
      " 8. feature_3         :  -0.002624 ‚Üì (x=0.0905693)\n",
      " 9. feature_4         :  +0.001582 ‚Üë (x=0.717391)\n",
      "10. feature_8         :  +0.001316 ‚Üë (x=1.25493)\n"
     ]
    }
   ],
   "source": [
    "from shap import *\n",
    "\n",
    "BoostRegressor = add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "model_level = BoostRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    tree_learner=\"leaf\",  # Original approach\n",
    "    tree_method=\"hist\",\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model_level.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "level_time = time.time() - start_time\n",
    "# Set background for proper expected value\n",
    "model_level.set_shap_background(X_train[:100])  # Use sample of training data\n",
    "\n",
    "# Compute SHAP values (should have much lower additivity errors)\n",
    "shap_values = model_level.shap_values(X_test[:10], debug=True)\n",
    "\n",
    "# Validate the fix\n",
    "# Explain individual predictions\n",
    "explanation = model_level.explain_prediction(X_test[0])\n",
    "# Get feature importance\n",
    "#importance = model_level.shap_feature_importance(X_test[:100])\n",
    "#print(\"Feature Importance:\", importance)\n",
    "\n",
    "# Analyze model behavior\n",
    "#shap_values = model_level.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SHAP to your model\n",
    "from your_corrected_shap import add_shap_to_boostregressor\n",
    "\n",
    "add_shap_to_boostregressor(BoostRegressor)\n",
    "\n",
    "# Train model\n",
    "model = BoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91107196e-01,  4.20794623e-04, -8.77049296e-03, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12, -2.25141219e-10],\n",
       "       [ 5.39337510e-01,  4.20436384e-04,  3.54941428e-02, ...,\n",
       "         6.65237358e-27,  0.00000000e+00, -5.44576677e-25],\n",
       "       [ 1.04287985e-02,  2.56604225e-01,  7.41113806e-02, ...,\n",
       "        -4.26782522e-10,  0.00000000e+00,  2.08622529e-08],\n",
       "       ...,\n",
       "       [ 6.47171653e-03,  4.69619953e-02,  1.41931347e-03, ...,\n",
       "        -4.91279500e-08,  5.19424793e-12,  2.06371118e-08],\n",
       "       [ 2.42425815e-01, -1.79334008e-02, -1.95592688e-02, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12,  2.08622529e-08],\n",
       "       [-1.37218747e-01, -2.01557929e-01, -2.90648754e-01, ...,\n",
       "        -4.86752828e-08,  5.19424793e-12, -1.52744722e-08]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
