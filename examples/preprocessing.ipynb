{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from foreblocks import TimeSeriesPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "# Create a time series with trend, seasonality, and noise\n",
    "t = np.linspace(0, 4*np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)  # Daily pattern\n",
    "seasonality2 = 1 * np.sin(t/24)  # Weekly pattern\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Combine components\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# Create a second feature (to demonstrate multivariate capabilities)\n",
    "data2 = (0.5 * trend + 1.5 * np.cos(t) + 0.5 * np.random.normal(0, 0.3, n_samples)).reshape(-1, 1)\n",
    "data = np.hstack([data, data2])  # Now we have shape [n_samples, 2]\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[outlier_indices] = data[outlier_indices] + 5 * np.random.randn(10, 2)\n",
    "\n",
    "# Add some missing values (but not too many)\n",
    "missing_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[missing_indices, 0] = np.nan  # Only make some values missing in first feature\n",
    "\n",
    "# Create preprocessor with various techniques enabled\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    normalize=True,\n",
    "    differencing=False,\n",
    "    detrend=True,\n",
    "    apply_ewt=True,\n",
    "    window_size=24,\n",
    "    horizon=12,\n",
    "    remove_outliers=True,\n",
    "    outlier_threshold=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    impute_method=\"iterative\",\n",
    "    ewt_bands=5,\n",
    "    trend_imf_idx=0,\n",
    "    log_transform=False,\n",
    "    filter_window=5,\n",
    "    filter_polyorder=2,\n",
    "    apply_filter=True,\n",
    "    self_tune=True,\n",
    "    apply_imputation=True,\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X, y, processed_data, _ = preprocessor.fit_transform(data, time_stamps=timestamps)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Data with Outliers and Missing Values')\n",
    "plt.plot(data)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Processed Data')\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "plt.plot(processed_data)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('EWT Components')\n",
    "ewt_components = preprocessor.get_ewt_components()\n",
    "if ewt_components:\n",
    "    for i, imf in enumerate(ewt_components[0].T):\n",
    "        plt.plot(imf, label=f'IMF {i}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Target sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€ Fix path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# â”€â”€â”€ Synthetic time series â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)\n",
    "seasonality2 = 1 * np.sin(t / 24)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# â”€â”€â”€ Import feature extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from foreblocks.ts_fengine import SignalProcessor\n",
    "\n",
    "fengine = SignalProcessor()\n",
    "\n",
    "# â”€â”€â”€ Feature extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "window_size = 48\n",
    "step_size = 24\n",
    "signals = {'signal': data.flatten()}\n",
    "labels = {'signal': 0}  # dummy label\n",
    "\n",
    "features, feature_labels, raw_windows, window_labels = fengine.process_signals(\n",
    "    signals, labels, window_size=window_size, step_size=step_size, augment=True\n",
    ")\n",
    "\n",
    "selected_names = fengine.get_selected_feature_names()\n",
    "print(f\"Selected {len(selected_names)} features:\")\n",
    "print(selected_names)\n",
    "\n",
    "# â”€â”€â”€ Create dataframe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feature_names = fengine.feature_engineer.get_feature_names()\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "#features_df.index = timestamps[window_size - 1::step_size][:len(features_df)]\n",
    "\n",
    "# â”€â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(features_df.head())\n",
    "print(feature_names[:5], \"...\")  # preview names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc85fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n",
      "ðŸ” Initialized analyzer with 1,500 rows Ã— 10 columns\n",
      "   â€¢ Numeric features: 8\n",
      "   â€¢ Categorical features: 2\n",
      "ðŸ” ðŸš€ Starting comprehensive dataset analysis...\n",
      "ðŸ” Running distributions analysis...\n",
      "ðŸ” Running correlations analysis...\n",
      "ðŸ” Running outliers analysis...\n",
      "ðŸ” Running clusters analysis...\n",
      "ðŸ” Running dimensionality analysis...\n",
      "ðŸ” Running patterns analysis...\n",
      "ðŸ” Running missingness analysis...\n",
      "ðŸ” Running feature_engineering analysis...\n",
      "ðŸ” Running shap_explanation analysis...\n",
      "ðŸ” Running categorical_groups analysis...\n",
      "ðŸ” Running graph_analysis analysis...\n",
      "ðŸ” Running timeseries analysis...\n",
      "interval columns not set, guessing: ['x1_normal', 'x2_linear_combo', 'x3_skewed', 'x4_seasonal', 'x5_count', 'x6_bounded', 'x7_lognorm', 'x8_nonlinear']\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "Dataset Overview:\n",
      "â€¢ Shape: 1,500 rows Ã— 10 columns \n",
      "â€¢ Memory Usage: 0.29 MB \n",
      "â€¢ Numeric Features: 8 \n",
      "â€¢ Categorical Features: 2 \n",
      "â€¢ Missing Values: 1.61% ðŸŸ¡\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "Statistical Properties:\n",
      "â€¢ Normal Distributions: 1/8 (12.5%) \n",
      "â€¢ Skewed Distributions: 4/8 (50.0%) \n",
      "â€¢ Heavy-Tailed: 5/8 (62.5%) \n",
      "â€¢ Average Outlier Rate: 0.96% \n",
      "\n",
      "ðŸ”Ž Feature Quality Assessment\n",
      "   ------------------------------\n",
      "âœ… Normal Distributions (1):\n",
      "   x2_linear_combo\n",
      "ðŸ’¡ Safe for parametric methods and linear models\n",
      "\n",
      "âš ï¸ Highly Skewed Features (3):\n",
      "   â€¢ x1_normal: 3.43 (right-skewed)\n",
      "   â€¢ x7_lognorm: 3.41 (right-skewed)\n",
      "   â€¢ x8_nonlinear: 3.98 (right-skewed)\n",
      "âš ï¸ Apply log/sqrt transform or use robust methods\n",
      "\n",
      "ðŸ”€ Potential Bimodal Distributions (3):\n",
      "   x3_skewed, x4_seasonal, x8_nonlinear\n",
      "ðŸ’¡ Investigate mixtures or stratify data\n",
      "\n",
      "ðŸ”Ž Preprocessing Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Non-normal dominant â€” consider robust/non-parametric methods\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Correlation Summary:\n",
      "â€¢ Strong Positive (>0.7): 2 \n",
      "â€¢ Strong Negative (<-0.7): 0 \n",
      "â€¢ Moderate (0.5â€“0.7): 0 \n",
      "\n",
      "ðŸ”Ž Strong Positive Correlations\n",
      "   ------------------------------\n",
      "â€¢ x1_normal â†” x2_linear_combo: 0.739 \n",
      "â€¢ x3_skewed â†” x8_nonlinear: 0.917 \n",
      "âš ï¸ Consider dimensionality reduction or feature selection\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ADVANCED PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Feature Type Classification\n",
      "   ------------------------------\n",
      "â€¢ Gaussian: 1 features \n",
      "   x2_linear_combo\n",
      "â€¢ Log Normal Candidates: 3 features \n",
      "   x1_normal, x2_linear_combo, x7_lognorm\n",
      "â€¢ Bounded: 1 features \n",
      "   x6_bounded\n",
      "â€¢ Count Like: 1 features \n",
      "   x5_count\n",
      "â€¢ Continuous: 5 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x4_seasonal, x7_lognorm\n",
      "â€¢ Highly Skewed: 3 features \n",
      "   x1_normal, x8_nonlinear, x7_lognorm\n",
      "â€¢ Heavy Tailed: 5 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x7_lognorm, x5_count\n",
      "â€¢ Mixture Model: 7 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x4_seasonal, x6_bounded ... and 2 more\n",
      "â€¢ Power Law: 3 features \n",
      "   x1_normal, x3_skewed, x7_lognorm\n",
      "â€¢ Bimodal: 8 features \n",
      "   x1_normal, x2_linear_combo, x3_skewed, x8_nonlinear, x4_seasonal ... and 3 more\n",
      "â€¢ Uniform Like: 3 features \n",
      "   x2_linear_combo, x4_seasonal, x5_count\n",
      "â€¢ Transformable To Normal: 1 features \n",
      "   x7_lognorm\n",
      "\n",
      "ðŸ”Ž Transformation Opportunities\n",
      "   ------------------------------\n",
      "ðŸ”„ Transformable: x7_lognorm\n",
      "ðŸ’¡ Apply Box-Cox or Yeo-Johnson\n",
      "\n",
      "ðŸ”Ž Relationship Patterns\n",
      "   ------------------------------\n",
      "\n",
      "ðŸŒ€ Non-Linear Relationships:\n",
      "â€¢ x1_normal â†” x2_linear_combo: score: 0.343 \n",
      "     â””â”€ Best fit: Quadratic (RÂ²=0.894)\n",
      "ðŸ’¡ Try polynomial/kernels or specific functional forms\n",
      "\n",
      "ðŸ§¬ Complex Dependencies (High MI, Low Linear):\n",
      "â€¢ x2_linear_combo â†” x5_count: MI: 0.192 \n",
      "     â””â”€ Ensemble strength: 0.068\n",
      "â€¢ x5_count â†” x6_bounded: MI: 0.172 \n",
      "     â””â”€ Ensemble strength: 0.060\n",
      "â€¢ x3_skewed â†” x5_count: MI: 0.119 \n",
      "     â””â”€ Ensemble strength: 0.041\n",
      "ðŸ’¡ Explore interactions, non-linear models, or copula-based approaches\n",
      "\n",
      "ðŸ”„ Regime-Switching Relationships:\n",
      "â€¢ x1_normal â†” x2_linear_combo: regime diff: 0.975 \n",
      "     â””â”€ Low regime corr: 0.946\n",
      "     â””â”€ High regime corr: -0.029\n",
      "     â””â”€ Split at: 56.97\n",
      "     â””â”€ Strength: strong\n",
      "â€¢ x3_skewed â†” x8_nonlinear: regime diff: 0.849 \n",
      "     â””â”€ Low regime corr: 0.089\n",
      "     â””â”€ High regime corr: 0.938\n",
      "     â””â”€ Split at: 0.28\n",
      "     â””â”€ Strength: strong\n",
      "ðŸ’¡ Consider regime-aware models, mixture models, or threshold effects\n",
      "\n",
      "ðŸ“ Detected Functional Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: RÂ²: 0.983 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "â€¢ x1_normal â†” x2_linear_combo: RÂ²: 0.894 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "ðŸ’¡ Use detected functional forms for feature engineering or model selection\n",
      "\n",
      "ðŸŽ¯ Strong Multi-Method Dependencies:\n",
      "â€¢ x1_normal â†” x2_linear_combo: ensemble: 0.892 \n",
      "     â””â”€ Pearson: 0.739\n",
      "     â””â”€ Spearman: 0.953\n",
      "     â””â”€ Distance: 0.937\n",
      "     â””â”€ MI: 0.941\n",
      "     â””â”€ Strength: strong\n",
      "â€¢ x3_skewed â†” x8_nonlinear: ensemble: 0.838 \n",
      "     â””â”€ Pearson: 0.917\n",
      "     â””â”€ Spearman: 0.831\n",
      "     â””â”€ Distance: 0.926\n",
      "     â””â”€ MI: 0.677\n",
      "     â””â”€ Strength: strong\n",
      "ðŸ’¡ High-confidence relationships - prioritize for modeling\n",
      "\n",
      "ðŸ“ˆ Monotonic Relationships:\n",
      "â€¢ x1_normal â†” x2_linear_combo: Ï„: 0.835 \n",
      "     â””â”€ Type: monotonic\n",
      "     â””â”€ Spearman: 0.953\n",
      "â€¢ x3_skewed â†” x8_nonlinear: Ï„: 0.663 \n",
      "     â””â”€ Type: monotonic\n",
      "     â””â”€ Spearman: 0.831\n",
      "ðŸ’¡ Monotonic transforms, rank-based methods, or ordinal approaches\n",
      "\n",
      "ðŸ”Ž Statistical Distribution Fitting\n",
      "   ------------------------------\n",
      "   â€¢ x1_normal: T âœ…\n",
      "   â€¢ AIC: 11382.99 \n",
      "   â€¢ KS p-value: 0.4202 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x2_linear_combo: Normal âœ…\n",
      "   â€¢ AIC: 13330.59 \n",
      "   â€¢ KS p-value: 0.8224 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x3_skewed: Exponential âœ…\n",
      "   â€¢ AIC: 2773.47 \n",
      "   â€¢ KS p-value: 0.9065 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x4_seasonal: Weibull_Min âŒ\n",
      "   â€¢ AIC: 3259.47 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "   â€¢ x5_count: Lognormal âŒ\n",
      "   â€¢ AIC: 5013.16 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "ðŸ’¡ Use fitted distributions for simulation, anomaly detection, or Bayesian priors\n",
      "\n",
      "ðŸ”Ž Pattern Summary\n",
      "   ------------------------------\n",
      "â€¢ Total Relationship Patterns: 13 \n",
      "   Most Common Pattern Types:\n",
      "   â€¢ Complex: 4 relationships\n",
      "   â€¢ Monotonic: 2 relationships\n",
      "   â€¢ Regime Switching: 2 relationships\n",
      "\n",
      "ðŸ“‹ Advanced Modeling Recommendations:\n",
      "   â€¢ Consider regime-switching models (Markov switching, threshold VAR)\n",
      "   â€¢ Leverage detected functional forms for feature engineering\n",
      "   â€¢ High-confidence relationships - prioritize for interaction terms\n",
      "   â€¢ Rich relationship structure - consider ensemble methods\n",
      "   â€¢ Very strong dependencies detected - check for potential data leakage\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š OUTLIER DETECTION ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Methods Attempted: 6 \n",
      "â€¢ Successful Methods: 7 \n",
      "â€¢ Overall Outlier Rate: 8.5% ðŸŸ¡\n",
      "â€¢ Best Method: COPOD \n",
      "\n",
      "ðŸ”Ž Analysis Scope\n",
      "   ------------------------------\n",
      "â€¢ Total Samples: 1,500 \n",
      "â€¢ Analyzed Samples: 1,274 \n",
      "â€¢ Features Analyzed: 8 \n",
      "â€¢ Missing Data: 226 (15.1%) \n",
      "\n",
      "ðŸ”Ž Data Preprocessing\n",
      "   ------------------------------\n",
      "â€¢ Scaling Method: Power Transform \n",
      "â€¢ Missing Data Strategy: Complete Cases Only \n",
      "â€¢ Data Skewness: 1.67 (Moderately skewed) ðŸŸ \n",
      "\n",
      "ðŸ” DETAILED METHOD RESULTS\n",
      "--------------------------------------------------\n",
      "\n",
      "   ðŸ”¹ Pca Recon:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.734 ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Isolation Forest:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.093 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Ecod:\n",
      "   â€¢ Outliers Detected: 129 (8.60%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.068 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Copod:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 7.401 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Hbos:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 4.704 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Aeod:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.078 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Ensemble:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.032 ðŸŸ \n",
      "\n",
      "ðŸ” OUTLIER TREATMENT RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ 1. Recommended method: COPOD\n",
      "ðŸ’¡ 2. ðŸŽ¯ Multiple methods succeeded â€” ensemble reliability is high.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CLUSTERING ANALYSIS\n",
      "================================================================================\n",
      "Analysis Summary:\n",
      "â€¢ Methods Attempted: 6 \n",
      "â€¢ Successful Methods: 6 \n",
      "â€¢ Best Method: KMEANS \n",
      "\n",
      "ðŸ”Ž Optimal Cluster Analysis\n",
      "   ------------------------------\n",
      "â€¢ Estimated Optimal K: 4 \n",
      "â€¢ Confidence Level: Low \n",
      "\n",
      "ðŸ”Ž Method Performance\n",
      "   ------------------------------\n",
      "\n",
      "   ðŸ” Kmeans\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 3 \n",
      "   â€¢ Size Range: 188-584 points \n",
      "   â€¢ Balance Ratio: 0.32 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.175 ðŸŸ \n",
      "\n",
      "   ðŸ” Dbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 1 \n",
      "   â“˜ Single or no cluster detected (silhouette may be undefined).\n",
      "   â€¢ Size Range: 1251-1251 points \n",
      "   â€¢ Balance Ratio: 1.00 ðŸŸ¢\n",
      "   â€¢ Epsilon (DBSCAN): 2.168 \n",
      "   â€¢ Min Samples (DBSCAN): 8 \n",
      "\n",
      "   ðŸ” Hdbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 2 \n",
      "   â€¢ Size Range: 28-197 points \n",
      "   â€¢ Balance Ratio: 0.14 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.135 ðŸŸ \n",
      "\n",
      "   ðŸ” Gaussian Mixture\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 5 \n",
      "   â€¢ Size Range: 12-585 points \n",
      "   â€¢ Balance Ratio: 0.02 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.100 ðŸŸ \n",
      "\n",
      "   ðŸ” Spectral\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 4 \n",
      "   â€¢ Size Range: 148-407 points \n",
      "   â€¢ Balance Ratio: 0.36 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.143 ðŸŸ \n",
      "\n",
      "ðŸ” RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ Best performing method: ENSEMBLE (score: 0.454)\n",
      "ðŸ’¡ Moderate consensus - consider inspecting individual method results\n",
      "ðŸ’¡ Unclear cluster structure - may need feature engineering or different approach\n",
      "ðŸ’¡ Outliers detected and handled during preprocessing\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š TIME SERIES ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Total Series: 8 \n",
      "â€¢ Stationary Series: 8/8 (100.0%) \n",
      "â€¢ Forecast-Ready: 7/8 (87.5%) \n",
      "\n",
      "ðŸ”Ž Stationarity Assessment\n",
      "   ------------------------------\n",
      "âœ… Stationary (8):\n",
      "   â€¢ x1_normal: ADF p=0.0000 \n",
      "   â€¢ x2_linear_combo: ADF p=0.0000 \n",
      "   â€¢ x3_skewed: ADF p=0.0000 \n",
      "   ... and 5 more\n",
      "\n",
      "ðŸ”Ž Detected Temporal Patterns\n",
      "   ------------------------------\n",
      "ðŸ“ˆ Significant Trends:\n",
      "   â€¢ x4_seasonal: RÂ²=0.005 \n",
      "\n",
      "ðŸ”„ Seasonal Patterns:\n",
      "   â€¢ x3_skewed: strength=0.859 \n",
      "   â€¢ x6_bounded: strength=0.818 \n",
      "   â€¢ x5_count: strength=0.727 \n",
      "\n",
      "ðŸ”Ž Lag & Seasonality Suggestions\n",
      "   ------------------------------\n",
      "Per-series:\n",
      "   â€¢ x3_skewed: P=365 | strength=0.859 \n",
      "   â€¢ x6_bounded: lags=[15] | P=365 | strength=0.818 \n",
      "   â€¢ x5_count: P=365 | strength=0.727 \n",
      "   â€¢ x2_linear_combo: lags=[8] | P=365 | strength=0.678 \n",
      "   â€¢ x8_nonlinear: lags=[20] | P=365 | strength=0.542 \n",
      "   â€¢ x4_seasonal: lags=[1, 2, 3] | seasonal_lags=[12] | P=365 | strength=0.538 \n",
      "   ... and 2 more\n",
      "\n",
      "Summary:\n",
      "   â€¢ : Top lags: 6(1), 10(1), 8(1) \n",
      "   â€¢ : Top seasonal lags: 12(1) \n",
      "   â€¢ : Top periods: P=365(8) \n",
      "\n",
      "ðŸ”Ž Time Series Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Well-suited for forecasting\n",
      "ðŸ’¡ Use seasonal models (e.g., SARIMA, STL)\n",
      "ðŸ’¡ Include trend terms or detrend\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FEATURE ENGINEERING RECOMMENDATIONS\n",
      "================================================================================\n",
      "Engineering Scope:\n",
      "â€¢ Numeric Features: 8 \n",
      "â€¢ Categorical Features: 2 \n",
      "â€¢ Interaction Candidates: 30 \n",
      "\n",
      "ðŸ”Ž Priority Transformations\n",
      "   ------------------------------\n",
      "ðŸš¨ High Priority:\n",
      "   ðŸ“Œ x1_normal\n",
      "      Issues: skew=3.43, kurtosis=30.62\n",
      "      Top transforms:\n",
      "        1. QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42).fit_transform(x1_normal.values.reshape(-1,1)).ravel()\n",
      "        2. QuantileTransformer(output_distribution='uniform', n_quantiles=100, random_state=42).fit_transform(x1_normal.values.reshape(-1,1)).ravel()\n",
      "   ðŸ“Œ x7_lognorm\n",
      "      Issues: skew=3.41, kurtosis=23.80\n",
      "      Top transforms:\n",
      "        1. PowerTransformer(method='box-cox').fit_transform(x7_lognorm.values.reshape(-1,1)).ravel()\n",
      "        2. np.arcsinh(x7_lognorm)\n",
      "   ðŸ“Œ x8_nonlinear\n",
      "      Issues: skew=3.98, kurtosis=24.33\n",
      "      Top transforms:\n",
      "        1. QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42).fit_transform(x8_nonlinear.values.reshape(-1,1)).ravel()\n",
      "        2. PowerTransformer(method='yeo-johnson').fit_transform(x8_nonlinear.values.reshape(-1,1)).ravel()\n",
      "\n",
      "âš ï¸ Medium Priority:\n",
      "   â€¢ x3_skewed: skew=1.77 â†’ PowerTransformer(method='box-cox').fit_transform(x3_skewed.values.reshape(-1,1)).ravel() \n",
      "   â€¢ x5_count:  â†’ PowerTransformer(method='yeo-johnson').fit_transform(x5_count.values.reshape(-1,1)).ravel() \n",
      "\n",
      "ðŸ”Ž Categorical Encoding Strategy\n",
      "   ------------------------------\n",
      "âœ… Simple (â‰¤5):\n",
      "   â€¢ category_low_card: 3 cats â†’ OneHot(drop_first=True) \n",
      "\n",
      "ðŸ”„ Advanced (6â€“20):\n",
      "   â€¢ category_high_card: 20 cats â†’ OneHot(drop_first=False) \n",
      "\n",
      "ðŸ”Ž Interaction Features\n",
      "   ------------------------------\n",
      "â€¢ Total Candidates: 30 \n",
      "ðŸ’¡ Start with top 10 interactions\n",
      "   â€¢ x1_normal*x2_linear_combo\n",
      "   â€¢ x1_normal/(x2_linear_combo+1e-8)\n",
      "   â€¢ x2_linear_combo/(x1_normal+1e-8)\n",
      "   â€¢ abs(x1_normal-x2_linear_combo)\n",
      "   â€¢ (x1_normal+x2_linear_combo)/2\n",
      "   â€¢ np.sqrt(x1_normal**2+x2_linear_combo**2)\n",
      "   â€¢ np.minimum(x1_normal,x2_linear_combo)\n",
      "   â€¢ np.maximum(x1_normal,x2_linear_combo)\n",
      "   â€¢ (x1_normal+x2_linear_combo)**2\n",
      "   â€¢ (x1_normal-x2_linear_combo)**2\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š MISSINGNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Missing Value Rates\n",
      "   ------------------------------\n",
      "â€¢ x5_count: 5.7% ðŸŸ¡\n",
      "â€¢ x6_bounded: 5.3% ðŸŸ¡\n",
      "â€¢ x3_skewed: 5.1% ðŸŸ¡\n",
      "âš ï¸ 3 features with 5â€“20% missing\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DIMENSIONALITY ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Embedding Methods: 7 \n",
      "â€¢ Samples Ã— Features: 1274 Ã— 8 \n",
      "â€¢ Condition Number: 4.83e+00 \n",
      "â€¢ Effective Rank: 8 \n",
      "\n",
      "ðŸ”Ž Preprocessing\n",
      "   ------------------------------\n",
      "â€¢ Low-Variance Removed: 0 \n",
      "â€¢ Scaling: standard \n",
      "â€¢ Post-Preproc Shape: 1274 Ã— 8 \n",
      "\n",
      "ðŸ”Ž Method Leaderboard\n",
      "   ------------------------------\n",
      "   â€¢ tsne: score=0.269 | sil=0.422 stab=0.500 \n",
      "   â€¢ spectral: score=0.263 | sil=0.407 stab=0.500 \n",
      "   â€¢ isomap: score=0.230 | sil=0.324 stab=0.500 \n",
      "   â€¢ lle: score=0.228 | sil=0.320 stab=0.501 \n",
      "   â€¢ factor_analysis: score=0.216 | sil=0.291 stab=0.501 \n",
      "\n",
      "ðŸ”Ž Per-Method Details\n",
      "   ------------------------------\n",
      "   â€¢ pca: n_comp=3 | var=0.589 | sil=0.252 | stab=0.501 \n",
      "   â€¢ ica: n_comp=3 | sil=0.253 | stab=0.501 \n",
      "   â€¢ factor_analysis: n_comp=3 | sil=0.291 | stab=0.501 \n",
      "   â€¢ tsne: sil=0.422 | stab=0.500 \n",
      "   â€¢ isomap: sil=0.324 | stab=0.500 \n",
      "   â€¢ lle: sil=0.320 | stab=0.501 \n",
      "   â€¢ spectral: sil=0.407 | stab=0.500 \n",
      "\n",
      "ðŸ”Ž PCA Summary\n",
      "   ------------------------------\n",
      "â€¢ Variance â‰¥95%: 3 components \n",
      "â€¢ Total Variance Explained: 0.589 \n",
      "\n",
      "ðŸ”Ž 2D Visualizations Available\n",
      "   ------------------------------\n",
      "â€¢ Embeddings: tsne, isomap, lle, spectral \n",
      "\n",
      "ðŸ”Ž Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Best performing method: ica\n",
      "ðŸ’¡ Pre-reduced to 3 dims for stability and speed.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CATEGORICAL GROUP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Group Overview: category_low_card\n",
      "   ------------------------------\n",
      "â€¢ Total Groups: 3 \n",
      "â€¢ Total Samples: 1274 \n",
      "   Group Sizes:\n",
      "     â€¢ A: 661 samples\n",
      "     â€¢ B: 375 samples\n",
      "     â€¢ C: 238 samples\n",
      "   â€¢ Groups Balanced: Yes ðŸŸ¢\n",
      "   â€¢ Missing Data: 15.1% âš ï¸\n",
      "\n",
      "ðŸ”Ž Analysis Summary\n",
      "   ------------------------------\n",
      "â€¢ Variables Tested: 8 \n",
      "â€¢ Significant Variables: 2/8 ðŸŸ¡\n",
      "   Variables with significant differences:\n",
      "     â€¢ x3_skewed\n",
      "     â€¢ x4_seasonal\n",
      "\n",
      "ðŸ”Ž Variable-by-Variable Analysis\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ” Analysis for: x1_normal\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=50.939 (Â±12.219), n=661\n",
      "     â€¢ B: Î¼=51.535 (Â±14.587), n=375\n",
      "     â€¢ C: Î¼=51.044 (Â±13.747), n=238\n",
      "   Key Insights:\n",
      "     â€¢ âŒ **No significant difference detected** between groups\n",
      "     â€¢ ðŸ’ª Large sample sizes provide good statistical power\n",
      "\n",
      "ðŸ” Analysis for: x2_linear_combo\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=101.027 (Â±20.357), n=661\n",
      "     â€¢ B: Î¼=100.879 (Â±21.036), n=375\n",
      "     â€¢ C: Î¼=100.612 (Â±20.642), n=238\n",
      "   Key Insights:\n",
      "     â€¢ âŒ **No significant difference detected** between groups\n",
      "     â€¢ ðŸ’ª Large sample sizes provide good statistical power\n",
      "\n",
      "ðŸ” Analysis for: x3_skewed\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=0.976 (Â±0.956), n=661\n",
      "     â€¢ B: Î¼=0.895 (Â±0.970), n=375\n",
      "     â€¢ C: Î¼=1.059 (Â±1.004), n=238\n",
      "   Key Insights:\n",
      "     â€¢ ðŸŽ¯ **Significant difference detected** using Kruskal-Wallis H-test (p = 0.011480)\n",
      "     â€¢ ðŸ“Š Effect size is  with negligible effect size (Î·Â² = 0.005)\n",
      "\n",
      "ðŸ”Ž Categorical Variable Associations\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ“Š category_low_card â†” category_high_card\n",
      "   â€¢ Chi-square p-value: 0.910868 ðŸŸ \n",
      "   â€¢ CramÃ©r's V: 0.095 (negligible) \n",
      "     â””â”€ âŒ No significant association\n",
      "\n",
      "ðŸ”Ž Statistical Methods Performance\n",
      "   ------------------------------\n",
      "ðŸ“Š Method Usage and Success Rates:\n",
      "  â€¢ Welch's ANOVA (unequal variances): 8 uses, 0 significant (0%)\n",
      "  â€¢ Kruskal-Wallis H-test: 8 uses, 2 significant (25%)\n",
      "\n",
      "ðŸ”Ž Key Recommendations\n",
      "   ------------------------------\n",
      "   1. ðŸŽ¯ **2 variables** show significant differences between category_low_card groups\n",
      "   2. ðŸ“Š Variables with differences: x3_skewed, x4_seasonal\n",
      "   3. ðŸ”¬ **Modern methods used**: Welch's tests, bootstrap resampling, robust non-parametrics\n",
      "\n",
      "ðŸ”Ž Statistical Assumptions Summary\n",
      "   ------------------------------\n",
      "â€¢ Variables with Normal Distributions: 8/8 (100%) ðŸŸ¢\n",
      "â€¢ Variables with Equal Variances: 8/8 (100%) ðŸŸ¢\n",
      "\n",
      "ðŸ”Ž ðŸŽ¯ CONCLUSION\n",
      "   ------------------------------\n",
      "   ðŸ“Š SOME variables (2/8) show significant differences between category_low_card\n",
      "   ðŸ“ˆ Consider using category_low_card as a strong predictor in your models\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š GRAPH NETWORK ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Network Construction Overview\n",
      "   ------------------------------\n",
      "â€¢ Original Data: 1500 samples Ã— 10 features \n",
      "â€¢ Analyzed Data: 1274 samples Ã— 8 features \n",
      "\n",
      "ðŸ”Ž Network Summary\n",
      "   ------------------------------\n",
      "â€¢ Graph Construction Success: 3/3 (100%) ðŸŸ¢\n",
      "   ðŸ† Best Network Type: Knn\n",
      "   â€¢ Network Size: 8 nodes, 18 edges \n",
      "   â€¢ Network Density: 0.643 ðŸŸ¢\n",
      "   â€¢ Connectivity: Fully Connected ðŸŸ¢\n",
      "   â€¢ Clustering Coefficient: 0.783 ðŸŸ¢\n",
      "\n",
      "ðŸ”Ž Network Topology Analysis\n",
      "   ------------------------------\n",
      "ðŸ”— Connectivity Analysis:\n",
      "   â€¢ Network Diameter: 3 \n",
      "   â€¢ Average Path Length: 1.39 \n",
      "   â€¢ Network Radius: 2 \n",
      "     â””â”€ âš¡ Efficient information flow\n",
      "\n",
      "ðŸŒ Small World Network:\n",
      "   â€¢ Small-world Ïƒ: 1.21 ðŸŸ¢\n",
      "     â””â”€ âœ… Efficient global connectivity with local clustering\n",
      "\n",
      "ðŸ“Š Degree Distribution:\n",
      "   â€¢ Mean Degree: 4.5 \n",
      "   â€¢ Degree Range: 3 - 6 \n",
      "   â€¢ Degree Std Dev: 1.1 \n",
      "   â€¢ Degree Assortativity: -0.184 ðŸŸ¡\n",
      "     â””â”€ ðŸ“‰ Dissimilar-degree nodes tend to connect\n",
      "\n",
      "ðŸ”Ž Community Structure Analysis\n",
      "   ------------------------------\n",
      "ðŸ˜ï¸ Community Detection Results:\n",
      "   â€¢ Best Method: Louvain \n",
      "   â€¢ Communities Found: 2 \n",
      "   â€¢ Modularity Score: 0.083 ðŸŸ \n",
      "     â””â”€ ðŸ” Weak community structure\n",
      "   ðŸ“‹ All Community Detection Results:\n",
      "     â€¢ Louvain: 2 communities (Q=0.083)\n",
      "     â€¢ Girvan Newman: 2 communities (Q=-0.000)\n",
      "     â€¢ Spectral: 2 communities (Q=0.061)\n",
      "     â€¢ Label Propagation: 1 communities (Q=0.000)\n",
      "\n",
      "ðŸ”Ž Node Importance Analysis\n",
      "   ------------------------------\n",
      "â­ Most Important Variables:\n",
      "   â€¢ Degree: x5_count (0.857)\n",
      "     â””â”€ Measures local connectivity\n",
      "   â€¢ Pagerank: x3_skewed (0.208)\n",
      "   â€¢ Betweenness: x5_count (0.167)\n",
      "   â€¢ Eigenvector: x5_count (0.440)\n",
      "   â€¢ Closeness: x5_count (0.875)\n",
      "\n",
      "ðŸ‘‘ Top 3 Most Influential Variables:\n",
      "     1. x3_skewed (PageRank: 0.2081)\n",
      "     2. x6_bounded (PageRank: 0.2064)\n",
      "     3. x4_seasonal (PageRank: 0.1710)\n",
      "\n",
      "ðŸ”Ž Graph Construction Methods Comparison\n",
      "   ------------------------------\n",
      "ðŸ“Š Network Construction Results:\n",
      "   â€¢ Correlation: 8 nodes, 2 edges\n",
      "     â””â”€ Edge types: correlation\n",
      "     â””â”€ Density: 0.071\n",
      "   â€¢ Mutual Info: 8 nodes, 7 edges\n",
      "     â””â”€ Edge types: mutual_info\n",
      "     â””â”€ Density: 0.250\n",
      "   â€¢ Knn: 8 nodes, 18 edges\n",
      "     â””â”€ Edge types: knn\n",
      "     â””â”€ Density: 0.643\n",
      "\n",
      "ðŸ”Ž Key Insights & Recommendations\n",
      "   ------------------------------\n",
      "   1. ðŸ† **Best graph structure**: Knn Network\n",
      "   2. ðŸ”— **Highly connected network** - variables form a cohesive system\n",
      "   3. âš¡ **Short path lengths** - information flows efficiently through network\n",
      "   4. ðŸ”˜ **High clustering** - variables form tight-knit groups\n",
      "   5. ðŸŒ **Small-world network** - efficient global connectivity with local clustering\n",
      "   6. ðŸš€ Ideal for feature engineering and dimensionality reduction\n",
      "\n",
      "ðŸ”Ž ðŸŽ¯ NETWORK ANALYSIS CONCLUSION\n",
      "   ------------------------------\n",
      "   ðŸŒŸ STRONG network structure detected with 8 interconnected variables\n",
      "   ðŸ”— Rich connectivity (18 relationships) enables advanced graph-based analysis\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPREHENSIVE DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "Quality Dimensions:\n",
      "â€¢ Completeness: 98.4% ðŸŸ¢\n",
      "â€¢ Uniqueness: 100.0% ðŸŸ¢\n",
      "â€¢ Consistency: 65.0% ðŸ”´\n",
      "â€¢ Validity: 57.3% ðŸ”´\n",
      "\n",
      "ðŸ”Ž Categorical Feature Health\n",
      "   ------------------------------\n",
      "   â€¢ category_low_card:\n",
      "   â€¢ Unique Values: 3 (0.2%) ðŸŸ¡\n",
      "   â€¢ category_high_card:\n",
      "   â€¢ Unique Values: 20 (1.3%) ðŸŸ¡\n",
      "\n",
      "ðŸ”Ž Overall Quality Summary\n",
      "   ------------------------------\n",
      "â€¢ Overall Quality Score: 80.2% \n",
      "   ðŸŸ¡ Good - minor preprocessing\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š EXECUTIVE RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Action Items by Priority\n",
      "   ------------------------------\n",
      "\n",
      "   âš ï¸ MEDIUM PRIORITY:\n",
      "   âš ï¸ Consider transformations for skewed features\n",
      "   âš ï¸ Review and validate detected outliers\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ANALYSIS COMPLETION SUMMARY\n",
      "================================================================================\n",
      "âœ… Completed Analyses: 9\n",
      "   â€¢ Distribution Analysis\n",
      "   â€¢ Correlation Analysis\n",
      "   â€¢ Pattern Detection\n",
      "   â€¢ Feature Engineering\n",
      "   â€¢ Time Series Analysis\n",
      "   â€¢ Clustering Analysis\n",
      "   â€¢ Outlier Detection\n",
      "   â€¢ Missingness Analysis\n",
      "   â€¢ Dimensionality Reduction\n",
      "\n",
      "ðŸ“Š Dataset Shape: 1,500 rows Ã— 10 columns\n",
      "ðŸ’¾ Memory Usage: 0.29 MB\n",
      "ðŸŽ¯ Overall Quality: 80.2% (Good)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ COMPREHENSIVE DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "ðŸ” Use these insights to guide preprocessing and modeling.\n",
      "ðŸ“ˆ Consider the priority recommendations for optimal results.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "from foreminer.foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
