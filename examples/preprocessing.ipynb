{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from foreblocks import TimeSeriesPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "# Create a time series with trend, seasonality, and noise\n",
    "t = np.linspace(0, 4*np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)  # Daily pattern\n",
    "seasonality2 = 1 * np.sin(t/24)  # Weekly pattern\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Combine components\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# Create a second feature (to demonstrate multivariate capabilities)\n",
    "data2 = (0.5 * trend + 1.5 * np.cos(t) + 0.5 * np.random.normal(0, 0.3, n_samples)).reshape(-1, 1)\n",
    "data = np.hstack([data, data2])  # Now we have shape [n_samples, 2]\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[outlier_indices] = data[outlier_indices] + 5 * np.random.randn(10, 2)\n",
    "\n",
    "# Add some missing values (but not too many)\n",
    "missing_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[missing_indices, 0] = np.nan  # Only make some values missing in first feature\n",
    "\n",
    "# Create preprocessor with various techniques enabled\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    normalize=True,\n",
    "    differencing=False,\n",
    "    detrend=True,\n",
    "    apply_ewt=True,\n",
    "    window_size=24,\n",
    "    horizon=12,\n",
    "    remove_outliers=True,\n",
    "    outlier_threshold=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    impute_method=\"iterative\",\n",
    "    ewt_bands=5,\n",
    "    trend_imf_idx=0,\n",
    "    log_transform=False,\n",
    "    filter_window=5,\n",
    "    filter_polyorder=2,\n",
    "    apply_filter=True,\n",
    "    self_tune=True,\n",
    "    apply_imputation=True,\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X, y, processed_data, _ = preprocessor.fit_transform(data, time_stamps=timestamps)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Data with Outliers and Missing Values')\n",
    "plt.plot(data)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Processed Data')\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "plt.plot(processed_data)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('EWT Components')\n",
    "ewt_components = preprocessor.get_ewt_components()\n",
    "if ewt_components:\n",
    "    for i, imf in enumerate(ewt_components[0].T):\n",
    "        plt.plot(imf, label=f'IMF {i}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Target sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Fix path ─────────────────────────────\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# ─── Synthetic time series ────────────────\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)\n",
    "seasonality2 = 1 * np.sin(t / 24)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# ─── Import feature extractor ─────────────\n",
    "from foreblocks.ts_fengine import SignalProcessor\n",
    "\n",
    "fengine = SignalProcessor()\n",
    "\n",
    "# ─── Feature extraction ───────────────────\n",
    "window_size = 48\n",
    "step_size = 24\n",
    "signals = {'signal': data.flatten()}\n",
    "labels = {'signal': 0}  # dummy label\n",
    "\n",
    "features, feature_labels, raw_windows, window_labels = fengine.process_signals(\n",
    "    signals, labels, window_size=window_size, step_size=step_size, augment=True\n",
    ")\n",
    "\n",
    "selected_names = fengine.get_selected_feature_names()\n",
    "print(f\"Selected {len(selected_names)} features:\")\n",
    "print(selected_names)\n",
    "\n",
    "# ─── Create dataframe ─────────────────────\n",
    "feature_names = fengine.feature_engineer.get_feature_names()\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "#features_df.index = timestamps[window_size - 1::step_size][:len(features_df)]\n",
    "\n",
    "# ─── Display ──────────────────────────────\n",
    "print(features_df.head())\n",
    "print(feature_names[:5], \"...\")  # preview names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc85fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02184ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3166270/2748512264.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DatasetAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     67\u001b[39m df = generate_synthetic_dataset(n_samples=\u001b[32m1500\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m analyzer = \u001b[43mDatasetAnalyzer\u001b[49m(df, time_col=\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     69\u001b[39m analyzer.analyze_everything()\n",
      "\u001b[31mNameError\u001b[39m: name 'DatasetAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n",
      "🔍 Initialized analyzer with 1,500 rows × 10 columns\n",
      "   • Numeric features: 8\n",
      "   • Categorical features: 2\n",
      "🔍 🚀 Starting comprehensive dataset analysis...\n",
      "🔍 Running distributions analysis...\n",
      "🔍 Running correlations analysis...\n",
      "🔍 Running outliers analysis...\n",
      "🔍 Running clusters analysis...\n",
      "🔍 Running dimensionality analysis...\n",
      "🔍 Running patterns analysis...\n",
      "🔍 Running missingness analysis...\n",
      "🔍 Running feature_engineering analysis...\n",
      "🔍 Running shap_explanation analysis...\n",
      "🔍 Running categorical_groups analysis...\n",
      "🔍 Running graph_analysis analysis...\n",
      "🔍 Running timeseries analysis...\n",
      "interval columns not set, guessing: ['x1_normal', 'x2_linear_combo', 'x3_skewed', 'x4_seasonal', 'x5_count', 'x6_bounded', 'x7_lognorm', 'x8_nonlinear']\n",
      "\n",
      "================================================================================\n",
      "📊 EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "Dataset Overview:\n",
      "• Shape: 1,500 rows × 10 columns \n",
      "• Memory Usage: 0.29 MB \n",
      "• Numeric Features: 8 \n",
      "• Categorical Features: 2 \n",
      "• Missing Values: 1.61% 🟡\n",
      "\n",
      "================================================================================\n",
      "📊 DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "Statistical Properties:\n",
      "• Normal Distributions: 1/8 (12.5%) \n",
      "• Skewed Distributions: 4/8 (50.0%) \n",
      "• Heavy-Tailed: 5/8 (62.5%) \n",
      "• Average Outlier Rate: 0.96% \n",
      "\n",
      "🔎 Feature Quality Assessment\n",
      "   ------------------------------\n",
      "✅ Normal Distributions (1):\n",
      "   x2_linear_combo\n",
      "💡 Safe for parametric methods and linear models\n",
      "\n",
      "⚠️ Highly Skewed Features (3):\n",
      "   • x1_normal: 3.43 (right-skewed)\n",
      "   • x7_lognorm: 3.41 (right-skewed)\n",
      "   • x8_nonlinear: 3.98 (right-skewed)\n",
      "⚠️ Apply log/sqrt transform or use robust methods\n",
      "\n",
      "🔀 Potential Bimodal Distributions (3):\n",
      "   x3_skewed, x4_seasonal, x8_nonlinear\n",
      "💡 Investigate mixtures or stratify data\n",
      "\n",
      "🔎 Preprocessing Recommendations\n",
      "   ------------------------------\n",
      "💡 Non-normal dominant — consider robust/non-parametric methods\n",
      "\n",
      "================================================================================\n",
      "📊 CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Correlation Summary:\n",
      "• Strong Positive (>0.7): 2 \n",
      "• Strong Negative (<-0.7): 0 \n",
      "• Moderate (0.5–0.7): 0 \n",
      "\n",
      "🔎 Strong Positive Correlations\n",
      "   ------------------------------\n",
      "• x1_normal ↔ x2_linear_combo: 0.739 \n",
      "• x3_skewed ↔ x8_nonlinear: 0.917 \n",
      "⚠️ Consider dimensionality reduction or feature selection\n",
      "\n",
      "================================================================================\n",
      "📊 ADVANCED PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "🔎 Feature Type Classification\n",
      "   ------------------------------\n",
      "• Gaussian: 1 features \n",
      "   x2_linear_combo\n",
      "• Log Normal Candidates: 3 features \n",
      "   x1_normal, x2_linear_combo, x7_lognorm\n",
      "• Bounded: 1 features \n",
      "   x6_bounded\n",
      "• Count Like: 1 features \n",
      "   x5_count\n",
      "• Continuous: 5 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x7_lognorm, x8_nonlinear\n",
      "• Highly Skewed: 3 features \n",
      "   x1_normal, x7_lognorm, x8_nonlinear\n",
      "• Heavy Tailed: 5 features \n",
      "   x1_normal, x3_skewed, x5_count, x7_lognorm, x8_nonlinear\n",
      "• Mixture Model: 7 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x5_count, x6_bounded ... and 2 more\n",
      "• Power Law: 3 features \n",
      "   x1_normal, x3_skewed, x7_lognorm\n",
      "• Bimodal: 8 features \n",
      "   x1_normal, x2_linear_combo, x3_skewed, x4_seasonal, x5_count ... and 3 more\n",
      "• Uniform Like: 3 features \n",
      "   x2_linear_combo, x4_seasonal, x5_count\n",
      "• Transformable To Normal: 1 features \n",
      "   x7_lognorm\n",
      "\n",
      "🔎 Transformation Opportunities\n",
      "   ------------------------------\n",
      "🔄 Transformable: x7_lognorm\n",
      "💡 Apply Box-Cox or Yeo-Johnson\n",
      "\n",
      "🔎 Relationship Patterns\n",
      "   ------------------------------\n",
      "\n",
      "🌀 Non-Linear Relationships:\n",
      "     └─ Best fit: Quadratic (R²=0.894)\n",
      "💡 Try polynomial/kernels or specific functional forms\n",
      "\n",
      "🔄 Regime-Switching Relationships:\n",
      "• x3_skewed ↔ x8_nonlinear: regime diff: 0.703 \n",
      "     └─ Low regime corr: 0.253\n",
      "     └─ High regime corr: 0.957\n",
      "     └─ Split at: 0.66\n",
      "     └─ Strength: strong\n",
      "• x1_normal ↔ x2_linear_combo: regime diff: 0.588 \n",
      "     └─ Low regime corr: 0.925\n",
      "     └─ High regime corr: 0.337\n",
      "     └─ Split at: 50.59\n",
      "     └─ Strength: strong\n",
      "💡 Consider regime-aware models, mixture models, or threshold effects\n",
      "\n",
      "📐 Detected Functional Relationships:\n",
      "• x3_skewed ↔ x8_nonlinear: R²: 0.983 \n",
      "     └─ Complexity: moderate\n",
      "     └─ Alternatives: \n",
      "• x1_normal ↔ x2_linear_combo: R²: 0.894 \n",
      "     └─ Complexity: moderate\n",
      "     └─ Alternatives: \n",
      "💡 Use detected functional forms for feature engineering or model selection\n",
      "\n",
      "🎯 Strong Multi-Method Dependencies:\n",
      "• x3_skewed ↔ x8_nonlinear: ensemble: 0.979 \n",
      "     └─ Pearson: 0.917\n",
      "     └─ Spearman: 0.831\n",
      "     └─ Distance: 0.926\n",
      "     └─ MI: 1.129\n",
      "     └─ Strength: very_strong\n",
      "• x1_normal ↔ x2_linear_combo: ensemble: 0.978 \n",
      "     └─ Pearson: 0.739\n",
      "     └─ Spearman: 0.953\n",
      "     └─ Distance: 0.937\n",
      "     └─ MI: 1.414\n",
      "     └─ Strength: very_strong\n",
      "💡 High-confidence relationships - prioritize for modeling\n",
      "\n",
      "🔗 Copula-Based Dependencies:\n",
      "• x1_normal ↔ x2_linear_combo: τ: 0.835 \n",
      "     └─ Type: body dependence\n",
      "     └─ Tail coefficient: 0.036\n",
      "• x3_skewed ↔ x8_nonlinear: τ: 0.663 \n",
      "     └─ Type: body dependence\n",
      "     └─ Tail coefficient: 0.048\n",
      "💡 Consider copula models for capturing rank-based dependencies\n",
      "\n",
      "📈 Monotonic Relationships:\n",
      "• x3_skewed ↔ x8_nonlinear: τ: 0.663 \n",
      "     └─ Type: monotonic\n",
      "• x1_normal ↔ x2_linear_combo: τ: 0.835 \n",
      "     └─ Type: monotonic\n",
      "💡 Monotonic transforms, rank-based methods, or ordinal approaches\n",
      "\n",
      "🔎 Statistical Distribution Fitting\n",
      "   ------------------------------\n",
      "   • x1_normal: T ✅\n",
      "   • AIC: 11382.99 \n",
      "   • KS p-value: 0.4202 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x2_linear_combo: Normal ✅\n",
      "   • AIC: 13330.59 \n",
      "   • KS p-value: 0.8224 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x3_skewed: Exponential ✅\n",
      "   • AIC: 2773.47 \n",
      "   • KS p-value: 0.9065 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x4_seasonal: Weibull_Min ❌\n",
      "   • AIC: 3259.47 \n",
      "   • KS p-value: 0.0000 🔴\n",
      "     └─ Fit Quality: Poor\n",
      "   • x5_count: Lognormal ❌\n",
      "   • AIC: 5013.16 \n",
      "   • KS p-value: 0.0000 🔴\n",
      "     └─ Fit Quality: Poor\n",
      "💡 Use fitted distributions for simulation, anomaly detection, or Bayesian priors\n",
      "\n",
      "🔎 Pattern Summary\n",
      "   ------------------------------\n",
      "• Total Relationship Patterns: 13 \n",
      "   Most Common Pattern Types:\n",
      "   • Monotonic: 2 relationships\n",
      "   • Conditional: 2 relationships\n",
      "   • Regime Switching: 2 relationships\n",
      "\n",
      "📋 Advanced Modeling Recommendations:\n",
      "   • Consider regime-switching models (Markov switching, threshold VAR)\n",
      "   • Leverage detected functional forms for feature engineering\n",
      "   • Explore copula-based models for dependency modeling\n",
      "   • High-confidence relationships - prioritize for interaction terms\n",
      "   • Rich relationship structure - consider ensemble methods\n",
      "   • Very strong dependencies detected - check for potential data leakage\n",
      "\n",
      "================================================================================\n",
      "📊 OUTLIER DETECTION ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "• Methods Attempted: 14 \n",
      "• Successful Methods: 15 \n",
      "• Overall Outlier Rate: 7.4% 🟡\n",
      "• Best Method: ELLIPTIC_ENVELOPE \n",
      "\n",
      "🔎 Analysis Scope\n",
      "   ------------------------------\n",
      "• Total Samples: 1,500 \n",
      "• Analyzed Samples: 1,274 \n",
      "• Features Analyzed: 8 \n",
      "• Missing Data: 226 (15.1%) \n",
      "\n",
      "🔎 Data Preprocessing\n",
      "   ------------------------------\n",
      "• Scaling Method: Power Transform \n",
      "• Missing Data Strategy: Complete Cases Only \n",
      "• Data Skewness: 1.67 (Moderately skewed) 🟠\n",
      "\n",
      "🔍 DETAILED METHOD RESULTS\n",
      "--------------------------------------------------\n",
      "\n",
      "   🔹 Z Score:\n",
      "   • Outliers Detected: 26 (1.73%) 🟡\n",
      "   • Separation Quality: 2.116 🟢\n",
      "\n",
      "   🔹 Modified Z Score:\n",
      "   • Outliers Detected: 21 (1.40%) 🟡\n",
      "   • Separation Quality: 2.920 🟢\n",
      "\n",
      "   🔹 Iqr:\n",
      "   • Outliers Detected: 71 (4.73%) 🟡\n",
      "\n",
      "   🔹 Pca Recon:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.734 🟡\n",
      "\n",
      "   🔹 Knn Distance:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "\n",
      "   🔹 Dbscan:\n",
      "   • Outliers Detected: 41 (2.73%) 🟡\n",
      "\n",
      "   🔹 Mahalanobis Robust:\n",
      "   • Outliers Detected: 226 (15.07%) ⚠️\n",
      "\n",
      "   🔹 Isolation Forest:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.089 🟠\n",
      "\n",
      "   🔹 Local Outlier Factor:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.274 🟠\n",
      "\n",
      "   🔹 One Class Svm:\n",
      "   • Outliers Detected: 129 (8.60%) 🟠\n",
      "   • Separation Quality: 4.277 🟢\n",
      "\n",
      "   🔹 Elliptic Envelope:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 44.806 🟢\n",
      "\n",
      "   🔹 Ecod:\n",
      "   • Outliers Detected: 129 (8.60%) 🟠\n",
      "   • Separation Quality: 0.068 🟠\n",
      "\n",
      "   🔹 Copod:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 7.401 🟢\n",
      "\n",
      "   🔹 Hbos:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 4.704 🟢\n",
      "\n",
      "   🔹 Ensemble:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "\n",
      "🔍 OUTLIER TREATMENT RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "💡 1. Recommended method: COPOD (score: 2.20)\n",
      "💡 2. Healthy outlier rate: 10.0%\n",
      "💡 3. 🎯 Multiple methods succeeded — ensemble reliability is high.\n",
      "\n",
      "================================================================================\n",
      "📊 CLUSTERING ANALYSIS\n",
      "================================================================================\n",
      "Analysis Summary:\n",
      "• Methods Attempted: 7 \n",
      "• Successful Methods: 7 \n",
      "• Best Method: HIERARCHICAL \n",
      "\n",
      "🔎 Optimal Cluster Analysis\n",
      "   ------------------------------\n",
      "• Estimated Optimal K: 4 \n",
      "• Confidence Level: Low \n",
      "\n",
      "🔎 Method Performance\n",
      "   ------------------------------\n",
      "\n",
      "   🔍 Kmeans\n",
      "      ------------------------------\n",
      "   • Clusters Found: 3 \n",
      "   • Size Range: 115-632 points \n",
      "   • Balance Ratio: 0.18 🟠\n",
      "   • Silhouette Score: 0.163 🟠\n",
      "   • Best K (internal): 3 \n",
      "\n",
      "   🔍 Hierarchical\n",
      "      ------------------------------\n",
      "   • Clusters Found: 4 \n",
      "   • Size Range: 3-1230 points \n",
      "   • Balance Ratio: 0.00 🟠\n",
      "   • Silhouette Score: 0.493 🟠\n",
      "\n",
      "   🔍 Dbscan\n",
      "      ------------------------------\n",
      "   • Clusters Found: 1 \n",
      "   ⓘ Single or no cluster detected (silhouette may be undefined).\n",
      "   • Size Range: 1207-1207 points \n",
      "   • Balance Ratio: 1.00 🟢\n",
      "   • Epsilon (DBSCAN): 1.777 \n",
      "   • Min Samples (DBSCAN): 5 \n",
      "\n",
      "   🔍 Hdbscan\n",
      "      ------------------------------\n",
      "   • Clusters Found: 5 \n",
      "   • Size Range: 16-40 points \n",
      "   • Balance Ratio: 0.40 🟠\n",
      "   • Silhouette Score: 0.264 🟠\n",
      "\n",
      "   🔍 Gmm\n",
      "      ------------------------------\n",
      "   • Clusters Found: 7 \n",
      "   • Size Range: 20-378 points \n",
      "   • Balance Ratio: 0.05 🟠\n",
      "   • Silhouette Score: 0.048 🟠\n",
      "\n",
      "🔍 RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "💡 Best method: KMEANS (score: 0.453)\n",
      "💡 Unclear cluster structure — verify if clustering is appropriate for this data.\n",
      "\n",
      "================================================================================\n",
      "📊 TIME SERIES ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "• Total Series: 8 \n",
      "• Stationary Series: 8/8 (100.0%) \n",
      "• Forecast-Ready: 3/8 (37.5%) \n",
      "\n",
      "🔎 Stationarity Assessment\n",
      "   ------------------------------\n",
      "✅ Stationary (8):\n",
      "   • x1_normal: ADF p=0.0000 \n",
      "   • x2_linear_combo: ADF p=0.0000 \n",
      "   • x3_skewed: ADF p=0.0000 \n",
      "   ... and 5 more\n",
      "\n",
      "🔎 Detected Temporal Patterns\n",
      "   ------------------------------\n",
      "\n",
      "🔄 Seasonal Patterns:\n",
      "   • x3_skewed: strength=0.859 \n",
      "   • x6_bounded: strength=0.818 \n",
      "   • x5_count: strength=0.727 \n",
      "\n",
      "🔎 Lag & Seasonality Suggestions\n",
      "   ------------------------------\n",
      "Per-series:\n",
      "   • x3_skewed: lags=[9] | P=365 | strength=0.859 \n",
      "   • x6_bounded: lags=[1] | P=365 | strength=0.818 \n",
      "   • x5_count: lags=[10] | P=365 | strength=0.727 \n",
      "   • x2_linear_combo: lags=[10] | P=365 | strength=0.678 \n",
      "   • x8_nonlinear: lags=[10] | P=365 | strength=0.542 \n",
      "   • x4_seasonal: lags=[1, 2, 3] | seasonal_lags=[12] | P=365 | strength=0.538 \n",
      "   ... and 2 more\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m df = generate_synthetic_dataset(n_samples=\u001b[32m1500\u001b[39m)\n\u001b[32m     67\u001b[39m analyzer = DatasetAnalyzer(df, time_col=\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foreminer/foreminer.py:245\u001b[39m, in \u001b[36mDatasetAnalyzer.analyze_everything\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m results = \u001b[38;5;28mself\u001b[39m.analyze()\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Generate comprehensive insights report\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprint_detailed_insights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foreminer/foreminer.py:1054\u001b[39m, in \u001b[36mDatasetAnalyzer.print_detailed_insights\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m all_rec_lags   = [lag \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m (r[\u001b[33m\"\u001b[39m\u001b[33mrec_lags\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1053\u001b[39m all_seas_lags  = [lag \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m (r[\u001b[33m\"\u001b[39m\u001b[33mseas_lags\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m period_counts  = \u001b[43mCounter\u001b[49m([r[\u001b[33m\"\u001b[39m\u001b[33mstl_period\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mstl_period\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m   1055\u001b[39m rec_counts     = Counter(all_rec_lags)\n\u001b[32m   1056\u001b[39m seaslag_counts = Counter(all_seas_lags)\n",
      "\u001b[31mNameError\u001b[39m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "from foreminer.foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
