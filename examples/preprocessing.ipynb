{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from foreblocks import TimeSeriesPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "# Create a time series with trend, seasonality, and noise\n",
    "t = np.linspace(0, 4*np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)  # Daily pattern\n",
    "seasonality2 = 1 * np.sin(t/24)  # Weekly pattern\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Combine components\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# Create a second feature (to demonstrate multivariate capabilities)\n",
    "data2 = (0.5 * trend + 1.5 * np.cos(t) + 0.5 * np.random.normal(0, 0.3, n_samples)).reshape(-1, 1)\n",
    "data = np.hstack([data, data2])  # Now we have shape [n_samples, 2]\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[outlier_indices] = data[outlier_indices] + 5 * np.random.randn(10, 2)\n",
    "\n",
    "# Add some missing values (but not too many)\n",
    "missing_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[missing_indices, 0] = np.nan  # Only make some values missing in first feature\n",
    "\n",
    "# Create preprocessor with various techniques enabled\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    normalize=True,\n",
    "    differencing=False,\n",
    "    detrend=True,\n",
    "    apply_ewt=True,\n",
    "    window_size=24,\n",
    "    horizon=12,\n",
    "    remove_outliers=True,\n",
    "    outlier_threshold=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    impute_method=\"iterative\",\n",
    "    ewt_bands=5,\n",
    "    trend_imf_idx=0,\n",
    "    log_transform=False,\n",
    "    filter_window=5,\n",
    "    filter_polyorder=2,\n",
    "    apply_filter=True,\n",
    "    self_tune=True,\n",
    "    apply_imputation=True,\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X, y, processed_data, _ = preprocessor.fit_transform(data, time_stamps=timestamps)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Data with Outliers and Missing Values')\n",
    "plt.plot(data)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Processed Data')\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "plt.plot(processed_data)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('EWT Components')\n",
    "ewt_components = preprocessor.get_ewt_components()\n",
    "if ewt_components:\n",
    "    for i, imf in enumerate(ewt_components[0].T):\n",
    "        plt.plot(imf, label=f'IMF {i}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Target sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€ Fix path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# â”€â”€â”€ Synthetic time series â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)\n",
    "seasonality2 = 1 * np.sin(t / 24)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# â”€â”€â”€ Import feature extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from foreblocks.ts_fengine import SignalProcessor\n",
    "\n",
    "fengine = SignalProcessor()\n",
    "\n",
    "# â”€â”€â”€ Feature extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "window_size = 48\n",
    "step_size = 24\n",
    "signals = {'signal': data.flatten()}\n",
    "labels = {'signal': 0}  # dummy label\n",
    "\n",
    "features, feature_labels, raw_windows, window_labels = fengine.process_signals(\n",
    "    signals, labels, window_size=window_size, step_size=step_size, augment=True\n",
    ")\n",
    "\n",
    "selected_names = fengine.get_selected_feature_names()\n",
    "print(f\"Selected {len(selected_names)} features:\")\n",
    "print(selected_names)\n",
    "\n",
    "# â”€â”€â”€ Create dataframe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feature_names = fengine.feature_engineer.get_feature_names()\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "#features_df.index = timestamps[window_size - 1::step_size][:len(features_df)]\n",
    "\n",
    "# â”€â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(features_df.head())\n",
    "print(feature_names[:5], \"...\")  # preview names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc85fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02184ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3166270/2748512264.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DatasetAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     67\u001b[39m df = generate_synthetic_dataset(n_samples=\u001b[32m1500\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m analyzer = \u001b[43mDatasetAnalyzer\u001b[49m(df, time_col=\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     69\u001b[39m analyzer.analyze_everything()\n",
      "\u001b[31mNameError\u001b[39m: name 'DatasetAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n",
      "ðŸ” Initialized analyzer with 1,500 rows Ã— 10 columns\n",
      "   â€¢ Numeric features: 8\n",
      "   â€¢ Categorical features: 2\n",
      "ðŸ” ðŸš€ Starting comprehensive dataset analysis...\n",
      "ðŸ” Running distributions analysis...\n",
      "ðŸ” Running correlations analysis...\n",
      "ðŸ” Running outliers analysis...\n",
      "ðŸ” Running clusters analysis...\n",
      "ðŸ” Running dimensionality analysis...\n",
      "ðŸ” Running patterns analysis...\n",
      "ðŸ” Running missingness analysis...\n",
      "ðŸ” Running feature_engineering analysis...\n",
      "ðŸ” Running shap_explanation analysis...\n",
      "ðŸ” Running categorical_groups analysis...\n",
      "ðŸ” Running graph_analysis analysis...\n",
      "ðŸ” Running timeseries analysis...\n",
      "interval columns not set, guessing: ['x1_normal', 'x2_linear_combo', 'x3_skewed', 'x4_seasonal', 'x5_count', 'x6_bounded', 'x7_lognorm', 'x8_nonlinear']\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "Dataset Overview:\n",
      "â€¢ Shape: 1,500 rows Ã— 10 columns \n",
      "â€¢ Memory Usage: 0.29 MB \n",
      "â€¢ Numeric Features: 8 \n",
      "â€¢ Categorical Features: 2 \n",
      "â€¢ Missing Values: 1.61% ðŸŸ¡\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "Statistical Properties:\n",
      "â€¢ Normal Distributions: 1/8 (12.5%) \n",
      "â€¢ Skewed Distributions: 4/8 (50.0%) \n",
      "â€¢ Heavy-Tailed: 5/8 (62.5%) \n",
      "â€¢ Average Outlier Rate: 0.96% \n",
      "\n",
      "ðŸ”Ž Feature Quality Assessment\n",
      "   ------------------------------\n",
      "âœ… Normal Distributions (1):\n",
      "   x2_linear_combo\n",
      "ðŸ’¡ Safe for parametric methods and linear models\n",
      "\n",
      "âš ï¸ Highly Skewed Features (3):\n",
      "   â€¢ x1_normal: 3.43 (right-skewed)\n",
      "   â€¢ x7_lognorm: 3.41 (right-skewed)\n",
      "   â€¢ x8_nonlinear: 3.98 (right-skewed)\n",
      "âš ï¸ Apply log/sqrt transform or use robust methods\n",
      "\n",
      "ðŸ”€ Potential Bimodal Distributions (3):\n",
      "   x3_skewed, x4_seasonal, x8_nonlinear\n",
      "ðŸ’¡ Investigate mixtures or stratify data\n",
      "\n",
      "ðŸ”Ž Preprocessing Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Non-normal dominant â€” consider robust/non-parametric methods\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Correlation Summary:\n",
      "â€¢ Strong Positive (>0.7): 2 \n",
      "â€¢ Strong Negative (<-0.7): 0 \n",
      "â€¢ Moderate (0.5â€“0.7): 0 \n",
      "\n",
      "ðŸ”Ž Strong Positive Correlations\n",
      "   ------------------------------\n",
      "â€¢ x1_normal â†” x2_linear_combo: 0.739 \n",
      "â€¢ x3_skewed â†” x8_nonlinear: 0.917 \n",
      "âš ï¸ Consider dimensionality reduction or feature selection\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ADVANCED PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Feature Type Classification\n",
      "   ------------------------------\n",
      "â€¢ Gaussian: 1 features \n",
      "   x2_linear_combo\n",
      "â€¢ Log Normal Candidates: 3 features \n",
      "   x1_normal, x2_linear_combo, x7_lognorm\n",
      "â€¢ Bounded: 1 features \n",
      "   x6_bounded\n",
      "â€¢ Count Like: 1 features \n",
      "   x5_count\n",
      "â€¢ Continuous: 5 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x7_lognorm, x8_nonlinear\n",
      "â€¢ Highly Skewed: 3 features \n",
      "   x1_normal, x7_lognorm, x8_nonlinear\n",
      "â€¢ Heavy Tailed: 5 features \n",
      "   x1_normal, x3_skewed, x5_count, x7_lognorm, x8_nonlinear\n",
      "â€¢ Mixture Model: 7 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x5_count, x6_bounded ... and 2 more\n",
      "â€¢ Power Law: 3 features \n",
      "   x1_normal, x3_skewed, x7_lognorm\n",
      "â€¢ Bimodal: 8 features \n",
      "   x1_normal, x2_linear_combo, x3_skewed, x4_seasonal, x5_count ... and 3 more\n",
      "â€¢ Uniform Like: 3 features \n",
      "   x2_linear_combo, x4_seasonal, x5_count\n",
      "â€¢ Transformable To Normal: 1 features \n",
      "   x7_lognorm\n",
      "\n",
      "ðŸ”Ž Transformation Opportunities\n",
      "   ------------------------------\n",
      "ðŸ”„ Transformable: x7_lognorm\n",
      "ðŸ’¡ Apply Box-Cox or Yeo-Johnson\n",
      "\n",
      "ðŸ”Ž Relationship Patterns\n",
      "   ------------------------------\n",
      "\n",
      "ðŸŒ€ Non-Linear Relationships:\n",
      "     â””â”€ Best fit: Quadratic (RÂ²=0.894)\n",
      "ðŸ’¡ Try polynomial/kernels or specific functional forms\n",
      "\n",
      "ðŸ”„ Regime-Switching Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: regime diff: 0.703 \n",
      "     â””â”€ Low regime corr: 0.253\n",
      "     â””â”€ High regime corr: 0.957\n",
      "     â””â”€ Split at: 0.66\n",
      "     â””â”€ Strength: strong\n",
      "â€¢ x1_normal â†” x2_linear_combo: regime diff: 0.588 \n",
      "     â””â”€ Low regime corr: 0.925\n",
      "     â””â”€ High regime corr: 0.337\n",
      "     â””â”€ Split at: 50.59\n",
      "     â””â”€ Strength: strong\n",
      "ðŸ’¡ Consider regime-aware models, mixture models, or threshold effects\n",
      "\n",
      "ðŸ“ Detected Functional Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: RÂ²: 0.983 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "â€¢ x1_normal â†” x2_linear_combo: RÂ²: 0.894 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "ðŸ’¡ Use detected functional forms for feature engineering or model selection\n",
      "\n",
      "ðŸŽ¯ Strong Multi-Method Dependencies:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: ensemble: 0.979 \n",
      "     â””â”€ Pearson: 0.917\n",
      "     â””â”€ Spearman: 0.831\n",
      "     â””â”€ Distance: 0.926\n",
      "     â””â”€ MI: 1.129\n",
      "     â””â”€ Strength: very_strong\n",
      "â€¢ x1_normal â†” x2_linear_combo: ensemble: 0.978 \n",
      "     â””â”€ Pearson: 0.739\n",
      "     â””â”€ Spearman: 0.953\n",
      "     â””â”€ Distance: 0.937\n",
      "     â””â”€ MI: 1.414\n",
      "     â””â”€ Strength: very_strong\n",
      "ðŸ’¡ High-confidence relationships - prioritize for modeling\n",
      "\n",
      "ðŸ”— Copula-Based Dependencies:\n",
      "â€¢ x1_normal â†” x2_linear_combo: Ï„: 0.835 \n",
      "     â””â”€ Type: body dependence\n",
      "     â””â”€ Tail coefficient: 0.036\n",
      "â€¢ x3_skewed â†” x8_nonlinear: Ï„: 0.663 \n",
      "     â””â”€ Type: body dependence\n",
      "     â””â”€ Tail coefficient: 0.048\n",
      "ðŸ’¡ Consider copula models for capturing rank-based dependencies\n",
      "\n",
      "ðŸ“ˆ Monotonic Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: Ï„: 0.663 \n",
      "     â””â”€ Type: monotonic\n",
      "â€¢ x1_normal â†” x2_linear_combo: Ï„: 0.835 \n",
      "     â””â”€ Type: monotonic\n",
      "ðŸ’¡ Monotonic transforms, rank-based methods, or ordinal approaches\n",
      "\n",
      "ðŸ”Ž Statistical Distribution Fitting\n",
      "   ------------------------------\n",
      "   â€¢ x1_normal: T âœ…\n",
      "   â€¢ AIC: 11382.99 \n",
      "   â€¢ KS p-value: 0.4202 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x2_linear_combo: Normal âœ…\n",
      "   â€¢ AIC: 13330.59 \n",
      "   â€¢ KS p-value: 0.8224 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x3_skewed: Exponential âœ…\n",
      "   â€¢ AIC: 2773.47 \n",
      "   â€¢ KS p-value: 0.9065 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x4_seasonal: Weibull_Min âŒ\n",
      "   â€¢ AIC: 3259.47 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "   â€¢ x5_count: Lognormal âŒ\n",
      "   â€¢ AIC: 5013.16 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "ðŸ’¡ Use fitted distributions for simulation, anomaly detection, or Bayesian priors\n",
      "\n",
      "ðŸ”Ž Pattern Summary\n",
      "   ------------------------------\n",
      "â€¢ Total Relationship Patterns: 13 \n",
      "   Most Common Pattern Types:\n",
      "   â€¢ Monotonic: 2 relationships\n",
      "   â€¢ Conditional: 2 relationships\n",
      "   â€¢ Regime Switching: 2 relationships\n",
      "\n",
      "ðŸ“‹ Advanced Modeling Recommendations:\n",
      "   â€¢ Consider regime-switching models (Markov switching, threshold VAR)\n",
      "   â€¢ Leverage detected functional forms for feature engineering\n",
      "   â€¢ Explore copula-based models for dependency modeling\n",
      "   â€¢ High-confidence relationships - prioritize for interaction terms\n",
      "   â€¢ Rich relationship structure - consider ensemble methods\n",
      "   â€¢ Very strong dependencies detected - check for potential data leakage\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š OUTLIER DETECTION ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Methods Attempted: 14 \n",
      "â€¢ Successful Methods: 15 \n",
      "â€¢ Overall Outlier Rate: 7.4% ðŸŸ¡\n",
      "â€¢ Best Method: ELLIPTIC_ENVELOPE \n",
      "\n",
      "ðŸ”Ž Analysis Scope\n",
      "   ------------------------------\n",
      "â€¢ Total Samples: 1,500 \n",
      "â€¢ Analyzed Samples: 1,274 \n",
      "â€¢ Features Analyzed: 8 \n",
      "â€¢ Missing Data: 226 (15.1%) \n",
      "\n",
      "ðŸ”Ž Data Preprocessing\n",
      "   ------------------------------\n",
      "â€¢ Scaling Method: Power Transform \n",
      "â€¢ Missing Data Strategy: Complete Cases Only \n",
      "â€¢ Data Skewness: 1.67 (Moderately skewed) ðŸŸ \n",
      "\n",
      "ðŸ” DETAILED METHOD RESULTS\n",
      "--------------------------------------------------\n",
      "\n",
      "   ðŸ”¹ Z Score:\n",
      "   â€¢ Outliers Detected: 26 (1.73%) ðŸŸ¡\n",
      "   â€¢ Separation Quality: 2.116 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Modified Z Score:\n",
      "   â€¢ Outliers Detected: 21 (1.40%) ðŸŸ¡\n",
      "   â€¢ Separation Quality: 2.920 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Iqr:\n",
      "   â€¢ Outliers Detected: 71 (4.73%) ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Pca Recon:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.734 ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Knn Distance:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Dbscan:\n",
      "   â€¢ Outliers Detected: 41 (2.73%) ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Mahalanobis Robust:\n",
      "   â€¢ Outliers Detected: 226 (15.07%) âš ï¸\n",
      "\n",
      "   ðŸ”¹ Isolation Forest:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.089 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Local Outlier Factor:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.274 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ One Class Svm:\n",
      "   â€¢ Outliers Detected: 129 (8.60%) ðŸŸ \n",
      "   â€¢ Separation Quality: 4.277 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Elliptic Envelope:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 44.806 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Ecod:\n",
      "   â€¢ Outliers Detected: 129 (8.60%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.068 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Copod:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 7.401 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Hbos:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 4.704 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Ensemble:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "\n",
      "ðŸ” OUTLIER TREATMENT RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ 1. Recommended method: COPOD (score: 2.20)\n",
      "ðŸ’¡ 2. Healthy outlier rate: 10.0%\n",
      "ðŸ’¡ 3. ðŸŽ¯ Multiple methods succeeded â€” ensemble reliability is high.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CLUSTERING ANALYSIS\n",
      "================================================================================\n",
      "Analysis Summary:\n",
      "â€¢ Methods Attempted: 7 \n",
      "â€¢ Successful Methods: 7 \n",
      "â€¢ Best Method: HIERARCHICAL \n",
      "\n",
      "ðŸ”Ž Optimal Cluster Analysis\n",
      "   ------------------------------\n",
      "â€¢ Estimated Optimal K: 4 \n",
      "â€¢ Confidence Level: Low \n",
      "\n",
      "ðŸ”Ž Method Performance\n",
      "   ------------------------------\n",
      "\n",
      "   ðŸ” Kmeans\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 3 \n",
      "   â€¢ Size Range: 115-632 points \n",
      "   â€¢ Balance Ratio: 0.18 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.163 ðŸŸ \n",
      "   â€¢ Best K (internal): 3 \n",
      "\n",
      "   ðŸ” Hierarchical\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 4 \n",
      "   â€¢ Size Range: 3-1230 points \n",
      "   â€¢ Balance Ratio: 0.00 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.493 ðŸŸ \n",
      "\n",
      "   ðŸ” Dbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 1 \n",
      "   â“˜ Single or no cluster detected (silhouette may be undefined).\n",
      "   â€¢ Size Range: 1207-1207 points \n",
      "   â€¢ Balance Ratio: 1.00 ðŸŸ¢\n",
      "   â€¢ Epsilon (DBSCAN): 1.777 \n",
      "   â€¢ Min Samples (DBSCAN): 5 \n",
      "\n",
      "   ðŸ” Hdbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 5 \n",
      "   â€¢ Size Range: 16-40 points \n",
      "   â€¢ Balance Ratio: 0.40 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.264 ðŸŸ \n",
      "\n",
      "   ðŸ” Gmm\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 7 \n",
      "   â€¢ Size Range: 20-378 points \n",
      "   â€¢ Balance Ratio: 0.05 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.048 ðŸŸ \n",
      "\n",
      "ðŸ” RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ Best method: KMEANS (score: 0.453)\n",
      "ðŸ’¡ Unclear cluster structure â€” verify if clustering is appropriate for this data.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š TIME SERIES ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Total Series: 8 \n",
      "â€¢ Stationary Series: 8/8 (100.0%) \n",
      "â€¢ Forecast-Ready: 3/8 (37.5%) \n",
      "\n",
      "ðŸ”Ž Stationarity Assessment\n",
      "   ------------------------------\n",
      "âœ… Stationary (8):\n",
      "   â€¢ x1_normal: ADF p=0.0000 \n",
      "   â€¢ x2_linear_combo: ADF p=0.0000 \n",
      "   â€¢ x3_skewed: ADF p=0.0000 \n",
      "   ... and 5 more\n",
      "\n",
      "ðŸ”Ž Detected Temporal Patterns\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ”„ Seasonal Patterns:\n",
      "   â€¢ x3_skewed: strength=0.859 \n",
      "   â€¢ x6_bounded: strength=0.818 \n",
      "   â€¢ x5_count: strength=0.727 \n",
      "\n",
      "ðŸ”Ž Lag & Seasonality Suggestions\n",
      "   ------------------------------\n",
      "Per-series:\n",
      "   â€¢ x3_skewed: lags=[9] | P=365 | strength=0.859 \n",
      "   â€¢ x6_bounded: lags=[1] | P=365 | strength=0.818 \n",
      "   â€¢ x5_count: lags=[10] | P=365 | strength=0.727 \n",
      "   â€¢ x2_linear_combo: lags=[10] | P=365 | strength=0.678 \n",
      "   â€¢ x8_nonlinear: lags=[10] | P=365 | strength=0.542 \n",
      "   â€¢ x4_seasonal: lags=[1, 2, 3] | seasonal_lags=[12] | P=365 | strength=0.538 \n",
      "   ... and 2 more\n",
      "\n",
      "Summary:\n",
      "   â€¢ : Top lags: 10(5), 1(2), 9(1) \n",
      "   â€¢ : Top seasonal lags: 12(1) \n",
      "   â€¢ : Top periods: P=365(8) \n",
      "\n",
      "ðŸ”Ž Time Series Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Use seasonal models (e.g., SARIMA, STL)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FEATURE ENGINEERING RECOMMENDATIONS\n",
      "================================================================================\n",
      "Engineering Scope:\n",
      "â€¢ Numeric Features: 8 \n",
      "â€¢ Categorical Features: 2 \n",
      "â€¢ Interaction Candidates: 0 \n",
      "\n",
      "ðŸ”Ž Priority Transformations\n",
      "   ------------------------------\n",
      "ðŸš¨ High Priority:\n",
      "   ðŸ“Œ x8_nonlinear\n",
      "      Issues: skew=3.98, kurtosis=24.33, outliers=10.7%\n",
      "      Top transforms:\n",
      "        1. np.sign(x8_nonlinear)*np.log1p(np.abs(x8_nonlinear))\n",
      "        2. scipy.stats.yeojohnson(x8_nonlinear)[0]\n",
      "   ðŸ“Œ x7_lognorm\n",
      "      Issues: skew=3.41, kurtosis=23.80, outliers=6.2%\n",
      "      Top transforms:\n",
      "        1. np.log1p(x7_lognorm)\n",
      "        2. np.sqrt(x7_lognorm)\n",
      "   ðŸ“Œ x1_normal\n",
      "      Issues: skew=3.43, kurtosis=30.62\n",
      "      Top transforms:\n",
      "        1. np.log1p(x1_normal)\n",
      "        2. np.sqrt(x1_normal)\n",
      "\n",
      "âš ï¸ Medium Priority:\n",
      "   â€¢ x5_count:  â†’ np.tanh(x5_count/(+2.23537+1e-12)) \n",
      "\n",
      "ðŸ”Ž Categorical Encoding Strategy\n",
      "   ------------------------------\n",
      "âœ… Simple (â‰¤5):\n",
      "   â€¢ category_low_card: 3 cats â†’ OneHot(drop_first=True) \n",
      "\n",
      "ðŸ”„ Advanced (6â€“20):\n",
      "   â€¢ category_high_card: 20 cats â†’ OrdinalEncoder \n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š MISSINGNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Missing Value Rates\n",
      "   ------------------------------\n",
      "â€¢ x5_count: 5.7% ðŸŸ¡\n",
      "â€¢ x6_bounded: 5.3% ðŸŸ¡\n",
      "â€¢ x3_skewed: 5.1% ðŸŸ¡\n",
      "âš ï¸ 3 features with 5â€“20% missing\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DIMENSIONALITY ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Embedding Methods: 7 \n",
      "â€¢ Samples Ã— Features: 1274 Ã— 8 \n",
      "â€¢ Condition Number: 2.33e+01 \n",
      "â€¢ Effective Rank: 8 \n",
      "\n",
      "ðŸ”Ž Preprocessing\n",
      "   ------------------------------\n",
      "â€¢ Low-Variance Removed: 0 \n",
      "â€¢ Scaling: standard \n",
      "â€¢ Post-Preproc Shape: 1274 Ã— 8 \n",
      "\n",
      "ðŸ”Ž Method Leaderboard\n",
      "   ------------------------------\n",
      "   â€¢ lle: score=0.390 | sil=0.488 stab=0.973 \n",
      "   â€¢ spectral: score=0.376 | sil=0.443 stab=0.996 \n",
      "   â€¢ factor_analysis: score=0.225 | sil=0.259 stab=0.608 \n",
      "   â€¢ isomap: score=0.217 | sil=0.349 stab=0.384 \n",
      "   â€¢ ica: score=0.178 | sil=0.194 stab=0.500 \n",
      "\n",
      "ðŸ”Ž Per-Method Details\n",
      "   ------------------------------\n",
      "   â€¢ pca: n_comp=4 | var=0.714 | sil=0.198 | stab=0.458 \n",
      "   â€¢ ica: n_comp=4 | sil=0.194 | stab=0.500 \n",
      "   â€¢ factor_analysis: n_comp=4 | sil=0.259 | stab=0.608 \n",
      "   â€¢ tsne: perplexity=50 | sil=0.406 | stab=0.069 \n",
      "   â€¢ isomap: neighbors=50 | sil=0.349 | stab=0.384 \n",
      "   â€¢ lle: neighbors=50 | sil=0.488 | stab=0.973 \n",
      "   â€¢ spectral: neighbors=50 | sil=0.443 | stab=0.996 \n",
      "\n",
      "ðŸ”Ž PCA Summary\n",
      "   ------------------------------\n",
      "â€¢ Variance â‰¥95%: 4 components \n",
      "â€¢ Total Variance Explained: 0.714 \n",
      "\n",
      "ðŸ”Ž 2D Visualizations Available\n",
      "   ------------------------------\n",
      "â€¢ Embeddings: tsne, isomap, lle, spectral \n",
      "\n",
      "ðŸ”Ž Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Best performing method: lle\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CATEGORICAL GROUP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Group Overview: category_low_card\n",
      "   ------------------------------\n",
      "â€¢ Total Groups: 3 \n",
      "â€¢ Total Samples: 1274 \n",
      "   Group Sizes:\n",
      "     â€¢ A: 661 samples\n",
      "     â€¢ B: 375 samples\n",
      "     â€¢ C: 238 samples\n",
      "   â€¢ Groups Balanced: Yes ðŸŸ¢\n",
      "   â€¢ Missing Data: 15.1% âš ï¸\n",
      "\n",
      "ðŸ”Ž Analysis Summary\n",
      "   ------------------------------\n",
      "â€¢ Variables Tested: 8 \n",
      "â€¢ Significant Variables: 2/8 ðŸŸ¡\n",
      "   Variables with significant differences:\n",
      "     â€¢ x3_skewed\n",
      "     â€¢ x4_seasonal\n",
      "\n",
      "ðŸ”Ž Variable-by-Variable Analysis\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ” Analysis for: x1_normal\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=50.939 (Â±12.219), n=661\n",
      "     â€¢ B: Î¼=51.535 (Â±14.587), n=375\n",
      "     â€¢ C: Î¼=51.044 (Â±13.747), n=238\n",
      "   â€¢ Best Test: Pairwise t-tests with Bonferroni correction ðŸŸ \n",
      "      â€¢ P-value: 1.000000 ðŸŸ \n",
      "     â””â”€ âŒ No significant difference\n",
      "   Key Insights:\n",
      "     â€¢ âŒ **No significant difference detected** between groups\n",
      "     â€¢ ðŸ“Š Non-normal distributions detected - non-parametric tests recommended\n",
      "\n",
      "ðŸ” Analysis for: x2_linear_combo\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=101.027 (Â±20.357), n=661\n",
      "     â€¢ B: Î¼=100.879 (Â±21.036), n=375\n",
      "     â€¢ C: Î¼=100.612 (Â±20.642), n=238\n",
      "   â€¢ Best Test: One-way ANOVA ðŸŸ \n",
      "      â€¢ P-value: 0.964868 ðŸŸ \n",
      "      â€¢ Effect Size: Î·Â² = 0.000 (negligible) \n",
      "     â””â”€ âŒ No significant difference\n",
      "   Key Insights:\n",
      "     â€¢ âŒ **No significant difference detected** between groups\n",
      "     â€¢ ðŸ¤” Difference may be statistically significant but practically small\n",
      "\n",
      "ðŸ” Analysis for: x3_skewed\n",
      "   Group Statistics:\n",
      "     â€¢ A: Î¼=0.976 (Â±0.956), n=661\n",
      "     â€¢ B: Î¼=0.895 (Â±0.970), n=375\n",
      "     â€¢ C: Î¼=1.059 (Â±1.004), n=238\n",
      "   â€¢ Best Test: Kruskal-Wallis H-test ðŸŸ¡\n",
      "      â€¢ P-value: 0.011480 ðŸŸ¡\n",
      "     â””â”€ âœ… Significant difference detected\n",
      "   Key Insights:\n",
      "     â€¢ ðŸŽ¯ **Significant difference detected** using Kruskal-Wallis H-test (p = 0.011480)\n",
      "     â€¢ ðŸ“Š Non-normal distributions detected - non-parametric tests recommended\n",
      "\n",
      "ðŸ”Ž Categorical Variable Associations\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ“Š category_low_card â†” category_high_card\n",
      "   â€¢ Chi-square p-value: 0.910868 ðŸŸ \n",
      "   â€¢ CramÃ©r's V: 0.095 (negligible) \n",
      "     â””â”€ âŒ No significant association\n",
      "\n",
      "ðŸ”Ž Statistical Methods Performance\n",
      "   ------------------------------\n",
      "   Method Usage and Success Rates:\n",
      "     â€¢ One-way ANOVA: 8 uses, 0 significant (0%)\n",
      "     â€¢ Welch's ANOVA (unequal variances): 8 uses, 0 significant (0%)\n",
      "     â€¢ Pairwise t-tests with Bonferroni correction: 8 uses, 0 significant (0%)\n",
      "     â€¢ Kruskal-Wallis H-test: 8 uses, 2 significant (25%)\n",
      "     â€¢ Pairwise Mann-Whitney tests with Bonferroni correction: 8 uses, 0 significant (0%)\n",
      "\n",
      "ðŸ”Ž Key Recommendations\n",
      "   ------------------------------\n",
      "   1. ðŸŽ¯ **2 variables** show significant differences between category_low_card groups\n",
      "   2. ðŸ“Š Variables with differences: x3_skewed, x4_seasonal\n",
      "\n",
      "ðŸ”Ž Statistical Assumptions Summary\n",
      "   ------------------------------\n",
      "â€¢ Variables with Normal Distributions: 1/8 (12%) âš ï¸\n",
      "â€¢ Variables with Equal Variances: 8/8 (100%) ðŸŸ¢\n",
      "   ðŸ’¡ Many variables violate normality - non-parametric methods recommended\n",
      "\n",
      "ðŸ”Ž ðŸŽ¯ CONCLUSION\n",
      "   ------------------------------\n",
      "   ðŸ“Š SOME variables (2/8) show significant differences between category_low_card\n",
      "   ðŸ“ˆ Consider using category_low_card as a strong predictor in your models\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š GRAPH NETWORK ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Network Construction Overview\n",
      "   ------------------------------\n",
      "â€¢ Original Data: 1500 samples Ã— 10 features \n",
      "â€¢ Analyzed Data: 1274 samples Ã— 8 features \n",
      "\n",
      "ðŸ”Ž Network Summary\n",
      "   ------------------------------\n",
      "â€¢ Graph Construction Success: 3/3 (100%) ðŸŸ¢\n",
      "   ðŸ† Best Network Type: Knn\n",
      "   â€¢ Network Size: 8 nodes, 18 edges \n",
      "   â€¢ Network Density: 0.643 ðŸŸ¢\n",
      "   â€¢ Connectivity: Fully Connected ðŸŸ¢\n",
      "   â€¢ Clustering Coefficient: 0.783 ðŸŸ¢\n",
      "\n",
      "ðŸ”Ž Network Topology Analysis\n",
      "   ------------------------------\n",
      "ðŸ”— Connectivity Analysis:\n",
      "   â€¢ Network Diameter: 3 \n",
      "   â€¢ Average Path Length: 1.39 \n",
      "   â€¢ Network Radius: 2 \n",
      "     â””â”€ âš¡ Efficient information flow\n",
      "\n",
      "ðŸŒ Small World Network:\n",
      "   â€¢ Small-world Ïƒ: 1.21 ðŸŸ¢\n",
      "     â””â”€ âœ… Efficient global connectivity with local clustering\n",
      "\n",
      "ðŸ“Š Degree Distribution:\n",
      "   â€¢ Mean Degree: 4.5 \n",
      "   â€¢ Degree Range: 3 - 6 \n",
      "   â€¢ Degree Std Dev: 1.1 \n",
      "   â€¢ Degree Assortativity: -0.184 ðŸŸ¡\n",
      "     â””â”€ ðŸ“‰ Dissimilar-degree nodes tend to connect\n",
      "\n",
      "ðŸ”Ž Community Structure Analysis\n",
      "   ------------------------------\n",
      "ðŸ˜ï¸ Community Detection Results:\n",
      "   â€¢ Best Method: Louvain \n",
      "   â€¢ Communities Found: 2 \n",
      "   â€¢ Modularity Score: 0.083 ðŸŸ \n",
      "     â””â”€ ðŸ” Weak community structure\n",
      "   ðŸ“‹ All Community Detection Results:\n",
      "     â€¢ Louvain: 2 communities (Q=0.083)\n",
      "     â€¢ Girvan Newman: 2 communities (Q=-0.000)\n",
      "     â€¢ Spectral: 2 communities (Q=0.061)\n",
      "     â€¢ Label Propagation: 1 communities (Q=-0.000)\n",
      "\n",
      "ðŸ”Ž Node Importance Analysis\n",
      "   ------------------------------\n",
      "â­ Most Important Variables:\n",
      "   â€¢ Degree: x5_count (0.857)\n",
      "     â””â”€ Measures local connectivity\n",
      "   â€¢ Pagerank: x3_skewed (0.208)\n",
      "   â€¢ Betweenness: x5_count (0.167)\n",
      "   â€¢ Eigenvector: x5_count (0.440)\n",
      "   â€¢ Closeness: x5_count (0.875)\n",
      "\n",
      "ðŸ‘‘ Top 3 Most Influential Variables:\n",
      "     1. x3_skewed (PageRank: 0.2081)\n",
      "     2. x6_bounded (PageRank: 0.2064)\n",
      "     3. x4_seasonal (PageRank: 0.1710)\n",
      "\n",
      "ðŸ”Ž Graph Construction Methods Comparison\n",
      "   ------------------------------\n",
      "ðŸ“Š Network Construction Results:\n",
      "   â€¢ Correlation: 8 nodes, 2 edges\n",
      "     â””â”€ Edge types: correlation\n",
      "     â””â”€ Density: 0.071\n",
      "   â€¢ Mutual Info: 8 nodes, 7 edges\n",
      "     â””â”€ Edge types: mutual_info\n",
      "     â””â”€ Density: 0.250\n",
      "   â€¢ Knn: 8 nodes, 18 edges\n",
      "     â””â”€ Edge types: knn\n",
      "     â””â”€ Density: 0.643\n",
      "\n",
      "ðŸ”Ž Key Insights & Recommendations\n",
      "   ------------------------------\n",
      "   1. ðŸ† **Best graph structure**: Knn Network\n",
      "   2. ðŸ”— **Highly connected network** - variables form a cohesive system\n",
      "   3. âš¡ **Short path lengths** - information flows efficiently through network\n",
      "   4. ðŸ”˜ **High clustering** - variables form tight-knit groups\n",
      "   5. ðŸŒ **Small-world network** - efficient global connectivity with local clustering\n",
      "   6. ðŸš€ Ideal for feature engineering and dimensionality reduction\n",
      "\n",
      "ðŸ”Ž ðŸŽ¯ NETWORK ANALYSIS CONCLUSION\n",
      "   ------------------------------\n",
      "   ðŸŒŸ STRONG network structure detected with 8 interconnected variables\n",
      "   ðŸ”— Rich connectivity (18 relationships) enables advanced graph-based analysis\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPREHENSIVE DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "Quality Dimensions:\n",
      "â€¢ Completeness: 98.4% ðŸŸ¢\n",
      "â€¢ Uniqueness: 100.0% ðŸŸ¢\n",
      "â€¢ Consistency: 65.0% ðŸ”´\n",
      "â€¢ Validity: 63.0% ðŸ”´\n",
      "\n",
      "ðŸ”Ž Categorical Feature Health\n",
      "   ------------------------------\n",
      "   â€¢ category_low_card:\n",
      "   â€¢ Unique Values: 3 (0.2%) ðŸŸ¡\n",
      "   â€¢ category_high_card:\n",
      "   â€¢ Unique Values: 20 (1.3%) ðŸŸ¡\n",
      "\n",
      "ðŸ”Ž Overall Quality Summary\n",
      "   ------------------------------\n",
      "â€¢ Overall Quality Score: 81.6% \n",
      "   ðŸŸ¡ Good - minor preprocessing\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š EXECUTIVE RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Action Items by Priority\n",
      "   ------------------------------\n",
      "\n",
      "   âš ï¸ MEDIUM PRIORITY:\n",
      "   âš ï¸ Consider transformations for skewed features\n",
      "   âš ï¸ Review and validate detected outliers\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ANALYSIS COMPLETION SUMMARY\n",
      "================================================================================\n",
      "âœ… Completed Analyses: 9\n",
      "   â€¢ Distribution Analysis\n",
      "   â€¢ Correlation Analysis\n",
      "   â€¢ Pattern Detection\n",
      "   â€¢ Feature Engineering\n",
      "   â€¢ Time Series Analysis\n",
      "   â€¢ Clustering Analysis\n",
      "   â€¢ Outlier Detection\n",
      "   â€¢ Missingness Analysis\n",
      "   â€¢ Dimensionality Reduction\n",
      "\n",
      "ðŸ“Š Dataset Shape: 1,500 rows Ã— 10 columns\n",
      "ðŸ’¾ Memory Usage: 0.29 MB\n",
      "ðŸŽ¯ Overall Quality: 81.6% (Good)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ COMPREHENSIVE DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "ðŸ” Use these insights to guide preprocessing and modeling.\n",
      "ðŸ“ˆ Consider the priority recommendations for optimal results.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "from foreminer.foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
