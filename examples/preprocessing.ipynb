{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from foreblocks import TimeSeriesPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "# Create a time series with trend, seasonality, and noise\n",
    "t = np.linspace(0, 4*np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)  # Daily pattern\n",
    "seasonality2 = 1 * np.sin(t/24)  # Weekly pattern\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Combine components\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# Create a second feature (to demonstrate multivariate capabilities)\n",
    "data2 = (0.5 * trend + 1.5 * np.cos(t) + 0.5 * np.random.normal(0, 0.3, n_samples)).reshape(-1, 1)\n",
    "data = np.hstack([data, data2])  # Now we have shape [n_samples, 2]\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[outlier_indices] = data[outlier_indices] + 5 * np.random.randn(10, 2)\n",
    "\n",
    "# Add some missing values (but not too many)\n",
    "missing_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[missing_indices, 0] = np.nan  # Only make some values missing in first feature\n",
    "\n",
    "# Create preprocessor with various techniques enabled\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    normalize=True,\n",
    "    differencing=False,\n",
    "    detrend=True,\n",
    "    apply_ewt=True,\n",
    "    window_size=24,\n",
    "    horizon=12,\n",
    "    remove_outliers=True,\n",
    "    outlier_threshold=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    impute_method=\"iterative\",\n",
    "    ewt_bands=5,\n",
    "    trend_imf_idx=0,\n",
    "    log_transform=False,\n",
    "    filter_window=5,\n",
    "    filter_polyorder=2,\n",
    "    apply_filter=True,\n",
    "    self_tune=True,\n",
    "    apply_imputation=True,\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X, y, processed_data, _ = preprocessor.fit_transform(data, time_stamps=timestamps)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Data with Outliers and Missing Values')\n",
    "plt.plot(data)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Processed Data')\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "plt.plot(processed_data)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('EWT Components')\n",
    "ewt_components = preprocessor.get_ewt_components()\n",
    "if ewt_components:\n",
    "    for i, imf in enumerate(ewt_components[0].T):\n",
    "        plt.plot(imf, label=f'IMF {i}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Target sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Fix path ─────────────────────────────\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# ─── Synthetic time series ────────────────\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)\n",
    "seasonality2 = 1 * np.sin(t / 24)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# ─── Import feature extractor ─────────────\n",
    "from foreblocks.ts_fengine import SignalProcessor\n",
    "\n",
    "fengine = SignalProcessor()\n",
    "\n",
    "# ─── Feature extraction ───────────────────\n",
    "window_size = 48\n",
    "step_size = 24\n",
    "signals = {'signal': data.flatten()}\n",
    "labels = {'signal': 0}  # dummy label\n",
    "\n",
    "features, feature_labels, raw_windows, window_labels = fengine.process_signals(\n",
    "    signals, labels, window_size=window_size, step_size=step_size, augment=True\n",
    ")\n",
    "\n",
    "selected_names = fengine.get_selected_feature_names()\n",
    "print(f\"Selected {len(selected_names)} features:\")\n",
    "print(selected_names)\n",
    "\n",
    "# ─── Create dataframe ─────────────────────\n",
    "feature_names = fengine.feature_engineer.get_feature_names()\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "#features_df.index = timestamps[window_size - 1::step_size][:len(features_df)]\n",
    "\n",
    "# ─── Display ──────────────────────────────\n",
    "print(features_df.head())\n",
    "print(feature_names[:5], \"...\")  # preview names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc85fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n",
      "🔍 Initialized analyzer with 1,500 rows × 10 columns\n",
      "   • Numeric features: 8\n",
      "   • Categorical features: 2\n",
      "🔍 🚀 Starting comprehensive dataset analysis...\n",
      "🔍 Running distributions analysis...\n",
      "🔍 Running correlations analysis...\n",
      "🔍 Running outliers analysis...\n",
      "🔍 Running clusters analysis...\n",
      "🔍 Running dimensionality analysis...\n",
      "🔍 Running patterns analysis...\n",
      "🔍 Running missingness analysis...\n",
      "🔍 Running feature_engineering analysis...\n",
      "🔍 Running shap_explanation analysis...\n",
      "🔍 Running categorical_groups analysis...\n",
      "🔍 Running graph_analysis analysis...\n",
      "🔍 Running timeseries analysis...\n",
      "interval columns not set, guessing: ['x1_normal', 'x2_linear_combo', 'x3_skewed', 'x4_seasonal', 'x5_count', 'x6_bounded', 'x7_lognorm', 'x8_nonlinear']\n",
      "\n",
      "================================================================================\n",
      "📊 EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "Dataset Overview:\n",
      "• Shape: 1,500 rows × 10 columns \n",
      "• Memory Usage: 0.29 MB \n",
      "• Numeric Features: 8 \n",
      "• Categorical Features: 2 \n",
      "• Missing Values: 1.61% 🟡\n",
      "\n",
      "================================================================================\n",
      "📊 DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "Statistical Properties:\n",
      "• Normal Distributions: 1/8 (12.5%) \n",
      "• Skewed Distributions: 4/8 (50.0%) \n",
      "• Heavy-Tailed: 5/8 (62.5%) \n",
      "• Average Outlier Rate: 0.96% \n",
      "\n",
      "🔎 Feature Quality Assessment\n",
      "   ------------------------------\n",
      "✅ Normal Distributions (1):\n",
      "   x2_linear_combo\n",
      "💡 Safe for parametric methods and linear models\n",
      "\n",
      "⚠️ Highly Skewed Features (3):\n",
      "   • x1_normal: 3.43 (right-skewed)\n",
      "   • x7_lognorm: 3.41 (right-skewed)\n",
      "   • x8_nonlinear: 3.98 (right-skewed)\n",
      "⚠️ Apply log/sqrt transform or use robust methods\n",
      "\n",
      "🔀 Potential Bimodal Distributions (3):\n",
      "   x3_skewed, x4_seasonal, x8_nonlinear\n",
      "💡 Investigate mixtures or stratify data\n",
      "\n",
      "🔎 Preprocessing Recommendations\n",
      "   ------------------------------\n",
      "💡 Non-normal dominant — consider robust/non-parametric methods\n",
      "\n",
      "================================================================================\n",
      "📊 CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Correlation Summary:\n",
      "• Strong Positive (>0.7): 2 \n",
      "• Strong Negative (<-0.7): 0 \n",
      "• Moderate (0.5–0.7): 0 \n",
      "\n",
      "🔎 Strong Positive Correlations\n",
      "   ------------------------------\n",
      "• x1_normal ↔ x2_linear_combo: 0.739 \n",
      "• x3_skewed ↔ x8_nonlinear: 0.917 \n",
      "⚠️ Consider dimensionality reduction or feature selection\n",
      "\n",
      "================================================================================\n",
      "📊 ADVANCED PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "🔎 Feature Type Classification\n",
      "   ------------------------------\n",
      "• Gaussian: 1 features \n",
      "   x2_linear_combo\n",
      "• Log Normal Candidates: 3 features \n",
      "   x1_normal, x2_linear_combo, x7_lognorm\n",
      "• Bounded: 1 features \n",
      "   x6_bounded\n",
      "• Count Like: 1 features \n",
      "   x5_count\n",
      "• Continuous: 5 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x4_seasonal, x7_lognorm\n",
      "• Highly Skewed: 3 features \n",
      "   x1_normal, x8_nonlinear, x7_lognorm\n",
      "• Heavy Tailed: 5 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x7_lognorm, x5_count\n",
      "• Mixture Model: 7 features \n",
      "   x1_normal, x3_skewed, x8_nonlinear, x4_seasonal, x6_bounded ... and 2 more\n",
      "• Power Law: 3 features \n",
      "   x1_normal, x3_skewed, x7_lognorm\n",
      "• Bimodal: 8 features \n",
      "   x1_normal, x2_linear_combo, x3_skewed, x8_nonlinear, x4_seasonal ... and 3 more\n",
      "• Uniform Like: 3 features \n",
      "   x2_linear_combo, x4_seasonal, x5_count\n",
      "• Transformable To Normal: 1 features \n",
      "   x7_lognorm\n",
      "\n",
      "🔎 Transformation Opportunities\n",
      "   ------------------------------\n",
      "🔄 Transformable: x7_lognorm\n",
      "💡 Apply Box-Cox or Yeo-Johnson\n",
      "\n",
      "🔎 Relationship Patterns\n",
      "   ------------------------------\n",
      "\n",
      "🌀 Non-Linear Relationships:\n",
      "• x1_normal ↔ x2_linear_combo: score: 0.343 \n",
      "     └─ Best fit: Quadratic (R²=0.894)\n",
      "💡 Try polynomial/kernels or specific functional forms\n",
      "\n",
      "🧬 Complex Dependencies (High MI, Low Linear):\n",
      "• x2_linear_combo ↔ x5_count: MI: 0.192 \n",
      "     └─ Ensemble strength: 0.068\n",
      "• x5_count ↔ x6_bounded: MI: 0.172 \n",
      "     └─ Ensemble strength: 0.060\n",
      "• x3_skewed ↔ x5_count: MI: 0.119 \n",
      "     └─ Ensemble strength: 0.041\n",
      "💡 Explore interactions, non-linear models, or copula-based approaches\n",
      "\n",
      "🔄 Regime-Switching Relationships:\n",
      "• x1_normal ↔ x2_linear_combo: regime diff: 0.975 \n",
      "     └─ Low regime corr: 0.946\n",
      "     └─ High regime corr: -0.029\n",
      "     └─ Split at: 56.97\n",
      "     └─ Strength: strong\n",
      "• x3_skewed ↔ x8_nonlinear: regime diff: 0.849 \n",
      "     └─ Low regime corr: 0.089\n",
      "     └─ High regime corr: 0.938\n",
      "     └─ Split at: 0.28\n",
      "     └─ Strength: strong\n",
      "💡 Consider regime-aware models, mixture models, or threshold effects\n",
      "\n",
      "📐 Detected Functional Relationships:\n",
      "• x3_skewed ↔ x8_nonlinear: R²: 0.983 \n",
      "     └─ Complexity: moderate\n",
      "     └─ Alternatives: \n",
      "• x1_normal ↔ x2_linear_combo: R²: 0.894 \n",
      "     └─ Complexity: moderate\n",
      "     └─ Alternatives: \n",
      "💡 Use detected functional forms for feature engineering or model selection\n",
      "\n",
      "🎯 Strong Multi-Method Dependencies:\n",
      "• x1_normal ↔ x2_linear_combo: ensemble: 0.892 \n",
      "     └─ Pearson: 0.739\n",
      "     └─ Spearman: 0.953\n",
      "     └─ Distance: 0.937\n",
      "     └─ MI: 0.941\n",
      "     └─ Strength: strong\n",
      "• x3_skewed ↔ x8_nonlinear: ensemble: 0.838 \n",
      "     └─ Pearson: 0.917\n",
      "     └─ Spearman: 0.831\n",
      "     └─ Distance: 0.926\n",
      "     └─ MI: 0.677\n",
      "     └─ Strength: strong\n",
      "💡 High-confidence relationships - prioritize for modeling\n",
      "\n",
      "📈 Monotonic Relationships:\n",
      "• x1_normal ↔ x2_linear_combo: τ: 0.835 \n",
      "     └─ Type: monotonic\n",
      "     └─ Spearman: 0.953\n",
      "• x3_skewed ↔ x8_nonlinear: τ: 0.663 \n",
      "     └─ Type: monotonic\n",
      "     └─ Spearman: 0.831\n",
      "💡 Monotonic transforms, rank-based methods, or ordinal approaches\n",
      "\n",
      "🔎 Statistical Distribution Fitting\n",
      "   ------------------------------\n",
      "   • x1_normal: T ✅\n",
      "   • AIC: 11382.99 \n",
      "   • KS p-value: 0.4202 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x2_linear_combo: Normal ✅\n",
      "   • AIC: 13330.59 \n",
      "   • KS p-value: 0.8224 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x3_skewed: Exponential ✅\n",
      "   • AIC: 2773.47 \n",
      "   • KS p-value: 0.9065 🟢\n",
      "     └─ Fit Quality: Excellent\n",
      "   • x4_seasonal: Weibull_Min ❌\n",
      "   • AIC: 3259.47 \n",
      "   • KS p-value: 0.0000 🔴\n",
      "     └─ Fit Quality: Poor\n",
      "   • x5_count: Lognormal ❌\n",
      "   • AIC: 5013.16 \n",
      "   • KS p-value: 0.0000 🔴\n",
      "     └─ Fit Quality: Poor\n",
      "💡 Use fitted distributions for simulation, anomaly detection, or Bayesian priors\n",
      "\n",
      "🔎 Pattern Summary\n",
      "   ------------------------------\n",
      "• Total Relationship Patterns: 13 \n",
      "   Most Common Pattern Types:\n",
      "   • Complex: 4 relationships\n",
      "   • Monotonic: 2 relationships\n",
      "   • Regime Switching: 2 relationships\n",
      "\n",
      "📋 Advanced Modeling Recommendations:\n",
      "   • Consider regime-switching models (Markov switching, threshold VAR)\n",
      "   • Leverage detected functional forms for feature engineering\n",
      "   • High-confidence relationships - prioritize for interaction terms\n",
      "   • Rich relationship structure - consider ensemble methods\n",
      "   • Very strong dependencies detected - check for potential data leakage\n",
      "\n",
      "================================================================================\n",
      "📊 OUTLIER DETECTION ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "• Methods Attempted: 6 \n",
      "• Successful Methods: 7 \n",
      "• Overall Outlier Rate: 8.5% 🟡\n",
      "• Best Method: COPOD \n",
      "\n",
      "🔎 Analysis Scope\n",
      "   ------------------------------\n",
      "• Total Samples: 1,500 \n",
      "• Analyzed Samples: 1,274 \n",
      "• Features Analyzed: 8 \n",
      "• Missing Data: 226 (15.1%) \n",
      "\n",
      "🔎 Data Preprocessing\n",
      "   ------------------------------\n",
      "• Scaling Method: Power Transform \n",
      "• Missing Data Strategy: Complete Cases Only \n",
      "• Data Skewness: 1.67 (Moderately skewed) 🟠\n",
      "\n",
      "🔍 DETAILED METHOD RESULTS\n",
      "--------------------------------------------------\n",
      "\n",
      "   🔹 Pca Recon:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.734 🟡\n",
      "\n",
      "   🔹 Isolation Forest:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.093 🟠\n",
      "\n",
      "   🔹 Ecod:\n",
      "   • Outliers Detected: 129 (8.60%) 🟠\n",
      "   • Separation Quality: 0.068 🟠\n",
      "\n",
      "   🔹 Copod:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 7.401 🟢\n",
      "\n",
      "   🔹 Hbos:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 4.704 🟢\n",
      "\n",
      "   🔹 Aeod:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.078 🟠\n",
      "\n",
      "   🔹 Ensemble:\n",
      "   • Outliers Detected: 128 (8.53%) 🟠\n",
      "   • Separation Quality: 0.032 🟠\n",
      "\n",
      "🔍 OUTLIER TREATMENT RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "💡 1. Recommended method: COPOD\n",
      "💡 2. 🎯 Multiple methods succeeded — ensemble reliability is high.\n",
      "\n",
      "================================================================================\n",
      "📊 CLUSTERING ANALYSIS\n",
      "================================================================================\n",
      "Analysis Summary:\n",
      "• Methods Attempted: 6 \n",
      "• Successful Methods: 6 \n",
      "• Best Method: KMEANS \n",
      "\n",
      "🔎 Optimal Cluster Analysis\n",
      "   ------------------------------\n",
      "• Estimated Optimal K: 4 \n",
      "• Confidence Level: Low \n",
      "\n",
      "🔎 Method Performance\n",
      "   ------------------------------\n",
      "\n",
      "   🔍 Kmeans\n",
      "      ------------------------------\n",
      "   • Clusters Found: 3 \n",
      "   • Size Range: 188-584 points \n",
      "   • Balance Ratio: 0.32 🟠\n",
      "   • Silhouette Score: 0.175 🟠\n",
      "\n",
      "   🔍 Dbscan\n",
      "      ------------------------------\n",
      "   • Clusters Found: 1 \n",
      "   ⓘ Single or no cluster detected (silhouette may be undefined).\n",
      "   • Size Range: 1251-1251 points \n",
      "   • Balance Ratio: 1.00 🟢\n",
      "   • Epsilon (DBSCAN): 2.168 \n",
      "   • Min Samples (DBSCAN): 8 \n",
      "\n",
      "   🔍 Hdbscan\n",
      "      ------------------------------\n",
      "   • Clusters Found: 2 \n",
      "   • Size Range: 28-197 points \n",
      "   • Balance Ratio: 0.14 🟠\n",
      "   • Silhouette Score: 0.135 🟠\n",
      "\n",
      "   🔍 Gaussian Mixture\n",
      "      ------------------------------\n",
      "   • Clusters Found: 5 \n",
      "   • Size Range: 12-585 points \n",
      "   • Balance Ratio: 0.02 🟠\n",
      "   • Silhouette Score: 0.100 🟠\n",
      "\n",
      "   🔍 Spectral\n",
      "      ------------------------------\n",
      "   • Clusters Found: 4 \n",
      "   • Size Range: 148-407 points \n",
      "   • Balance Ratio: 0.36 🟠\n",
      "   • Silhouette Score: 0.143 🟠\n",
      "\n",
      "🔍 RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "💡 Best performing method: ENSEMBLE (score: 0.454)\n",
      "💡 Moderate consensus - consider inspecting individual method results\n",
      "💡 Unclear cluster structure - may need feature engineering or different approach\n",
      "💡 Outliers detected and handled during preprocessing\n",
      "\n",
      "================================================================================\n",
      "📊 TIME SERIES ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "• Total Series: 8 \n",
      "• Stationary Series: 8/8 (100.0%) \n",
      "• Forecast-Ready: 7/8 (87.5%) \n",
      "\n",
      "🔎 Stationarity Assessment\n",
      "   ------------------------------\n",
      "✅ Stationary (8):\n",
      "   • x1_normal: ADF p=0.0000 \n",
      "   • x2_linear_combo: ADF p=0.0000 \n",
      "   • x3_skewed: ADF p=0.0000 \n",
      "   ... and 5 more\n",
      "\n",
      "🔎 Detected Temporal Patterns\n",
      "   ------------------------------\n",
      "📈 Significant Trends:\n",
      "   • x4_seasonal: R²=0.005 \n",
      "\n",
      "🔄 Seasonal Patterns:\n",
      "   • x3_skewed: strength=0.859 \n",
      "   • x6_bounded: strength=0.818 \n",
      "   • x5_count: strength=0.727 \n",
      "\n",
      "🔎 Lag & Seasonality Suggestions\n",
      "   ------------------------------\n",
      "Per-series:\n",
      "   • x3_skewed: P=365 | strength=0.859 \n",
      "   • x6_bounded: lags=[15] | P=365 | strength=0.818 \n",
      "   • x5_count: P=365 | strength=0.727 \n",
      "   • x2_linear_combo: lags=[8] | P=365 | strength=0.678 \n",
      "   • x8_nonlinear: lags=[20] | P=365 | strength=0.542 \n",
      "   • x4_seasonal: lags=[1, 2, 3] | seasonal_lags=[12] | P=365 | strength=0.538 \n",
      "   ... and 2 more\n",
      "\n",
      "Summary:\n",
      "   • : Top lags: 6(1), 10(1), 8(1) \n",
      "   • : Top seasonal lags: 12(1) \n",
      "   • : Top periods: P=365(8) \n",
      "\n",
      "🔎 Time Series Recommendations\n",
      "   ------------------------------\n",
      "💡 Well-suited for forecasting\n",
      "💡 Use seasonal models (e.g., SARIMA, STL)\n",
      "💡 Include trend terms or detrend\n",
      "\n",
      "================================================================================\n",
      "📊 FEATURE ENGINEERING RECOMMENDATIONS\n",
      "================================================================================\n",
      "Engineering Scope:\n",
      "• Numeric Features: 8 \n",
      "• Categorical Features: 2 \n",
      "• Interaction Candidates: 30 \n",
      "\n",
      "🔎 Priority Transformations\n",
      "   ------------------------------\n",
      "🚨 High Priority:\n",
      "   📌 x1_normal\n",
      "      Issues: skew=3.43, kurtosis=30.62\n",
      "      Top transforms:\n",
      "        1. QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42).fit_transform(x1_normal.values.reshape(-1,1)).ravel()\n",
      "        2. QuantileTransformer(output_distribution='uniform', n_quantiles=100, random_state=42).fit_transform(x1_normal.values.reshape(-1,1)).ravel()\n",
      "   📌 x7_lognorm\n",
      "      Issues: skew=3.41, kurtosis=23.80\n",
      "      Top transforms:\n",
      "        1. PowerTransformer(method='box-cox').fit_transform(x7_lognorm.values.reshape(-1,1)).ravel()\n",
      "        2. np.arcsinh(x7_lognorm)\n",
      "   📌 x8_nonlinear\n",
      "      Issues: skew=3.98, kurtosis=24.33\n",
      "      Top transforms:\n",
      "        1. QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42).fit_transform(x8_nonlinear.values.reshape(-1,1)).ravel()\n",
      "        2. PowerTransformer(method='yeo-johnson').fit_transform(x8_nonlinear.values.reshape(-1,1)).ravel()\n",
      "\n",
      "⚠️ Medium Priority:\n",
      "   • x3_skewed: skew=1.77 → PowerTransformer(method='box-cox').fit_transform(x3_skewed.values.reshape(-1,1)).ravel() \n",
      "   • x5_count:  → PowerTransformer(method='yeo-johnson').fit_transform(x5_count.values.reshape(-1,1)).ravel() \n",
      "\n",
      "🔎 Categorical Encoding Strategy\n",
      "   ------------------------------\n",
      "✅ Simple (≤5):\n",
      "   • category_low_card: 3 cats → OneHot(drop_first=True) \n",
      "\n",
      "🔄 Advanced (6–20):\n",
      "   • category_high_card: 20 cats → OneHot(drop_first=False) \n",
      "\n",
      "🔎 Interaction Features\n",
      "   ------------------------------\n",
      "• Total Candidates: 30 \n",
      "💡 Start with top 10 interactions\n",
      "   • x1_normal*x2_linear_combo\n",
      "   • x1_normal/(x2_linear_combo+1e-8)\n",
      "   • x2_linear_combo/(x1_normal+1e-8)\n",
      "   • abs(x1_normal-x2_linear_combo)\n",
      "   • (x1_normal+x2_linear_combo)/2\n",
      "   • np.sqrt(x1_normal**2+x2_linear_combo**2)\n",
      "   • np.minimum(x1_normal,x2_linear_combo)\n",
      "   • np.maximum(x1_normal,x2_linear_combo)\n",
      "   • (x1_normal+x2_linear_combo)**2\n",
      "   • (x1_normal-x2_linear_combo)**2\n",
      "\n",
      "================================================================================\n",
      "📊 MISSINGNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔎 Missing Value Rates\n",
      "   ------------------------------\n",
      "• x5_count: 5.7% 🟡\n",
      "• x6_bounded: 5.3% 🟡\n",
      "• x3_skewed: 5.1% 🟡\n",
      "⚠️ 3 features with 5–20% missing\n",
      "\n",
      "================================================================================\n",
      "📊 DIMENSIONALITY ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "• Embedding Methods: 7 \n",
      "• Samples × Features: 1274 × 8 \n",
      "• Condition Number: 4.83e+00 \n",
      "• Effective Rank: 8 \n",
      "\n",
      "🔎 Preprocessing\n",
      "   ------------------------------\n",
      "• Low-Variance Removed: 0 \n",
      "• Scaling: standard \n",
      "• Post-Preproc Shape: 1274 × 8 \n",
      "\n",
      "🔎 Method Leaderboard\n",
      "   ------------------------------\n",
      "   • tsne: score=0.269 | sil=0.422 stab=0.500 \n",
      "   • spectral: score=0.263 | sil=0.407 stab=0.500 \n",
      "   • isomap: score=0.230 | sil=0.324 stab=0.500 \n",
      "   • lle: score=0.228 | sil=0.320 stab=0.501 \n",
      "   • factor_analysis: score=0.216 | sil=0.291 stab=0.501 \n",
      "\n",
      "🔎 Per-Method Details\n",
      "   ------------------------------\n",
      "   • pca: n_comp=3 | var=0.589 | sil=0.252 | stab=0.501 \n",
      "   • ica: n_comp=3 | sil=0.253 | stab=0.501 \n",
      "   • factor_analysis: n_comp=3 | sil=0.291 | stab=0.501 \n",
      "   • tsne: sil=0.422 | stab=0.500 \n",
      "   • isomap: sil=0.324 | stab=0.500 \n",
      "   • lle: sil=0.320 | stab=0.501 \n",
      "   • spectral: sil=0.407 | stab=0.500 \n",
      "\n",
      "🔎 PCA Summary\n",
      "   ------------------------------\n",
      "• Variance ≥95%: 3 components \n",
      "• Total Variance Explained: 0.589 \n",
      "\n",
      "🔎 2D Visualizations Available\n",
      "   ------------------------------\n",
      "• Embeddings: tsne, isomap, lle, spectral \n",
      "\n",
      "🔎 Recommendations\n",
      "   ------------------------------\n",
      "💡 Best performing method: ica\n",
      "💡 Pre-reduced to 3 dims for stability and speed.\n",
      "\n",
      "================================================================================\n",
      "📊 CATEGORICAL GROUP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔎 Group Overview: category_low_card\n",
      "   ------------------------------\n",
      "• Total Groups: 3 \n",
      "• Total Samples: 1274 \n",
      "   Group Sizes:\n",
      "     • A: 661 samples\n",
      "     • B: 375 samples\n",
      "     • C: 238 samples\n",
      "   • Groups Balanced: Yes 🟢\n",
      "   • Missing Data: 15.1% ⚠️\n",
      "\n",
      "🔎 Analysis Summary\n",
      "   ------------------------------\n",
      "• Variables Tested: 8 \n",
      "• Significant Variables: 2/8 🟡\n",
      "   Variables with significant differences:\n",
      "     • x3_skewed\n",
      "     • x4_seasonal\n",
      "\n",
      "🔎 Variable-by-Variable Analysis\n",
      "   ------------------------------\n",
      "\n",
      "🔍 Analysis for: x1_normal\n",
      "   Group Statistics:\n",
      "     • A: μ=50.939 (±12.219), n=661\n",
      "     • B: μ=51.535 (±14.587), n=375\n",
      "     • C: μ=51.044 (±13.747), n=238\n",
      "   Key Insights:\n",
      "     • ❌ **No significant difference detected** between groups\n",
      "     • 💪 Large sample sizes provide good statistical power\n",
      "\n",
      "🔍 Analysis for: x2_linear_combo\n",
      "   Group Statistics:\n",
      "     • A: μ=101.027 (±20.357), n=661\n",
      "     • B: μ=100.879 (±21.036), n=375\n",
      "     • C: μ=100.612 (±20.642), n=238\n",
      "   Key Insights:\n",
      "     • ❌ **No significant difference detected** between groups\n",
      "     • 💪 Large sample sizes provide good statistical power\n",
      "\n",
      "🔍 Analysis for: x3_skewed\n",
      "   Group Statistics:\n",
      "     • A: μ=0.976 (±0.956), n=661\n",
      "     • B: μ=0.895 (±0.970), n=375\n",
      "     • C: μ=1.059 (±1.004), n=238\n",
      "   Key Insights:\n",
      "     • 🎯 **Significant difference detected** using Kruskal-Wallis H-test (p = 0.011480)\n",
      "     • 📊 Effect size is  with negligible effect size (η² = 0.005)\n",
      "\n",
      "🔎 Categorical Variable Associations\n",
      "   ------------------------------\n",
      "\n",
      "📊 category_low_card ↔ category_high_card\n",
      "   • Chi-square p-value: 0.910868 🟠\n",
      "   • Cramér's V: 0.095 (negligible) \n",
      "     └─ ❌ No significant association\n",
      "\n",
      "🔎 Statistical Methods Performance\n",
      "   ------------------------------\n",
      "📊 Method Usage and Success Rates:\n",
      "  • Welch's ANOVA (unequal variances): 8 uses, 0 significant (0%)\n",
      "  • Kruskal-Wallis H-test: 8 uses, 2 significant (25%)\n",
      "\n",
      "🔎 Key Recommendations\n",
      "   ------------------------------\n",
      "   1. 🎯 **2 variables** show significant differences between category_low_card groups\n",
      "   2. 📊 Variables with differences: x3_skewed, x4_seasonal\n",
      "   3. 🔬 **Modern methods used**: Welch's tests, bootstrap resampling, robust non-parametrics\n",
      "\n",
      "🔎 Statistical Assumptions Summary\n",
      "   ------------------------------\n",
      "• Variables with Normal Distributions: 8/8 (100%) 🟢\n",
      "• Variables with Equal Variances: 8/8 (100%) 🟢\n",
      "\n",
      "🔎 🎯 CONCLUSION\n",
      "   ------------------------------\n",
      "   📊 SOME variables (2/8) show significant differences between category_low_card\n",
      "   📈 Consider using category_low_card as a strong predictor in your models\n",
      "\n",
      "================================================================================\n",
      "📊 GRAPH NETWORK ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔎 Network Construction Overview\n",
      "   ------------------------------\n",
      "• Original Data: 1500 samples × 10 features \n",
      "• Analyzed Data: 1274 samples × 8 features \n",
      "\n",
      "🔎 Network Summary\n",
      "   ------------------------------\n",
      "• Graph Construction Success: 3/3 (100%) 🟢\n",
      "   🏆 Best Network Type: Knn\n",
      "   • Network Size: 8 nodes, 18 edges \n",
      "   • Network Density: 0.643 🟢\n",
      "   • Connectivity: Fully Connected 🟢\n",
      "   • Clustering Coefficient: 0.783 🟢\n",
      "\n",
      "🔎 Network Topology Analysis\n",
      "   ------------------------------\n",
      "🔗 Connectivity Analysis:\n",
      "   • Network Diameter: 3 \n",
      "   • Average Path Length: 1.39 \n",
      "   • Network Radius: 2 \n",
      "     └─ ⚡ Efficient information flow\n",
      "\n",
      "🌍 Small World Network:\n",
      "   • Small-world σ: 1.21 🟢\n",
      "     └─ ✅ Efficient global connectivity with local clustering\n",
      "\n",
      "📊 Degree Distribution:\n",
      "   • Mean Degree: 4.5 \n",
      "   • Degree Range: 3 - 6 \n",
      "   • Degree Std Dev: 1.1 \n",
      "   • Degree Assortativity: -0.184 🟡\n",
      "     └─ 📉 Dissimilar-degree nodes tend to connect\n",
      "\n",
      "🔎 Community Structure Analysis\n",
      "   ------------------------------\n",
      "🏘️ Community Detection Results:\n",
      "   • Best Method: Louvain \n",
      "   • Communities Found: 2 \n",
      "   • Modularity Score: 0.083 🟠\n",
      "     └─ 🔍 Weak community structure\n",
      "   📋 All Community Detection Results:\n",
      "     • Louvain: 2 communities (Q=0.083)\n",
      "     • Girvan Newman: 2 communities (Q=-0.000)\n",
      "     • Spectral: 2 communities (Q=0.061)\n",
      "     • Label Propagation: 1 communities (Q=0.000)\n",
      "\n",
      "🔎 Node Importance Analysis\n",
      "   ------------------------------\n",
      "⭐ Most Important Variables:\n",
      "   • Degree: x5_count (0.857)\n",
      "     └─ Measures local connectivity\n",
      "   • Pagerank: x3_skewed (0.208)\n",
      "   • Betweenness: x5_count (0.167)\n",
      "   • Eigenvector: x5_count (0.440)\n",
      "   • Closeness: x5_count (0.875)\n",
      "\n",
      "👑 Top 3 Most Influential Variables:\n",
      "     1. x3_skewed (PageRank: 0.2081)\n",
      "     2. x6_bounded (PageRank: 0.2064)\n",
      "     3. x4_seasonal (PageRank: 0.1710)\n",
      "\n",
      "🔎 Graph Construction Methods Comparison\n",
      "   ------------------------------\n",
      "📊 Network Construction Results:\n",
      "   • Correlation: 8 nodes, 2 edges\n",
      "     └─ Edge types: correlation\n",
      "     └─ Density: 0.071\n",
      "   • Mutual Info: 8 nodes, 7 edges\n",
      "     └─ Edge types: mutual_info\n",
      "     └─ Density: 0.250\n",
      "   • Knn: 8 nodes, 18 edges\n",
      "     └─ Edge types: knn\n",
      "     └─ Density: 0.643\n",
      "\n",
      "🔎 Key Insights & Recommendations\n",
      "   ------------------------------\n",
      "   1. 🏆 **Best graph structure**: Knn Network\n",
      "   2. 🔗 **Highly connected network** - variables form a cohesive system\n",
      "   3. ⚡ **Short path lengths** - information flows efficiently through network\n",
      "   4. 🔘 **High clustering** - variables form tight-knit groups\n",
      "   5. 🌍 **Small-world network** - efficient global connectivity with local clustering\n",
      "   6. 🚀 Ideal for feature engineering and dimensionality reduction\n",
      "\n",
      "🔎 🎯 NETWORK ANALYSIS CONCLUSION\n",
      "   ------------------------------\n",
      "   🌟 STRONG network structure detected with 8 interconnected variables\n",
      "   🔗 Rich connectivity (18 relationships) enables advanced graph-based analysis\n",
      "\n",
      "================================================================================\n",
      "📊 COMPREHENSIVE DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "Quality Dimensions:\n",
      "• Completeness: 98.4% 🟢\n",
      "• Uniqueness: 100.0% 🟢\n",
      "• Consistency: 65.0% 🔴\n",
      "• Validity: 57.3% 🔴\n",
      "\n",
      "🔎 Categorical Feature Health\n",
      "   ------------------------------\n",
      "   • category_low_card:\n",
      "   • Unique Values: 3 (0.2%) 🟡\n",
      "   • category_high_card:\n",
      "   • Unique Values: 20 (1.3%) 🟡\n",
      "\n",
      "🔎 Overall Quality Summary\n",
      "   ------------------------------\n",
      "• Overall Quality Score: 80.2% \n",
      "   🟡 Good - minor preprocessing\n",
      "\n",
      "================================================================================\n",
      "📊 EXECUTIVE RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "🔎 Action Items by Priority\n",
      "   ------------------------------\n",
      "\n",
      "   ⚠️ MEDIUM PRIORITY:\n",
      "   ⚠️ Consider transformations for skewed features\n",
      "   ⚠️ Review and validate detected outliers\n",
      "\n",
      "================================================================================\n",
      "📊 ANALYSIS COMPLETION SUMMARY\n",
      "================================================================================\n",
      "✅ Completed Analyses: 9\n",
      "   • Distribution Analysis\n",
      "   • Correlation Analysis\n",
      "   • Pattern Detection\n",
      "   • Feature Engineering\n",
      "   • Time Series Analysis\n",
      "   • Clustering Analysis\n",
      "   • Outlier Detection\n",
      "   • Missingness Analysis\n",
      "   • Dimensionality Reduction\n",
      "\n",
      "📊 Dataset Shape: 1,500 rows × 10 columns\n",
      "💾 Memory Usage: 0.29 MB\n",
      "🎯 Overall Quality: 80.2% (Good)\n",
      "\n",
      "================================================================================\n",
      "📋 COMPREHENSIVE DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "🔍 Use these insights to guide preprocessing and modeling.\n",
      "📈 Consider the priority recommendations for optimal results.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "from foreminer.foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
