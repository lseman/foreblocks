{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from foreblocks import TimeSeriesPreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "# Create a time series with trend, seasonality, and noise\n",
    "t = np.linspace(0, 4*np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)  # Daily pattern\n",
    "seasonality2 = 1 * np.sin(t/24)  # Weekly pattern\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Combine components\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# Create a second feature (to demonstrate multivariate capabilities)\n",
    "data2 = (0.5 * trend + 1.5 * np.cos(t) + 0.5 * np.random.normal(0, 0.3, n_samples)).reshape(-1, 1)\n",
    "data = np.hstack([data, data2])  # Now we have shape [n_samples, 2]\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[outlier_indices] = data[outlier_indices] + 5 * np.random.randn(10, 2)\n",
    "\n",
    "# Add some missing values (but not too many)\n",
    "missing_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "data[missing_indices, 0] = np.nan  # Only make some values missing in first feature\n",
    "\n",
    "# Create preprocessor with various techniques enabled\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    normalize=True,\n",
    "    differencing=False,\n",
    "    detrend=True,\n",
    "    apply_ewt=True,\n",
    "    window_size=24,\n",
    "    horizon=12,\n",
    "    remove_outliers=True,\n",
    "    outlier_threshold=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    impute_method=\"iterative\",\n",
    "    ewt_bands=5,\n",
    "    trend_imf_idx=0,\n",
    "    log_transform=False,\n",
    "    filter_window=5,\n",
    "    filter_polyorder=2,\n",
    "    apply_filter=True,\n",
    "    self_tune=True,\n",
    "    apply_imputation=True,\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X, y, processed_data, _ = preprocessor.fit_transform(data, time_stamps=timestamps)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Data with Outliers and Missing Values')\n",
    "plt.plot(data)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Processed Data')\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "plt.plot(processed_data)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('EWT Components')\n",
    "ewt_components = preprocessor.get_ewt_components()\n",
    "if ewt_components:\n",
    "    for i, imf in enumerate(ewt_components[0].T):\n",
    "        plt.plot(imf, label=f'IMF {i}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input sequence shape: {X.shape}\")\n",
    "print(f\"Target sequence shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€ Fix path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# â”€â”€â”€ Synthetic time series â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='h')\n",
    "\n",
    "t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "trend = 0.1 * t\n",
    "seasonality1 = 2 * np.sin(t)\n",
    "seasonality2 = 1 * np.sin(t / 24)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "data = (trend + seasonality1 + seasonality2 + noise).reshape(-1, 1)\n",
    "\n",
    "# â”€â”€â”€ Import feature extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from foreblocks.ts_fengine import SignalProcessor\n",
    "\n",
    "fengine = SignalProcessor()\n",
    "\n",
    "# â”€â”€â”€ Feature extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "window_size = 48\n",
    "step_size = 24\n",
    "signals = {'signal': data.flatten()}\n",
    "labels = {'signal': 0}  # dummy label\n",
    "\n",
    "features, feature_labels, raw_windows, window_labels = fengine.process_signals(\n",
    "    signals, labels, window_size=window_size, step_size=step_size, augment=True\n",
    ")\n",
    "\n",
    "selected_names = fengine.get_selected_feature_names()\n",
    "print(f\"Selected {len(selected_names)} features:\")\n",
    "print(selected_names)\n",
    "\n",
    "# â”€â”€â”€ Create dataframe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feature_names = fengine.feature_engineer.get_feature_names()\n",
    "features_df = pd.DataFrame(features, columns=feature_names)\n",
    "#features_df.index = timestamps[window_size - 1::step_size][:len(features_df)]\n",
    "\n",
    "# â”€â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(features_df.head())\n",
    "print(feature_names[:5], \"...\")  # preview names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc85fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02184ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3166270/2748512264.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DatasetAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     67\u001b[39m df = generate_synthetic_dataset(n_samples=\u001b[32m1500\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m analyzer = \u001b[43mDatasetAnalyzer\u001b[49m(df, time_col=\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     69\u001b[39m analyzer.analyze_everything()\n",
      "\u001b[31mNameError\u001b[39m: name 'DatasetAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "\n",
    "from foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac319d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook directory: /home/seman/baseline/foreblocks/examples\n",
      "Parent directory: /home/seman/baseline/foreblocks\n",
      "Added /home/seman/baseline/foreblocks/foretools to sys.path\n",
      "ðŸ” Initialized analyzer with 1,500 rows Ã— 10 columns\n",
      "   â€¢ Numeric features: 8\n",
      "   â€¢ Categorical features: 2\n",
      "ðŸ” ðŸš€ Starting comprehensive dataset analysis...\n",
      "ðŸ” Running distributions analysis...\n",
      "ðŸ” Running correlations analysis...\n",
      "ðŸ” Running outliers analysis...\n",
      "ðŸ” Running clusters analysis...\n",
      "ðŸ” Running dimensionality analysis...\n",
      "ðŸ” Running patterns analysis...\n",
      "ðŸ” Running missingness analysis...\n",
      "ðŸ” Running feature_engineering analysis...\n",
      "ðŸ” Running shap_explanation analysis...\n",
      "ðŸ” Running categorical_groups analysis...\n",
      "ðŸ” Running graph_analysis analysis...\n",
      "ðŸ” Running timeseries analysis...\n",
      "interval columns not set, guessing: ['x1_normal', 'x2_linear_combo', 'x3_skewed', 'x4_seasonal', 'x5_count', 'x6_bounded', 'x7_lognorm', 'x8_nonlinear']\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "Dataset Overview:\n",
      "â€¢ Shape: 1,500 rows Ã— 10 columns \n",
      "â€¢ Memory Usage: 0.29 MB \n",
      "â€¢ Numeric Features: 8 \n",
      "â€¢ Categorical Features: 2 \n",
      "â€¢ Missing Values: 1.61% ðŸŸ¡\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "Statistical Properties:\n",
      "â€¢ Normal Distributions: 1/8 (12.5%) \n",
      "â€¢ Skewed Distributions: 4/8 (50.0%) \n",
      "â€¢ Heavy-Tailed: 5/8 (62.5%) \n",
      "â€¢ Average Outlier Rate: 0.96% \n",
      "\n",
      "ðŸ”Ž Feature Quality Assessment\n",
      "   ------------------------------\n",
      "âœ… Normal Distributions (1):\n",
      "   x2_linear_combo\n",
      "ðŸ’¡ Safe for parametric methods and linear models\n",
      "\n",
      "âš ï¸ Highly Skewed Features (3):\n",
      "   â€¢ x1_normal: 3.43 (right-skewed)\n",
      "   â€¢ x7_lognorm: 3.41 (right-skewed)\n",
      "   â€¢ x8_nonlinear: 3.98 (right-skewed)\n",
      "âš ï¸ Apply log/sqrt transform or use robust methods\n",
      "\n",
      "ðŸ”€ Potential Bimodal Distributions (3):\n",
      "   x3_skewed, x4_seasonal, x8_nonlinear\n",
      "ðŸ’¡ Investigate mixtures or stratify data\n",
      "\n",
      "ðŸ”Ž Preprocessing Recommendations\n",
      "   ------------------------------\n",
      "ðŸ’¡ Non-normal dominant â€” consider robust/non-parametric methods\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "Correlation Summary:\n",
      "â€¢ Strong Positive (>0.7): 2 \n",
      "â€¢ Strong Negative (<-0.7): 0 \n",
      "â€¢ Moderate (0.5â€“0.7): 0 \n",
      "\n",
      "ðŸ”Ž Strong Positive Correlations\n",
      "   ------------------------------\n",
      "â€¢ x1_normal â†” x2_linear_combo: 0.739 \n",
      "â€¢ x3_skewed â†” x8_nonlinear: 0.917 \n",
      "âš ï¸ Consider dimensionality reduction or feature selection\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ADVANCED PATTERN DETECTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ž Feature Type Classification\n",
      "   ------------------------------\n",
      "â€¢ Gaussian: 1 features \n",
      "   x2_linear_combo\n",
      "â€¢ Log Normal Candidates: 3 features \n",
      "   x1_normal, x2_linear_combo, x7_lognorm\n",
      "â€¢ Bounded: 1 features \n",
      "   x6_bounded\n",
      "â€¢ Count Like: 1 features \n",
      "   x5_count\n",
      "â€¢ Continuous: 5 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x7_lognorm, x8_nonlinear\n",
      "â€¢ Highly Skewed: 3 features \n",
      "   x1_normal, x7_lognorm, x8_nonlinear\n",
      "â€¢ Heavy Tailed: 5 features \n",
      "   x1_normal, x3_skewed, x5_count, x7_lognorm, x8_nonlinear\n",
      "â€¢ Mixture Model: 7 features \n",
      "   x1_normal, x3_skewed, x4_seasonal, x5_count, x6_bounded ... and 2 more\n",
      "â€¢ Power Law: 3 features \n",
      "   x1_normal, x3_skewed, x7_lognorm\n",
      "â€¢ Bimodal: 8 features \n",
      "   x1_normal, x2_linear_combo, x3_skewed, x4_seasonal, x5_count ... and 3 more\n",
      "â€¢ Uniform Like: 3 features \n",
      "   x2_linear_combo, x4_seasonal, x5_count\n",
      "â€¢ Transformable To Normal: 1 features \n",
      "   x7_lognorm\n",
      "\n",
      "ðŸ”Ž Transformation Opportunities\n",
      "   ------------------------------\n",
      "ðŸ”„ Transformable: x7_lognorm\n",
      "ðŸ’¡ Apply Box-Cox or Yeo-Johnson\n",
      "\n",
      "ðŸ”Ž Relationship Patterns\n",
      "   ------------------------------\n",
      "\n",
      "ðŸŒ€ Non-Linear Relationships:\n",
      "     â””â”€ Best fit: Quadratic (RÂ²=0.894)\n",
      "ðŸ’¡ Try polynomial/kernels or specific functional forms\n",
      "\n",
      "ðŸ”„ Regime-Switching Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: regime diff: 0.703 \n",
      "     â””â”€ Low regime corr: 0.253\n",
      "     â””â”€ High regime corr: 0.957\n",
      "     â””â”€ Split at: 0.66\n",
      "     â””â”€ Strength: strong\n",
      "â€¢ x1_normal â†” x2_linear_combo: regime diff: 0.588 \n",
      "     â””â”€ Low regime corr: 0.925\n",
      "     â””â”€ High regime corr: 0.337\n",
      "     â””â”€ Split at: 50.59\n",
      "     â””â”€ Strength: strong\n",
      "ðŸ’¡ Consider regime-aware models, mixture models, or threshold effects\n",
      "\n",
      "ðŸ“ Detected Functional Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: RÂ²: 0.983 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "â€¢ x1_normal â†” x2_linear_combo: RÂ²: 0.894 \n",
      "     â””â”€ Complexity: moderate\n",
      "     â””â”€ Alternatives: \n",
      "ðŸ’¡ Use detected functional forms for feature engineering or model selection\n",
      "\n",
      "ðŸŽ¯ Strong Multi-Method Dependencies:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: ensemble: 0.979 \n",
      "     â””â”€ Pearson: 0.917\n",
      "     â””â”€ Spearman: 0.831\n",
      "     â””â”€ Distance: 0.926\n",
      "     â””â”€ MI: 1.129\n",
      "     â””â”€ Strength: very_strong\n",
      "â€¢ x1_normal â†” x2_linear_combo: ensemble: 0.978 \n",
      "     â””â”€ Pearson: 0.739\n",
      "     â””â”€ Spearman: 0.953\n",
      "     â””â”€ Distance: 0.937\n",
      "     â””â”€ MI: 1.414\n",
      "     â””â”€ Strength: very_strong\n",
      "ðŸ’¡ High-confidence relationships - prioritize for modeling\n",
      "\n",
      "ðŸ”— Copula-Based Dependencies:\n",
      "â€¢ x1_normal â†” x2_linear_combo: Ï„: 0.835 \n",
      "     â””â”€ Type: body dependence\n",
      "     â””â”€ Tail coefficient: 0.036\n",
      "â€¢ x3_skewed â†” x8_nonlinear: Ï„: 0.663 \n",
      "     â””â”€ Type: body dependence\n",
      "     â””â”€ Tail coefficient: 0.048\n",
      "ðŸ’¡ Consider copula models for capturing rank-based dependencies\n",
      "\n",
      "ðŸ“ˆ Monotonic Relationships:\n",
      "â€¢ x3_skewed â†” x8_nonlinear: Ï„: 0.663 \n",
      "     â””â”€ Type: monotonic\n",
      "â€¢ x1_normal â†” x2_linear_combo: Ï„: 0.835 \n",
      "     â””â”€ Type: monotonic\n",
      "ðŸ’¡ Monotonic transforms, rank-based methods, or ordinal approaches\n",
      "\n",
      "ðŸ”Ž Statistical Distribution Fitting\n",
      "   ------------------------------\n",
      "   â€¢ x1_normal: T âœ…\n",
      "   â€¢ AIC: 11382.99 \n",
      "   â€¢ KS p-value: 0.4202 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x2_linear_combo: Normal âœ…\n",
      "   â€¢ AIC: 13330.59 \n",
      "   â€¢ KS p-value: 0.8224 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x3_skewed: Exponential âœ…\n",
      "   â€¢ AIC: 2773.47 \n",
      "   â€¢ KS p-value: 0.9065 ðŸŸ¢\n",
      "     â””â”€ Fit Quality: Excellent\n",
      "   â€¢ x4_seasonal: Weibull_Min âŒ\n",
      "   â€¢ AIC: 3259.47 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "   â€¢ x5_count: Lognormal âŒ\n",
      "   â€¢ AIC: 5013.16 \n",
      "   â€¢ KS p-value: 0.0000 ðŸ”´\n",
      "     â””â”€ Fit Quality: Poor\n",
      "ðŸ’¡ Use fitted distributions for simulation, anomaly detection, or Bayesian priors\n",
      "\n",
      "ðŸ”Ž Pattern Summary\n",
      "   ------------------------------\n",
      "â€¢ Total Relationship Patterns: 13 \n",
      "   Most Common Pattern Types:\n",
      "   â€¢ Monotonic: 2 relationships\n",
      "   â€¢ Conditional: 2 relationships\n",
      "   â€¢ Regime Switching: 2 relationships\n",
      "\n",
      "ðŸ“‹ Advanced Modeling Recommendations:\n",
      "   â€¢ Consider regime-switching models (Markov switching, threshold VAR)\n",
      "   â€¢ Leverage detected functional forms for feature engineering\n",
      "   â€¢ Explore copula-based models for dependency modeling\n",
      "   â€¢ High-confidence relationships - prioritize for interaction terms\n",
      "   â€¢ Rich relationship structure - consider ensemble methods\n",
      "   â€¢ Very strong dependencies detected - check for potential data leakage\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š OUTLIER DETECTION ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Methods Attempted: 14 \n",
      "â€¢ Successful Methods: 15 \n",
      "â€¢ Overall Outlier Rate: 7.4% ðŸŸ¡\n",
      "â€¢ Best Method: ELLIPTIC_ENVELOPE \n",
      "\n",
      "ðŸ”Ž Analysis Scope\n",
      "   ------------------------------\n",
      "â€¢ Total Samples: 1,500 \n",
      "â€¢ Analyzed Samples: 1,274 \n",
      "â€¢ Features Analyzed: 8 \n",
      "â€¢ Missing Data: 226 (15.1%) \n",
      "\n",
      "ðŸ”Ž Data Preprocessing\n",
      "   ------------------------------\n",
      "â€¢ Scaling Method: Power Transform \n",
      "â€¢ Missing Data Strategy: Complete Cases Only \n",
      "â€¢ Data Skewness: 1.67 (Moderately skewed) ðŸŸ \n",
      "\n",
      "ðŸ” DETAILED METHOD RESULTS\n",
      "--------------------------------------------------\n",
      "\n",
      "   ðŸ”¹ Z Score:\n",
      "   â€¢ Outliers Detected: 26 (1.73%) ðŸŸ¡\n",
      "   â€¢ Separation Quality: 2.116 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Modified Z Score:\n",
      "   â€¢ Outliers Detected: 21 (1.40%) ðŸŸ¡\n",
      "   â€¢ Separation Quality: 2.920 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Iqr:\n",
      "   â€¢ Outliers Detected: 71 (4.73%) ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Pca Recon:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.734 ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Knn Distance:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Dbscan:\n",
      "   â€¢ Outliers Detected: 41 (2.73%) ðŸŸ¡\n",
      "\n",
      "   ðŸ”¹ Mahalanobis Robust:\n",
      "   â€¢ Outliers Detected: 226 (15.07%) âš ï¸\n",
      "\n",
      "   ðŸ”¹ Isolation Forest:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.089 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Local Outlier Factor:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.274 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ One Class Svm:\n",
      "   â€¢ Outliers Detected: 129 (8.60%) ðŸŸ \n",
      "   â€¢ Separation Quality: 4.277 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Elliptic Envelope:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 44.806 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Ecod:\n",
      "   â€¢ Outliers Detected: 129 (8.60%) ðŸŸ \n",
      "   â€¢ Separation Quality: 0.068 ðŸŸ \n",
      "\n",
      "   ðŸ”¹ Copod:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 7.401 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Hbos:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "   â€¢ Separation Quality: 4.704 ðŸŸ¢\n",
      "\n",
      "   ðŸ”¹ Ensemble:\n",
      "   â€¢ Outliers Detected: 128 (8.53%) ðŸŸ \n",
      "\n",
      "ðŸ” OUTLIER TREATMENT RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ 1. Recommended method: COPOD (score: 2.20)\n",
      "ðŸ’¡ 2. Healthy outlier rate: 10.0%\n",
      "ðŸ’¡ 3. ðŸŽ¯ Multiple methods succeeded â€” ensemble reliability is high.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š CLUSTERING ANALYSIS\n",
      "================================================================================\n",
      "Analysis Summary:\n",
      "â€¢ Methods Attempted: 7 \n",
      "â€¢ Successful Methods: 7 \n",
      "â€¢ Best Method: HIERARCHICAL \n",
      "\n",
      "ðŸ”Ž Optimal Cluster Analysis\n",
      "   ------------------------------\n",
      "â€¢ Estimated Optimal K: 4 \n",
      "â€¢ Confidence Level: Low \n",
      "\n",
      "ðŸ”Ž Method Performance\n",
      "   ------------------------------\n",
      "\n",
      "   ðŸ” Kmeans\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 3 \n",
      "   â€¢ Size Range: 115-632 points \n",
      "   â€¢ Balance Ratio: 0.18 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.163 ðŸŸ \n",
      "   â€¢ Best K (internal): 3 \n",
      "\n",
      "   ðŸ” Hierarchical\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 4 \n",
      "   â€¢ Size Range: 3-1230 points \n",
      "   â€¢ Balance Ratio: 0.00 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.493 ðŸŸ \n",
      "\n",
      "   ðŸ” Dbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 1 \n",
      "   â“˜ Single or no cluster detected (silhouette may be undefined).\n",
      "   â€¢ Size Range: 1207-1207 points \n",
      "   â€¢ Balance Ratio: 1.00 ðŸŸ¢\n",
      "   â€¢ Epsilon (DBSCAN): 1.777 \n",
      "   â€¢ Min Samples (DBSCAN): 5 \n",
      "\n",
      "   ðŸ” Hdbscan\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 5 \n",
      "   â€¢ Size Range: 16-40 points \n",
      "   â€¢ Balance Ratio: 0.40 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.264 ðŸŸ \n",
      "\n",
      "   ðŸ” Gmm\n",
      "      ------------------------------\n",
      "   â€¢ Clusters Found: 7 \n",
      "   â€¢ Size Range: 20-378 points \n",
      "   â€¢ Balance Ratio: 0.05 ðŸŸ \n",
      "   â€¢ Silhouette Score: 0.048 ðŸŸ \n",
      "\n",
      "ðŸ” RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "ðŸ’¡ Best method: KMEANS (score: 0.453)\n",
      "ðŸ’¡ Unclear cluster structure â€” verify if clustering is appropriate for this data.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š TIME SERIES ANALYSIS\n",
      "================================================================================\n",
      "Analysis Overview:\n",
      "â€¢ Total Series: 8 \n",
      "â€¢ Stationary Series: 8/8 (100.0%) \n",
      "â€¢ Forecast-Ready: 3/8 (37.5%) \n",
      "\n",
      "ðŸ”Ž Stationarity Assessment\n",
      "   ------------------------------\n",
      "âœ… Stationary (8):\n",
      "   â€¢ x1_normal: ADF p=0.0000 \n",
      "   â€¢ x2_linear_combo: ADF p=0.0000 \n",
      "   â€¢ x3_skewed: ADF p=0.0000 \n",
      "   ... and 5 more\n",
      "\n",
      "ðŸ”Ž Detected Temporal Patterns\n",
      "   ------------------------------\n",
      "\n",
      "ðŸ”„ Seasonal Patterns:\n",
      "   â€¢ x3_skewed: strength=0.859 \n",
      "   â€¢ x6_bounded: strength=0.818 \n",
      "   â€¢ x5_count: strength=0.727 \n",
      "\n",
      "ðŸ”Ž Lag & Seasonality Suggestions\n",
      "   ------------------------------\n",
      "Per-series:\n",
      "   â€¢ x3_skewed: lags=[9] | P=365 | strength=0.859 \n",
      "   â€¢ x6_bounded: lags=[1] | P=365 | strength=0.818 \n",
      "   â€¢ x5_count: lags=[10] | P=365 | strength=0.727 \n",
      "   â€¢ x2_linear_combo: lags=[10] | P=365 | strength=0.678 \n",
      "   â€¢ x8_nonlinear: lags=[10] | P=365 | strength=0.542 \n",
      "   â€¢ x4_seasonal: lags=[1, 2, 3] | seasonal_lags=[12] | P=365 | strength=0.538 \n",
      "   ... and 2 more\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m df = generate_synthetic_dataset(n_samples=\u001b[32m1500\u001b[39m)\n\u001b[32m     67\u001b[39m analyzer = DatasetAnalyzer(df, time_col=\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foreminer/foreminer.py:245\u001b[39m, in \u001b[36mDatasetAnalyzer.analyze_everything\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m results = \u001b[38;5;28mself\u001b[39m.analyze()\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Generate comprehensive insights report\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprint_detailed_insights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/baseline/foreblocks/foretools/foreminer/foreminer.py:1054\u001b[39m, in \u001b[36mDatasetAnalyzer.print_detailed_insights\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m all_rec_lags   = [lag \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m (r[\u001b[33m\"\u001b[39m\u001b[33mrec_lags\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1053\u001b[39m all_seas_lags  = [lag \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m (r[\u001b[33m\"\u001b[39m\u001b[33mseas_lags\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m period_counts  = \u001b[43mCounter\u001b[49m([r[\u001b[33m\"\u001b[39m\u001b[33mstl_period\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m lag_rows \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mstl_period\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m   1055\u001b[39m rec_counts     = Counter(all_rec_lags)\n\u001b[32m   1056\u001b[39m seaslag_counts = Counter(all_seas_lags)\n",
      "\u001b[31mNameError\u001b[39m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "import sys\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    # add parent_dir / foretools to sys.path\n",
    "    foretools_dir = os.path.join(parent_dir, 'foretools')\n",
    "    if foretools_dir not in sys.path:\n",
    "        sys.path.append(foretools_dir)\n",
    "        print(f\"Added {foretools_dir} to sys.path\")\n",
    "\n",
    "from foreminer.foreminer import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_dataset(n_samples=1000, seed=42) -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Time column\n",
    "    time = pd.date_range(start=\"2022-01-01\", periods=n_samples, freq=\"H\")\n",
    "\n",
    "    # Numeric features\n",
    "    x1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "    x2 = 2 * x1 + np.random.normal(0, 5, n_samples)  # strong linear correlation\n",
    "    x3 = np.random.exponential(scale=1.0, size=n_samples)  # skewed\n",
    "    x4 = np.sin(np.linspace(0, 20 * np.pi, n_samples)) + np.random.normal(0, 0.2, n_samples)  # seasonal\n",
    "    x5 = np.random.poisson(lam=5, size=n_samples).astype(float)  # count-like (converted to float to allow NaNs)\n",
    "    x6 = np.random.beta(2, 5, size=n_samples)  # bounded [0, 1]\n",
    "    x7 = np.random.lognormal(mean=2, sigma=0.8, size=n_samples)  # log-normal candidate\n",
    "    x8 = x3 ** 2 + np.random.normal(0, 0.5, n_samples)  # non-linear relation with x3\n",
    "\n",
    "    # Categorical features\n",
    "    cat1 = np.random.choice(['A', 'B', 'C'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "    cat2 = np.random.choice([f\"Category_{i}\" for i in range(20)], size=n_samples)\n",
    "\n",
    "    # Introduce missing values (5% in some columns)\n",
    "    for arr in [x3, x5, x6]:\n",
    "        mask = np.random.rand(n_samples) < 0.05\n",
    "        arr[mask] = np.nan\n",
    "\n",
    "    # Inject outliers in x1\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    x1[outlier_indices] += np.random.normal(100, 10, size=10)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': time,\n",
    "        'x1_normal': x1,\n",
    "        'x2_linear_combo': x2,\n",
    "        'x3_skewed': x3,\n",
    "        'x4_seasonal': x4,\n",
    "        'x5_count': x5,\n",
    "        'x6_bounded': x6,\n",
    "        'x7_lognorm': x7,\n",
    "        'x8_nonlinear': x8,\n",
    "        'category_low_card': cat1,\n",
    "        'category_high_card': cat2\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = generate_synthetic_dataset(n_samples=1500)\n",
    "analyzer = DatasetAnalyzer(df, time_col='timestamp')\n",
    "analyzer.analyze_everything()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
