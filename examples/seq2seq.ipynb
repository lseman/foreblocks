{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add21fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47886310",
   "metadata": {},
   "source": [
    "# Informer-Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeedForwardBlock] Using standard FFN (SwiGLU)\n",
      "[MultiAttention] Type: prob_sparse, Backends: {'flash': True, 'xformers': True, 'sdp': True, 'softpick': True}\n",
      "[FeedForwardBlock] Using Mixture-of-Experts\n",
      "[MultiAttention] Type: prob_sparse, Backends: {'flash': True, 'xformers': True, 'sdp': True, 'softpick': True}\n",
      "[MultiAttention] Type: prob_sparse, Backends: {'flash': True, 'xformers': True, 'sdp': True, 'softpick': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:12<00:00,  7.78epoch/s, train=nan, val=N/A, lr=1.00e-03, quant=, distill=]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from foreblocks import ForecastingModel\n",
    "from foreblocks import TransformerEncoder, TransformerDecoder, AttentionLayer\n",
    "from foreblocks import Trainer\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "target_len = 10\n",
    "seq_len = 50\n",
    "total_len = 1000  # Total synthetic time series length\n",
    "\n",
    "model_params = {\n",
    "    \"input_processor_output_size\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"nhead\": 4,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"dim_feedforward\": 2048,\n",
    "    \"seq_len\": 50,\n",
    "    \"target_len\": 10,\n",
    "    \"total_len\": 1000,\n",
    "    \"input_size\": 1,\n",
    "    \"output_size\": 1,\n",
    "    }\n",
    "\n",
    "# 1. Create encoder and decoder\n",
    "encoder = TransformerEncoder(\n",
    "    input_size=model_params.get(\"input_processor_output_size\", 1),\n",
    "    # hidden_size=model_params.get(\"hidden_size\", 64),\n",
    "    nhead=model_params.get(\"nhead\", 4),\n",
    "    num_layers=model_params.get(\"num_encoder_layers\", 1),\n",
    "    dropout=model_params.get(\"dropout\", 0.1),\n",
    "    dim_feedforward=model_params.get(\"dim_feedforward\", 2048),\n",
    "    att_type=\"prob_sparse\",  # Use probabilistic sparse attention\n",
    ")\n",
    "\n",
    "# Create transformer decoder\n",
    "decoder = TransformerDecoder(\n",
    "    input_size=model_params.get(\"input_processor_output_size\", 1),\n",
    "    # hidden_size=model_params.get(\"hidden_size\", 64),\n",
    "    output_size=output_size,\n",
    "    nhead=model_params.get(\"nhead\", 2),\n",
    "    num_layers=model_params.get(\"num_decoder_layers\", 1),\n",
    "    dropout=model_params.get(\"dropout\", 0.1),\n",
    "    dim_feedforward=model_params.get(\"dim_feedforward\", 2048),\n",
    "    informer_like=True,  # Use Informer-like architecture,\n",
    "    att_type=\"prob_sparse\",  # Use probabilistic sparse attention\n",
    "    use_moe=True,  # Use Mixture of Experts\n",
    ")\n",
    "\n",
    "model = ForecastingModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    target_len=target_len,\n",
    "    input_preprocessor=None,\n",
    "    output_postprocessor=None,\n",
    "    output_block=None,\n",
    "    #attention_module=attention_module,\n",
    "    forecasting_strategy=\"seq2seq\",\n",
    "    model_type=\"informer-like\",  # Use Informer-like architecture\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), criterion=nn.MSELoss())\n",
    "\n",
    "# 2. Generate synthetic time series\n",
    "time_series = np.sin(np.linspace(0, 30 * np.pi, total_len)) + 0.1 * np.random.randn(total_len)\n",
    "time_series = torch.tensor(time_series, dtype=torch.float32).unsqueeze(-1)  # Shape: [T, 1]\n",
    "\n",
    "# 3. Prepare sliding windows for training\n",
    "def create_sliding_windows(series, input_len, output_len):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(series) - input_len - output_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        Y.append(series[i+input_len:i+input_len+output_len])\n",
    "    return torch.stack(X), torch.stack(Y)\n",
    "\n",
    "X, Y = create_sliding_windows(time_series, seq_len, target_len)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:], Y[train_size:]\n",
    "# 3. Prepare sliding windows for training\n",
    "def create_sliding_windows(series, input_len, output_len):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(series) - input_len - output_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        Y.append(series[i+input_len:i+input_len+output_len])\n",
    "    return torch.stack(X), torch.stack(Y)\n",
    "\n",
    "X, Y = create_sliding_windows(time_series, seq_len, target_len)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:], Y[train_size:]\n",
    "\n",
    "# create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "data = trainer.train(train_loader)\n",
    "trainer.plot_prediction(X_val, Y_val, full_series=time_series, offset=train_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeb654",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from foreblocks import ForecastingModel\n",
    "from foreblocks import LSTMDecoder, LSTMEncoder\n",
    "from foreblocks import Trainer\n",
    "from foreblocks.att import AttentionLayer\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "target_len = 10\n",
    "seq_len = 50\n",
    "total_len = 300  # Total synthetic time series length\n",
    "\n",
    "# 1. Create encoder and decoder\n",
    "encoder = LSTMEncoder(input_size, hidden_size, num_layers)\n",
    "decoder = LSTMDecoder(output_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "attention_module = AttentionLayer(\n",
    "    method='dot',\n",
    "    attention_backend='xformers',\n",
    "    encoder_hidden_size=hidden_size,\n",
    "    decoder_hidden_size=hidden_size,\n",
    ")\n",
    "\n",
    "\n",
    "model = ForecastingModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    target_len=target_len,\n",
    "    forecasting_strategy=\"seq2seq\",\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    output_size=output_size,\n",
    "    #attention_module=attention_module,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), criterion=nn.MSELoss())\n",
    "\n",
    "# 2. Generate synthetic time series\n",
    "time_series = np.sin(np.linspace(0, 30 * np.pi, total_len)) + 0.1 * np.random.randn(total_len)\n",
    "time_series = torch.tensor(time_series, dtype=torch.float32).unsqueeze(-1)  # Shape: [T, 1]\n",
    "\n",
    "# 3. Prepare sliding windows for training\n",
    "def create_sliding_windows(series, input_len, output_len):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(series) - input_len - output_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        Y.append(series[i+input_len:i+input_len+output_len])\n",
    "    return torch.stack(X), torch.stack(Y)\n",
    "\n",
    "X, Y = create_sliding_windows(time_series, seq_len, target_len)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:], Y[train_size:]\n",
    "\n",
    "# create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "data = trainer.train(train_loader)\n",
    "metrics = trainer.metrics(X_val, Y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8db00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_prediction(X_val, Y_val, full_series=time_series, offset=train_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69436f8b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from foreblocks import ForecastingModel\n",
    "from foreblocks import TransformerEncoder, TransformerDecoder, AttentionLayer\n",
    "from foreblocks import Trainer\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "target_len = 10\n",
    "seq_len = 50\n",
    "total_len = 1000  # Total synthetic time series length\n",
    "\n",
    "model_params = {\n",
    "    \"input_processor_output_size\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"nhead\": 4,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"dim_feedforward\": 2048,\n",
    "    \"seq_len\": 50,\n",
    "    \"target_len\": 10,\n",
    "    \"total_len\": 1000,\n",
    "    \"input_size\": 1,\n",
    "    \"output_size\": 1,\n",
    "    }\n",
    "\n",
    "# 1. Create encoder and decoder\n",
    "encoder = TransformerEncoder(\n",
    "    input_size=model_params.get(\"input_processor_output_size\", 1),\n",
    "    # hidden_size=model_params.get(\"hidden_size\", 64),\n",
    "    nhead=model_params.get(\"nhead\", 4),\n",
    "    num_layers=model_params.get(\"num_encoder_layers\", 1),\n",
    "    dropout=model_params.get(\"dropout\", 0.1),\n",
    "    dim_feedforward=model_params.get(\"dim_feedforward\", 2048),\n",
    ")\n",
    "\n",
    "# Create transformer decoder\n",
    "decoder = TransformerDecoder(\n",
    "    input_size=model_params.get(\"input_processor_output_size\", 1),\n",
    "    # hidden_size=model_params.get(\"hidden_size\", 64),\n",
    "    output_size=output_size,\n",
    "    nhead=model_params.get(\"nhead\", 2),\n",
    "    num_layers=model_params.get(\"num_decoder_layers\", 1),\n",
    "    dropout=model_params.get(\"dropout\", 0.1),\n",
    "    dim_feedforward=model_params.get(\"dim_feedforward\", 2048),\n",
    ")\n",
    "\n",
    "model = ForecastingModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    target_len=target_len,\n",
    "    input_preprocessor=None,\n",
    "    output_postprocessor=None,\n",
    "    output_block=None,\n",
    "    #attention_module=attention_module,\n",
    "    forecasting_strategy=\"seq2seq\",\n",
    "    model_type=\"transformer\",\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    output_size=output_size\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), criterion=nn.MSELoss())\n",
    "#trainer.set_config('num_epochs', 200)\n",
    "# 2. Generate synthetic time series\n",
    "time_series = np.sin(np.linspace(0, 30 * np.pi, total_len)) + 0.1 * np.random.randn(total_len)\n",
    "time_series = torch.tensor(time_series, dtype=torch.float32).unsqueeze(-1)  # Shape: [T, 1]\n",
    "\n",
    "# 3. Prepare sliding windows for training\n",
    "def create_sliding_windows(series, input_len, output_len):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(series) - input_len - output_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        Y.append(series[i+input_len:i+input_len+output_len])\n",
    "    return torch.stack(X), torch.stack(Y)\n",
    "\n",
    "X, Y = create_sliding_windows(time_series, seq_len, target_len)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:], Y[train_size:]\n",
    "\n",
    "# create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "data = trainer.train(train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = trainer.plot_prediction(X_val, Y_val, full_series=time_series, offset=train_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
